{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5338f6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.4.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3123, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3178, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3641, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/x7/dwvwf5c92p71ds8swvmz8rlh0000gn/T/ipykernel_3576/3005221300.py\", line 5, in <module>\n",
      "    import torch\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/usr/local/anaconda3/envs/ai4chem/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89beefa8",
   "metadata": {},
   "source": [
    "# Tensors & Autograd\n",
    "\n",
    "1.1 Why Tensors?\n",
    "What to say (conceptual)\n",
    "\n",
    "In PyTorch, everything is a tensor: inputs, parameters, outputs, gradients.\n",
    "A tensor is just a multi-dimensional array plus information needed for differentiation and acceleration.\n",
    "\n",
    "Compare to NumPy:\n",
    "\n",
    "* Same idea as `np.ndarray`\n",
    "* But:\n",
    "\n",
    "    * Knows about gradients\n",
    "    * Can live on GPU\n",
    "    * Can participate in a computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd67ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "torch.Size([3]) torch.Size([3, 3])\n",
      "torch.float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "A = torch.randn(3, 3)\n",
    "\n",
    "print(x)\n",
    "print(x.shape, A.shape)\n",
    "print(x.dtype)\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2090c598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5]) torch.Size([5])\n",
      "torch.Size([4, 2])\n",
      "torch.Size([4, 3])\n",
      "tensor([[-1.7917,  1.6690,  2.5717],\n",
      "        [-0.7770,  0.9734,  1.9033],\n",
      "        [ 2.0456,  1.3837,  1.2479],\n",
      "        [-0.4538,  0.6920,  1.5942]])\n",
      "tensor([2.5500, 1.2421, 0.2769])\n",
      "tensor([[0.7583, 2.9111, 2.8487],\n",
      "        [1.7731, 2.2155, 2.1803],\n",
      "        [4.5957, 2.6258, 1.5248],\n",
      "        [2.0962, 1.9341, 1.8712]])\n"
     ]
    }
   ],
   "source": [
    "# Tensor Operations & Broadcasting\n",
    "\n",
    "x = torch.randn(5)\n",
    "y = torch.randn(5)\n",
    "\n",
    "z = x + y\n",
    "w = x * y\n",
    "print(z.shape,x.shape)\n",
    "\n",
    "# Matrix operations:\n",
    "A = torch.randn(4, 3)\n",
    "B = torch.randn(3, 2)\n",
    "C = A @ B\n",
    "print(C.shape)\n",
    "\n",
    "# Broadcasting\n",
    "x = torch.randn(4, 3)\n",
    "y = torch.randn(3)  \n",
    "z = x + y\n",
    "print(z.shape)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ee986",
   "metadata": {},
   "source": [
    "## Introducing Autograd\n",
    "In PyTorch, autograd is the built-in automatic differentiation engine that powers all neural network training. It allows for the automatic computation of the gradient of any scalar value (like a loss function) with respect to all variables (like model parameters) that contributed to its computation. <br>\n",
    "\n",
    "[Introduction to Autograd](https://docs.pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de401e9d",
   "metadata": {},
   "source": [
    "$$\n",
    "y = x^2 + 3x + 1\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = 2x + 3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267e3d6",
   "metadata": {},
   "source": [
    "* Every operation creates a node\n",
    "* The graph is built during the forward pass\n",
    "* `backward()` applies the chain rule automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2aa478a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2 + 3*x + 1\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76505dd",
   "metadata": {},
   "source": [
    "## Gradient Accumulation\n",
    "* Gradients accumulate\n",
    "* Torch assumes you want to sum gradients\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x^2}{\\partial x} = 2x \\quad \\text{and} \\quad \\frac{\\partial x^3}{\\partial x} = 3x^2  \n",
    "$$\n",
    "\n",
    "PyTorch literally does:<br>\n",
    "`x.grad += new_gradient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60cdab57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "tensor(16.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "y1 = x**2\n",
    "y1.backward()\n",
    "print(x.grad)  # 2x = 4\n",
    "\n",
    "# x.grad.zero_() # uncomment this line\n",
    "\n",
    "y2 = x**3\n",
    "y2.backward()\n",
    "print(x.grad)  # 4 + 3x^2 = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e1e1e",
   "metadata": {},
   "source": [
    "We can use gradient accumulation for multiple loss terms, \n",
    "$$\n",
    "{\\cal L} = {\\cal L}_{\\text{MSE}} + \\lambda {\\cal L}_{\\text{regularization}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9660b3d6",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Networks\n",
    "We will build a **PIP-NN**, permutationally invariant polynomial neural network.\n",
    "\n",
    "1. **Data loader**: provides an iterable over the data samples, simplifying and optimizing the process of feeding data to a model during training or evaluation\n",
    "2. **Model**: Feed Forward Neural Network\n",
    "3. **Training loop**: Main part of training stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a107166",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba40f966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 9000 points.\n",
      "Total energy points: (9000,)\n",
      "Total Geometries: (9000, 30)\n",
      "Training set size: (6300, 30)\n",
      "Test set size: (2700, 30)\n"
     ]
    }
   ],
   "source": [
    "# load data using pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_file = 'https://raw.githubusercontent.com/ChemAI-Lab/Math4Chem/main/website/Assignments/CH4_data.csv'\n",
    "data = pd.read_csv(data_file)\n",
    "data.head()\n",
    "\n",
    "# 1. How many points does the dataset contains?\n",
    "n = data.shape[0]\n",
    "print('The dataset contains {} points.'.format(n))\n",
    "\n",
    "# Load PIPs representation per molecule\n",
    "y_all = data['energy'].to_numpy()\n",
    "print('Total energy points:', y_all.shape)\n",
    "X_all = data.drop(['energy', 'Unnamed: 0'], axis=1).to_numpy()\n",
    "print(\"Total Geometries:\", X_all.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.3, random_state=42)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5410f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Dataset\n",
    "# -----------------------------\n",
    "class PIPDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85dedd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30])\n",
      "torch.Size([128, 1])\n",
      "tensor([[1.0000e+00, 2.0345e-01, 4.7021e-02, 1.5515e-02, 4.7844e-03, 1.8211e-04,\n",
      "         4.7820e-03, 7.3652e-04, 1.0362e-02, 3.7369e-04, 5.2563e-04, 1.2164e-04,\n",
      "         4.8637e-04, 3.7051e-05, 1.2153e-04, 3.7484e-05, 7.4922e-05, 5.7013e-06,\n",
      "         1.9188e-06, 3.7440e-05, 1.9243e-06, 1.5797e-03, 2.4375e-04, 2.4348e-04,\n",
      "         3.8026e-05, 2.8616e-06, 3.8001e-05, 1.1700e-05, 5.2843e-04, 3.0093e-06],\n",
      "        [1.0000e+00, 2.1653e-01, 5.4913e-02, 1.7322e-02, 5.9273e-03, 2.7113e-04,\n",
      "         5.9631e-03, 9.5792e-04, 1.2241e-02, 5.5734e-04, 6.0732e-04, 1.5786e-04,\n",
      "         6.3318e-04, 5.8708e-05, 1.6019e-04, 5.1338e-05, 1.0386e-04, 9.1450e-06,\n",
      "         2.5760e-06, 5.2222e-05, 2.5743e-06, 1.9289e-03, 3.3455e-04, 3.3763e-04,\n",
      "         6.0242e-05, 5.7436e-06, 6.0438e-05, 1.8862e-05, 7.2163e-04, 5.9997e-06],\n",
      "        [1.0000e+00, 2.1446e-01, 5.2470e-02, 1.7166e-02, 5.5929e-03, 2.3899e-04,\n",
      "         5.6598e-03, 8.9327e-04, 1.1661e-02, 4.8862e-04, 6.0784e-04, 1.4860e-04,\n",
      "         6.0007e-04, 5.1254e-05, 1.5201e-04, 4.7353e-05, 9.5740e-05, 7.9608e-06,\n",
      "         2.4409e-06, 4.8476e-05, 2.4428e-06, 1.8578e-03, 3.0217e-04, 3.0969e-04,\n",
      "         5.1762e-05, 4.5793e-06, 5.3026e-05, 1.6297e-05, 6.4302e-04, 4.7614e-06]])\n",
      "tensor([[-40.4813],\n",
      "        [-40.4413],\n",
      "        [-40.4664]])\n",
      "torch.Size([128, 30])\n",
      "torch.Size([128, 1])\n",
      "tensor([[1.0000e+00, 2.1260e-01, 5.3240e-02, 1.6869e-02, 5.6093e-03, 2.2273e-04,\n",
      "         5.7092e-03, 9.2641e-04, 1.1459e-02, 5.3618e-04, 5.9204e-04, 1.4693e-04,\n",
      "         5.9887e-04, 4.7351e-05, 1.5229e-04, 4.8323e-05, 9.8557e-05, 7.6596e-06,\n",
      "         2.6475e-06, 5.0072e-05, 2.6312e-06, 1.8101e-03, 2.9978e-04, 3.1029e-04,\n",
      "         5.6084e-05, 4.1985e-06, 5.7905e-05, 1.8167e-05, 6.2600e-04, 6.1807e-06],\n",
      "        [1.0000e+00, 2.0537e-01, 4.7497e-02, 1.5816e-02, 4.8797e-03, 1.8684e-04,\n",
      "         4.8747e-03, 7.5153e-04, 1.0546e-02, 3.7919e-04, 5.4132e-04, 1.2533e-04,\n",
      "         5.0080e-04, 3.8371e-05, 1.2507e-04, 3.8626e-05, 7.7172e-05, 5.9085e-06,\n",
      "         1.9804e-06, 3.8545e-05, 1.9812e-06, 1.6242e-03, 2.5070e-04, 2.5018e-04,\n",
      "         3.8976e-05, 2.9655e-06, 3.8898e-05, 1.1993e-05, 5.4156e-04, 3.0511e-06],\n",
      "        [1.0000e+00, 2.0741e-01, 4.8142e-02, 1.6132e-02, 4.9946e-03, 1.9261e-04,\n",
      "         4.9906e-03, 7.7158e-04, 1.0756e-02, 3.8924e-04, 5.5762e-04, 1.2954e-04,\n",
      "         5.1775e-04, 3.9951e-05, 1.2933e-04, 4.0041e-05, 8.0017e-05, 6.1671e-06,\n",
      "         2.0584e-06, 3.9978e-05, 2.0582e-06, 1.6731e-03, 2.5912e-04, 2.5870e-04,\n",
      "         4.0399e-05, 3.1057e-06, 4.0334e-05, 1.2461e-05, 5.5786e-04, 3.1717e-06]])\n",
      "tensor([[-40.4680],\n",
      "        [-40.4825],\n",
      "        [-40.4827]])\n",
      "torch.Size([128, 30])\n",
      "torch.Size([128, 1])\n",
      "tensor([[1.0000e+00, 1.8648e-01, 4.9995e-02, 1.2975e-02, 4.7067e-03, 2.4580e-04,\n",
      "         4.6166e-03, 7.1283e-04, 8.8265e-03, 5.8223e-04, 3.9923e-04, 1.1040e-04,\n",
      "         4.3196e-04, 4.5838e-05, 1.0632e-04, 3.3450e-05, 6.6454e-05, 6.0262e-06,\n",
      "         1.5455e-06, 3.3028e-05, 1.5834e-06, 1.2219e-03, 2.2496e-04, 2.1632e-04,\n",
      "         5.6119e-05, 6.2627e-06, 5.2458e-05, 1.4199e-05, 4.2406e-04, 8.6468e-06],\n",
      "        [1.0000e+00, 1.7830e-01, 4.2148e-02, 1.1812e-02, 3.6555e-03, 1.5695e-04,\n",
      "         3.8596e-03, 5.5000e-04, 8.1691e-03, 3.6252e-04, 3.4444e-04, 7.8539e-05,\n",
      "         3.3167e-04, 2.7985e-05, 8.7625e-05, 2.3147e-05, 4.9019e-05, 3.8629e-06,\n",
      "         1.0695e-06, 2.5902e-05, 1.0736e-06, 1.0727e-03, 1.6304e-04, 1.8127e-04,\n",
      "         3.0772e-05, 2.7523e-06, 3.3866e-05, 9.0266e-06, 3.8386e-04, 3.5005e-06],\n",
      "        [1.0000e+00, 2.1555e-01, 5.7730e-02, 1.7411e-02, 6.2317e-03, 3.2028e-04,\n",
      "         6.2122e-03, 1.0115e-03, 1.1642e-02, 6.6916e-04, 6.2454e-04, 1.6806e-04,\n",
      "         6.7009e-04, 6.9038e-05, 1.6698e-04, 5.4625e-05, 1.0899e-04, 1.0374e-05,\n",
      "         2.6779e-06, 5.4417e-05, 2.6787e-06, 1.8794e-03, 3.3706e-04, 3.3502e-04,\n",
      "         7.2475e-05, 8.1164e-06, 7.1765e-05, 2.1577e-05, 6.3007e-04, 8.9373e-06]])\n",
      "tensor([[-40.4421],\n",
      "        [-40.4429],\n",
      "        [-40.4503]])\n"
     ]
    }
   ],
   "source": [
    "dataset_tr = PIPDataset(X_train, y_train)\n",
    "dataset_test = PIPDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset_tr,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# Test/validation loader (no shuffle)\n",
    "test_loader = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "for i,(X_batch, y_batch) in enumerate(loader):\n",
    "    print(X_batch.shape)  # (batch_size, D)\n",
    "    print(y_batch.shape)  # (batch_size, 1)\n",
    "    print(X_batch[:3])\n",
    "    print(y_batch[:3])\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2953855d",
   "metadata": {},
   "source": [
    "## Stochastic approximation of MSE\n",
    "Commonly in ML, we do not use \"clean\" gradient, we usually use a subset of the data (mini-batch) to approximate the gradient\n",
    "\n",
    "$$\n",
    "\\nabla {\\cal L} \\approx \\frac{1}{B} \\sum_i^B \\nabla {\\cal L}_i\n",
    "$$\n",
    "where ${\\cal L}_i$ is the mean square error for each point in the mini-batch ($B$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c7cfed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple two layer neural network classifier using PyTorch (Sequential in a class)\n",
    "class SimpleNNSequential(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=2, output_size=1):\n",
    "        super(SimpleNNSequential, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "173807d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNNSequential(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# --- Model / Optim / Loss\n",
    "pipnn_model = SimpleNNSequential(input_size=X_train.shape[1], hidden_size=64, output_size=1)\n",
    "print(pipnn_model)\n",
    "\n",
    "lr = 2E-3\n",
    "weight_decay = 1E-5\n",
    "optimizer = torch.optim.AdamW(pipnn_model.parameters(), \n",
    "                              lr=lr, \n",
    "                              weight_decay=weight_decay)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# # --- LR scheduler (optional but nice)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162aad87",
   "metadata": {},
   "source": [
    "# Main training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0ad77bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/120], Loss: 1485.7707, Val Loss: 1096.4470\n",
      "Epoch [2/120], Loss: 392.2292, Val Loss: 7.0159\n",
      "Epoch [3/120], Loss: 2.6657, Val Loss: 0.1671\n",
      "Epoch [4/120], Loss: 0.1760, Val Loss: 0.1650\n",
      "Epoch [5/120], Loss: 0.1656, Val Loss: 0.1634\n",
      "Epoch [6/120], Loss: 0.1636, Val Loss: 0.1621\n",
      "Epoch [7/120], Loss: 0.1620, Val Loss: 0.1603\n",
      "Epoch [8/120], Loss: 0.1602, Val Loss: 0.1581\n",
      "Epoch [9/120], Loss: 0.1582, Val Loss: 0.1562\n",
      "Epoch [10/120], Loss: 0.1563, Val Loss: 0.1547\n",
      "Epoch [11/120], Loss: 0.1542, Val Loss: 0.1518\n",
      "Epoch [12/120], Loss: 0.1517, Val Loss: 0.1494\n",
      "Epoch [13/120], Loss: 0.1494, Val Loss: 0.1471\n",
      "Epoch [14/120], Loss: 0.1467, Val Loss: 0.1447\n",
      "Epoch [15/120], Loss: 0.1445, Val Loss: 0.1419\n",
      "Epoch [16/120], Loss: 0.1416, Val Loss: 0.1394\n",
      "Epoch [17/120], Loss: 0.1388, Val Loss: 0.1365\n",
      "Epoch [18/120], Loss: 0.1359, Val Loss: 0.1336\n",
      "Epoch [19/120], Loss: 0.1331, Val Loss: 0.1309\n",
      "Epoch [20/120], Loss: 0.1304, Val Loss: 0.1279\n",
      "Epoch [21/120], Loss: 0.1272, Val Loss: 0.1249\n",
      "Epoch [22/120], Loss: 0.1243, Val Loss: 0.1232\n",
      "Epoch [23/120], Loss: 0.1210, Val Loss: 0.1188\n",
      "Epoch [24/120], Loss: 0.1181, Val Loss: 0.1160\n",
      "Epoch [25/120], Loss: 0.1151, Val Loss: 0.1125\n",
      "Epoch [26/120], Loss: 0.1121, Val Loss: 0.1093\n",
      "Epoch [27/120], Loss: 0.1087, Val Loss: 0.1062\n",
      "Epoch [28/120], Loss: 0.1055, Val Loss: 0.1033\n",
      "Epoch [29/120], Loss: 0.1022, Val Loss: 0.1000\n",
      "Epoch [30/120], Loss: 0.0989, Val Loss: 0.0966\n",
      "Epoch [31/120], Loss: 0.0960, Val Loss: 0.0940\n",
      "Epoch [32/120], Loss: 0.0924, Val Loss: 0.0901\n",
      "Epoch [33/120], Loss: 0.0894, Val Loss: 0.0872\n",
      "Epoch [34/120], Loss: 0.0861, Val Loss: 0.0838\n",
      "Epoch [35/120], Loss: 0.0829, Val Loss: 0.0827\n",
      "Epoch [36/120], Loss: 0.0798, Val Loss: 0.0774\n",
      "Epoch [37/120], Loss: 0.0766, Val Loss: 0.0743\n",
      "Epoch [38/120], Loss: 0.0734, Val Loss: 0.0713\n",
      "Epoch [39/120], Loss: 0.0703, Val Loss: 0.0682\n",
      "Epoch [40/120], Loss: 0.0673, Val Loss: 0.0652\n",
      "Epoch [41/120], Loss: 0.0643, Val Loss: 0.0622\n",
      "Epoch [42/120], Loss: 0.0614, Val Loss: 0.0594\n",
      "Epoch [43/120], Loss: 0.0584, Val Loss: 0.0568\n",
      "Epoch [44/120], Loss: 0.0558, Val Loss: 0.0538\n",
      "Epoch [45/120], Loss: 0.0529, Val Loss: 0.0522\n",
      "Epoch [46/120], Loss: 0.0503, Val Loss: 0.0488\n",
      "Epoch [47/120], Loss: 0.0475, Val Loss: 0.0457\n",
      "Epoch [48/120], Loss: 0.0449, Val Loss: 0.0433\n",
      "Epoch [49/120], Loss: 0.0423, Val Loss: 0.0407\n",
      "Epoch [50/120], Loss: 0.0398, Val Loss: 0.0383\n",
      "Epoch [51/120], Loss: 0.0373, Val Loss: 0.0359\n",
      "Epoch [52/120], Loss: 0.0351, Val Loss: 0.0339\n",
      "Epoch [53/120], Loss: 0.0329, Val Loss: 0.0318\n",
      "Epoch [54/120], Loss: 0.0310, Val Loss: 0.0308\n",
      "Epoch [55/120], Loss: 0.0287, Val Loss: 0.0274\n",
      "Epoch [56/120], Loss: 0.0267, Val Loss: 0.0256\n",
      "Epoch [57/120], Loss: 0.0248, Val Loss: 0.0238\n",
      "Epoch [58/120], Loss: 0.0232, Val Loss: 0.0220\n",
      "Epoch [59/120], Loss: 0.0212, Val Loss: 0.0202\n",
      "Epoch [60/120], Loss: 0.0196, Val Loss: 0.0191\n",
      "Epoch [61/120], Loss: 0.0181, Val Loss: 0.0172\n",
      "Epoch [62/120], Loss: 0.0167, Val Loss: 0.0157\n",
      "Epoch [63/120], Loss: 0.0153, Val Loss: 0.0145\n",
      "Epoch [64/120], Loss: 0.0139, Val Loss: 0.0131\n",
      "Epoch [65/120], Loss: 0.0127, Val Loss: 0.0119\n",
      "Epoch [66/120], Loss: 0.0115, Val Loss: 0.0108\n",
      "Epoch [67/120], Loss: 0.0104, Val Loss: 0.0098\n",
      "Epoch [68/120], Loss: 0.0094, Val Loss: 0.0089\n",
      "Epoch [69/120], Loss: 0.0085, Val Loss: 0.0086\n",
      "Epoch [70/120], Loss: 0.0079, Val Loss: 0.0072\n",
      "Epoch [71/120], Loss: 0.0069, Val Loss: 0.0064\n",
      "Epoch [72/120], Loss: 0.0061, Val Loss: 0.0058\n",
      "Epoch [73/120], Loss: 0.0056, Val Loss: 0.0052\n",
      "Epoch [74/120], Loss: 0.0049, Val Loss: 0.0046\n",
      "Epoch [75/120], Loss: 0.0043, Val Loss: 0.0040\n",
      "Epoch [76/120], Loss: 0.0038, Val Loss: 0.0035\n",
      "Epoch [77/120], Loss: 0.0034, Val Loss: 0.0031\n",
      "Epoch [78/120], Loss: 0.0029, Val Loss: 0.0027\n",
      "Epoch [79/120], Loss: 0.0026, Val Loss: 0.0024\n",
      "Epoch [80/120], Loss: 0.0023, Val Loss: 0.0021\n",
      "Epoch [81/120], Loss: 0.0019, Val Loss: 0.0019\n",
      "Epoch [82/120], Loss: 0.0017, Val Loss: 0.0016\n",
      "Epoch [83/120], Loss: 0.0015, Val Loss: 0.0013\n",
      "Epoch [84/120], Loss: 0.0013, Val Loss: 0.0011\n",
      "Epoch [85/120], Loss: 0.0011, Val Loss: 0.0010\n",
      "Epoch [86/120], Loss: 0.0010, Val Loss: 0.0009\n",
      "Epoch [87/120], Loss: 0.0008, Val Loss: 0.0007\n",
      "Epoch [88/120], Loss: 0.0007, Val Loss: 0.0006\n",
      "Epoch [89/120], Loss: 0.0006, Val Loss: 0.0005\n",
      "Epoch [90/120], Loss: 0.0005, Val Loss: 0.0004\n",
      "Epoch [91/120], Loss: 0.0004, Val Loss: 0.0004\n",
      "Epoch [92/120], Loss: 0.0004, Val Loss: 0.0003\n",
      "Epoch [93/120], Loss: 0.0003, Val Loss: 0.0003\n",
      "Epoch [94/120], Loss: 0.0003, Val Loss: 0.0002\n",
      "Epoch [95/120], Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [96/120], Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [97/120], Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [98/120], Loss: 0.0002, Val Loss: 0.0001\n",
      "Epoch [99/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [100/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [101/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [102/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [103/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [104/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [105/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [106/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [107/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [108/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [109/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [110/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [111/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [112/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [113/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [114/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [115/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [116/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [117/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [118/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [119/120], Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch [120/120], Loss: 0.0001, Val Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 120\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    pipnn_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for i, (X_batch, y_batch) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = pipnn_model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * X_batch.size(0)\n",
    "    epoch_loss /= len(dataset_tr)\n",
    "\n",
    "    # Validation/test loss\n",
    "    pipnn_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = pipnn_model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "    val_loss /= len(dataset_test)\n",
    "\n",
    "    train_losses.append(epoch_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "    # if (epoch) % 10 == 0:\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc846828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArIAAAGGCAYAAACHemKmAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQYJJREFUeJzt3Ql8U2XWx/GTNl3YyioUZFV5ZZFNNhFcQVZZBGVQBFReGAWUZVSGERBBRRFRWQRxFHREQRxhgFFkHVFkRxQQkXdEYcRSHZaydUmT93Oe9sakCxYamnvb3/fziWmSS5reVvj35Dzncfl8Pp8AAAAADhMR7hcAAAAAXAyCLAAAAByJIAsAAABHIsgCAADAkQiyAAAAcCSCLAAAAByJIAsAAABHIsgCAADAkdzhfgFO4PV65ciRI1KqVClxuVzhfjkAAACFlu7VderUKalSpYpERJy/5kqQzQMNsdWqVQvV9wcAAAC/4/Dhw1K1atXzHkOQzQOtxFonNC4uLi9/BAAAABchKSnJFBCt/HU+BNk8sNoJNMQSZAEAAC69vLRzstgLAAAAjkSQBQAAgCMRZAEAAOBI9MgCAIA8SU9Pl7S0NM4W8iUqKkoiIyMlFAiyAADgd+d6JiQkyIkTJzhTCIkyZcpIfHx8vufzE2QBAMB5WSG2YsWKUrx4cTYHQr5+KTp79qwkJiaa25UrV774JyPIAgCA32snsEJs+fLlOVnIt2LFiplrDbP6c5WfNgMWewEAgFxZPbFaiQVCxfp5ym/PNUEWAAD8rvz2MgKX4ueJIAsAAJBHNWvWlJdffjnP5+tf//qXCW2XeqHc/PnzzQKqooYgazOpHq98cyRJ9vx0MtwvBQAAx9LweL7LhAkTLup5t23bJoMHD87z8ddff738/PPPUrp06Yv6fDg/phbYTOKpZOk8/TOJcUfI/qc7hfvlAADgSBoeLYsWLZLx48fL/v37/feVLFkyaCW9Lmpzu38/Fl122WUX9Dqio6PNmClcGlRkbSYqMuNb4vH6wv1SAABwLA2P1kWroVqFtW5/++23UqpUKfn444+ladOmEhMTI59//rn8+9//lu7du0ulSpVM0G3evLmsWbPmvK0F+rx//etf5Y477jALmGrXri3Lli3LtbXAagH45JNPpG7duubzdOzYMSh4ezweeeSRR8xxOili9OjRMmDAAOnRo8cFnYPZs2fLlVdeacL01VdfLX/729+CwrtWpatXr26+/ipVqpjPaXn11VfN1xIbG2vOx5133il2RJC1aZBN9/rES5gFAOCS+fOf/yzPPfec7Nu3Txo2bCinT5+Wzp07y9q1a+XLL780AbNr165y6NCh8z7PU089Jb1795avv/7a/Pm+ffvKsWPHcj1e56hOnTrVBMsNGzaY53/00Uf9jz///POyYMECmTdvnmzcuFGSkpJk6dKlF/S1LVmyRIYPHy5/+tOfZM+ePfLHP/5R7r//flm/fr15/O9//7u89NJL8tprr8mBAwfM8zdo0MA8tn37dhNqJ06caKrYK1eulBtvvFHsiNYCm3FH/raKL83rlZiI0GzhBgBAqGg171xaelhOaLGoyJCteNegdtttt/lvlytXTho1auS/PWnSJBMItcI6bNiwXJ/nvvvuk7vvvtt8/Oyzz8r06dNl69atJgjnREdOzZkzx1RLlT63vhbLjBkzZMyYMabKq2bOnCkfffTRBX1tU6dONa9ryJAh5vaoUaNk8+bN5v5bbrnFhGetTrdr185sGauV2RYtWphj9bESJUrI7bffbirXNWrUkCZNmogdEWRtJirityK5J90nMXyHAAA2oyG23vhPwvK5v5nYQYpHh+Yfx2bNmgXd1oqsvt3+z3/+07zVr2/xnzt37ncrslrNtWgAjIuL8+9clRNtQbBCrLW7lXX8yZMn5ejRo/5QqXTDAG2B8Hq9ef7a9u3bl21RWuvWreWVV14xH991112mReKKK64wgVsryVp91j5hDfcaXq3H9GK1TtgNrQU2ExVYkU3P+w8sAAC4MBo6A+nb+1qB1arqZ599Jrt27TJvt6empp73ebSiGUgrxucLnTkdr1XuglStWjXTNqC9sLrTllZutX1Aq8Vahd25c6e89957JmTrQjmtVF/qEWIXg3qfzURGBAZZFnwBAOxH397Xymi4Pvelov2o+na89Za+Vmh/+OEHKUi6ME0XV+mYL6svVScqaLBs3Lhxnp+nbt265uvRRWIWvV2vXj3/bQ2wWoXVy9ChQ6VOnTqye/duufbaa01lVtsO9PLkk0+ahWfr1q2Tnj17ip0QZG1GfyuLjoyQ1HQvFVkAgG3/rQrV2/t2oqv0P/zwQxPs9GscN27cBb2dHyoPP/ywTJ48Wa666ioTLrVn9vjx4xfUG/zYY4+ZBWja26phdPny5eZrs6Yw6PQEDcgtW7Y0LQPvvPOOCbbaUrBixQr5/vvvTZAuW7as6c/V86CTD+ym8P0UFpIFX6npGT2yAACgYEybNk0eeOABs4lBhQoVzNgrnRhQ0PTzJiQkSP/+/U1/rPa6dujQwXycVz169DD9sLq4S6cX1KpVy0xBuPnmm83jWmHViQ26CEwDrbZQaNjVcV/6mIZe7RdOTk42AV/bDOrXry924/IVdFOGA+kPsZb6tQFbG7gvtUZPrZKT59Jk7Z9ukisv+21gMwAABU2DzMGDB00Q0pmiKHhaDdVWAa2w6iSFwv5zlXQBuYuKrI0XfLHYCwCAoufHH3+UVatWyU033SQpKSlm/JaGvnvuuSfcL812mFpgQ+7MEVy0FgAAUPRERESYHlbdWUxHZukCLO1t1aosglGRtaEod0ZFVhd8AQCAokVHY+mEAdi8IqvbsunKQN3fV1finW/7tQcffNAcE7i/sdIt4HQrOO2h0ObkgQMHmnEZgXTLuBtuuMH0YOgPx5QpU8QJmyJQkQUAALBpkD1z5owZsDtr1qzzHqfDiXVbNQ28WWmI3bt3r6xevdqMi9BwHLiThTYMt2/f3oyT2LFjh7zwwgtmFd7cuXPFrqIirSBLRRYAAMCWrQWdOnUyl/P56aefzDy1Tz75RLp06ZJt+7WVK1eaocHWNnM6a023WdNxExp8FyxYYHbkePPNNyU6OtqMjtCdOnTERtat2+w0fkvRWgAAAODQxV46bqJfv35mqG9Os8s2bdpk2gkC90rWob/aJL1lyxb/MTrQV0OsRWex6bZsOlzY3hVZJqMBAAA4crHX888/b7ZIe+SRR3J8XIcFV6xYMeg+Pb5cuXLmMesYnVEWSLd+sx7THSuy0lEXerEU9DBkxm8BAAA4uCKr/ay6I4WOn7iQLdlCQbeF00G81kUXiIVj/Faal4osAACA44LsZ599JomJiVK9enVTZdWLDgj+05/+JDVr1jTHxMfHm2MCeTweM8lAH7OOOXr0aNAx1m3rmKzGjBljdpOwLocPH5aCFOXODLIeFnsBABBOuqXriBEj/Lc1g2SdoJTV701iyqtQPc/56AL4xo0bi1PZNshqb6yOzdKFWdZFF29pv6wu/FKtWrWSEydOmOqtZd26daa3tmXLlv5jdJJBWlqa/xidcHD11Vfn2FagYmJizDivwEtBiorIqEB7vARZAAAuho737NixY67FMg2JmjMulC4wD/Vi8dzC5M8///y7i+KLurD2yOq81//7v//z39bt1zSwao+rVmLLly8fdHxUVJSpomoIVbrDhf6QDho0SObMmWPC6rBhw6RPnz7+UV26ndtTTz1l5suOHj1a9uzZY1oWXnrpJbEra7FXGou9AAC4KPrvfq9eveQ///mPVK1aNeixefPmmYXiDRs2vODnveyyywrsO5LbO8ewSUV2+/bt0qRJE3NRo0aNMh+PHz8+z8+h47Xq1Kkjbdu2NWO32rRpEzQjVntcdb9iDclNmzY1rQn6/HYdvRU4fiuNObIAAFyU22+/3YROXWuTtYi2ePFiE3T/+9//yt133y2XX365FC9eXBo0aCDvvffeeZ83a2vBgQMHzHQk3XSpXr165l3frLSQ9j//8z/mc1xxxRUybtw4/zvF+vq04PbVV1+ZKrFerNectbVAt6q99dZbpVixYqbYN3jw4KBNoO677z7p0aOHGUFauXJlc8zQoUOD3pX+Pfqu9sSJE03413eotVKso04tOtJUi4b6/Po165x+XVukfD6fqS5rMVL/rBYVc1uwXygqstp3ol90Xv3www/Z7tPq7bvvvnveP6e/cenbCE7B+C0AAPJH19b079/fhMInnnjCv3BcQ2x6eroJsBoCtcilQVPbCP/5z3+a1sYrr7xSWrRokafQ17NnTzMNScd+6rqawH5aS6lSpczr0GCnYVTfSdb7Hn/8cfnDH/5g3i3WsLhmzRp/ES6nTaR0fKi2TGp7g64R+t///V8TKgPD+vr1603I1Gt911ufX8Oofs680HetX3zxRXnttddMcVHn8Hfr1s1sPlW7dm2ZPn26LFu2TN5//30TWHUdkbWW6O9//7t5x3vhwoVmbKpOh9KAXmTHbxVV1vgtNkQAANiSFqHSzobnc0cV11Jlng594IEHzI6en376qSmeWW0F2nJgTSZ69NFH/cdbGzBpSMtLkNXg+e2335o/Y7U0Pvvss9n6WseOHRtU0dXPqWFPg6xWV0uWLGmC9/laCbRol5ycLG+//baUKFHC3Ddz5kzTC6zjSq3Rorr+R++PjIw071jrZlJr167Nc5DVaq4Ge23TVPrcGoq1Cq07sR46dMgEWn0HXH850IqsRR/Tr0Fn+ms7qAbdvJzH/CDI2pCbDREAAHamIfbZ7NvGF4i/HBGJzghyv0eD3PXXX2+qihpktUKp79DqW+dKK7MaPDW46k6i+ra5zpHXFoC80B1GdUSnFWKVVkyzWrRokalk/vvf/zZVYJ2wdKELyfVzNWrUyB9iVevWrU1VWDd5soKsVkI1xFq0OqtV4LzQuflHjhwxzxtIb1uVVW1fuO2228x6JV2npC0c7du3N4/dddddJvBq+4Q+pi2fGrQ1pBe5qQVFWbR/sRdTCwAAyA/thdW3vE+dOmWqsdo2cNNNN5nHtFqrb6VrBVKrjrrgXN++10AbKrrDaN++fU2oW7FihXz55Zem1SGUnyOQVkIDadVUw26oXHvttWbd0aRJk+TcuXPSu3dvufPOO81jGuo1VL/66qum0jxkyBDTP3whPboXioqsDbkzx2+lMX4LAGBH+va+VkbD9bkvgAat4cOHm7fm9W35hx56yN8vu3HjRunevbvce++95rYGvu+++84s2soLnZ6k/aE6Jksrn2rz5s1Bx3zxxRfm7XcNrxadix8oOjraVId/73NpL6z2ylpV2Y0bN0pERIR/mlN+aZVYq8v6vFbYtz5PYIuAHqe9t3rREKvVV53hr+uWNMBqFVYvutBMq+JaEdYAfCkQZG3I2hDBw/gtAIAdaRDM49v74ab9pxq4dLMjfetc3xq3aK/nBx98YMKm9pZOmzbNbJqU1yCrvaA6jWDAgAGmuqvPHxhYrc+hvaPaE9u8eXOzoGzJkiVBx2jfrDWCVKcF6EIwXfUfSKu6Tz75pPlcOhngl19+MT29ujjNaisIBZ3Xr59HK9e6SEyr2Pq6dEqU0nOkoV0XgmmI1sVz2hdbpkwZE7Q1kOssf23PeOedd0ywDeyjDTVaC2zI2hCB1gIAAELTXnD8+HHTNhDYz6qLsLRSqPdrD60GMh1flVca5DSU6lvsWrHUKQLPPPNM0DG64n/kyJFmuoAGQw3NOn4rkC4+06rmLbfcYkaG5TQCTIOhLirTyqcGYq2Etm3b1izsCiUdl6XjUHVcqY4j02kKOqVAA7nSkD1lyhQzh1dfh06U+uijj8y50DD7+uuvm55anRili+GWL1+ebV+AUHL5LmT+VRGlv2HpykYdq1EQu3xNX3tApq3+Tu5uUV0m92xwyT8fAAC50ZXyWi2sVauWmRsKXOqfqwvJXVRkbb2zF4u9AAAAckOQtfEcWQ9BFgAAIFcEWTtXZL10fQAAAOSGIGtD7syKbJqH1gIAAIDcEGRtKCoic/wWFVkAAIBcEWRtKMrN+C0AgL0w5Ah2/HkiyNqQO7Miy9QCAEC4WVuenj17NtwvBYXI2cyfp6xb6l4odvay9fgtFnsBAMIrMjLSDLpPTEz0D+a3tngFLqYSqyFWf57050p/vvKDIGtDjN8CANiJ7nilrDAL5JeGWOvnKj8IsjZERRYAYCdaga1cubJUrFhR0tLSwv1y4HBRUVH5rsRaCLJ2Hr/FhggAABvR8BGqAAKEAou9bFyRZfwWAABA7giyNg6yqWyIAAAAkCuCrA25IzJaCzxedvYCAADIDUHWhqLdma0FjN8CAADIFUHWxhXZVBZ7AQAA5Ioga0P+xV5UZAEAAHJFkLX1HFl6ZAEAAHJDkLXxHFkdv6VbuQEAACA7gqyNK7IqjfYCAACAHBFkbSgqsyKrGMEFAACQM4KsDVGRBQAAsHmQ3bBhg3Tt2lWqVKkiLpdLli5d6n8sLS1NRo8eLQ0aNJASJUqYY/r37y9HjhwJeo5jx45J3759JS4uTsqUKSMDBw6U06dPBx3z9ddfyw033CCxsbFSrVo1mTJlijhh/JZiwRcAAIANg+yZM2ekUaNGMmvWrGyPnT17Vnbu3Cnjxo0z1x9++KHs379funXrFnSchti9e/fK6tWrZcWKFSYcDx482P94UlKStG/fXmrUqCE7duyQF154QSZMmCBz584Vu9JQ79/dix5ZAACAHLkljDp16mQuOSldurQJp4FmzpwpLVq0kEOHDkn16tVl3759snLlStm2bZs0a9bMHDNjxgzp3LmzTJ061VRxFyxYIKmpqfLmm29KdHS01K9fX3bt2iXTpk0LCrx2bC/weNOpyAIAABSGHtmTJ0+aaqW2EKhNmzaZj60Qq9q1aycRERGyZcsW/zE33nijCbGWDh06mOru8ePHxe4juGgtAAAAsGFF9kIkJyebntm7777b9MOqhIQEqVixYtBxbrdbypUrZx6zjqlVq1bQMZUqVfI/VrZs2WyfKyUlxVwC2xMKWrS1u5eXObIAAACOrcjqwq/evXubzQFmz559yT/f5MmTTWuDddEFYuGqyKZ62N0LAADAkUHWCrE//vij6Zm1qrEqPj5eEhMTg473eDxmkoE+Zh1z9OjRoGOs29YxWY0ZM8a0MViXw4cPS0FzR1CRBQAAcGyQtULsgQMHZM2aNVK+fPmgx1u1aiUnTpww0wgs69atE6/XKy1btvQfo5MM9LksGoivvvrqHNsKVExMjAnMgZeCFu3O+NbQIwsAAGDDIKvzXnWCgF7UwYMHzcc6lUCD55133inbt283kwfS09NNT6tedAqBqlu3rnTs2FEGDRokW7dulY0bN8qwYcOkT58+ZmKBuueee8xCL50vq2O6Fi1aJK+88oqMGjVK7Mwav0WQBQAAsOFiLw2pt9xyi/+2FS4HDBhgZr0uW7bM3G7cuHHQn1u/fr3cfPPN5mMNuRpe27Zta6YV9OrVS6ZPn+4/VntcV61aJUOHDpWmTZtKhQoVZPz48bYevRW4u1cac2QBAADsF2Q1jOoCrtyc7zGLTih49913z3tMw4YN5bPPPhMnicpc7OVJZ7EXAACA43pkizIqsgAAAOdHkLUpNkQAAAA4P4KszSuyHi+tBQAAADkhyNq9tcDDzl4AAAA5IcjaffwWFVkAAIAcEWRtKsraEIEtagEAAHJEkLWpqMyKrMdLawEAAEBOCLI25WZDBAAAgPMiyNp+jixTCwAAAHJCkLUpdvYCAAA4P4KszSuyqen0yAIAAOSEIGvznb08tBYAAADkiCBrU9H+nb2oyAIAAOTEneO9CJ+U0yL7P5L6v/4sIldJKhVZAACAHBFk7ebccZEPB0k7V7SIzKe1AAAAIBe0FthNZJS5ivB5zHUai70AAAByRJC1m8jozG+M11yYIwsAAJAzgqzdRPzW7RElHoIsAABALgiyNq3IKreki4fWAgAAgBwRZG3aI+uvyDJ+CwAAIEcEWbuJiBSRjM0QoiRd0jzecL8iAAAAWyLI2ri9QCuyHi9BFgAAICcEWRu3F7hd6ZJKjywAAECOCLI2DrKmIsvOXgAAADkiyNpRhBVkmVoAAACQG4KsjXtk3cyRBQAAyBVB1o4iMzZFiDbjt1jsBQAAkBOCrI1bC3RDhDSPL9yvBgAAwJYIsnYev+Vi/BYAAIAtg+yGDRuka9euUqVKFXG5XLJ06dKgx30+n4wfP14qV64sxYoVk3bt2smBAweCjjl27Jj07dtX4uLipEyZMjJw4EA5ffp00DFff/213HDDDRIbGyvVqlWTKVOmiBNaC3SxVyobIgAAANgvyJ45c0YaNWoks2bNyvFxDZzTp0+XOXPmyJYtW6REiRLSoUMHSU5O9h+jIXbv3r2yevVqWbFihQnHgwcP9j+elJQk7du3lxo1asiOHTvkhRdekAkTJsjcuXPFGRsi0FoAAACQk4zSX5h06tTJXHKi1diXX35Zxo4dK927dzf3vf3221KpUiVTue3Tp4/s27dPVq5cKdu2bZNmzZqZY2bMmCGdO3eWqVOnmkrvggULJDU1Vd58802Jjo6W+vXry65du2TatGlBgdeuPbIeNkQAAABwVo/swYMHJSEhwbQTWEqXLi0tW7aUTZs2mdt6re0EVohVenxERISp4FrH3HjjjSbEWrSqu3//fjl+/LjYe0ME3dnLa0I9AAAAbFSRPR8NsUorsIH0tvWYXlesWDHocbfbLeXKlQs6platWtmew3qsbNmy2T53SkqKuQS2J4RrZy+V7vWJO9JVsK8BAADA5mxbkQ2nyZMnm+qvddEFYmHZEMGVbq7TaC8AAABwTpCNj48310ePHg26X29bj+l1YmJi0OMej8dMMgg8JqfnCPwcWY0ZM0ZOnjzpvxw+fFgKVIQ7qCLLpggAAAAOCrLaDqBBc+3atUFv8Wvva6tWrcxtvT5x4oSZRmBZt26deL1e00trHaOTDNLS0vzH6ISDq6++Ose2AhUTE2PGeQVewtlawIIvAAAAmwVZnfeqEwT0Yi3w0o8PHTpk5sqOGDFCnn76aVm2bJns3r1b+vfvbyYR9OjRwxxft25d6dixowwaNEi2bt0qGzdulGHDhpmJBnqcuueee8xCL50vq2O6Fi1aJK+88oqMGjVKbCuztSDG31rANrUAAAC2Wuy1fft2ueWWW/y3rXA5YMAAmT9/vjz++ONm1qyOydLKa5s2bcy4Ld3YwKLjtTS8tm3b1kwr6NWrl5k9a9Ee11WrVsnQoUOladOmUqFCBbPJgm1HbwW0FkQTZAEAAHLl8jHb6XdpS4MGYu2XLZA2gxWjRLa/Ia/67pQpKT1l/aM3S60KJS795wUAAHBQ7rJtj2yRltkja1VkPbQWAAAAZEOQtXGQjYnI6I3VTREAAAAQjCBr4y1qrcVeTC0AAADIjiBrR9b4LSvIeqnIAgAAZEWQtXNrQWaQTfX4wvyCAAAA7Icga+PWAv+GCFRkAQAAsiHI2nhDhOgINkQAAADIDUHWjvxb1FpBltYCAACArAiyDgiyTC0AAADIjiBr5x5ZV0aPbBpzZAEAALIhyNq4R9btby1g/BYAAEBWBFk7inQHTS2gRxYAACA7gqwdMX4LAADgdxFkbd1akFGRTfXQWgAAAJAVQdbGrQVun7VFLeO3AAAAsiLIOqAi62GxFwAAQDYEWRv3yEb6MlsL2BABAAAgG4KsjTdEiKQiCwAAkCuCrI2DrDuzIsscWQAAgOwIsjZuLYjwB1kWewEAAGRFkLVzawEVWQAAgFwRZB0QZD1UZAEAALIhyNp4/FaEN81cp3nZEAEAACArgqyde2TFay70yAIAAGRHkLVxa4GKEg8bIgAAAOSAIGvzIOuWdMZvAQAA5IAga+Me2d+CLOO3AAAAsiLI2lFEpIi4zIfR2lrAYi8AAIBsCLJ2391LK7IeKrIAAACOCrLp6ekybtw4qVWrlhQrVkyuvPJKmTRpkvh8vwU7/Xj8+PFSuXJlc0y7du3kwIEDQc9z7Ngx6du3r8TFxUmZMmVk4MCBcvr0aXFCe0GUy8P4LQAAAKcF2eeff15mz54tM2fOlH379pnbU6ZMkRkzZviP0dvTp0+XOXPmyJYtW6REiRLSoUMHSU5O9h+jIXbv3r2yevVqWbFihWzYsEEGDx4sthbh9k8tSEtnjiwAAEBWGWnJpr744gvp3r27dOnSxdyuWbOmvPfee7J161Z/Nfbll1+WsWPHmuPU22+/LZUqVZKlS5dKnz59TABeuXKlbNu2TZo1a2aO0SDcuXNnmTp1qlSpUkVsXZGVdHb2AgAAcFpF9vrrr5e1a9fKd999Z25/9dVX8vnnn0unTp3M7YMHD0pCQoJpJ7CULl1aWrZsKZs2bTK39VrbCawQq/T4iIgIU8G1f4+sR1KpyAIAAISmInv48GFxuVxStWpVc1srpO+++67Uq1cvpG/Z//nPf5akpCSpU6eOREZGmp7ZZ555xrQKKA2xSiuwgfS29ZheV6xYMehxt9st5cqV8x+TVUpKirlY9DWEK8hSkQUAAAhhRfaee+6R9evXm481DN52220mzD7xxBMyceJECZX3339fFixYYELyzp075a233jLtAHp9KU2ePNlUdq1LtWrVJFzb1LKzFwAAQAiD7J49e6RFixb+sHnNNdeYflYNnfPnz5dQeeyxx0xVVntdGzRoIP369ZORI0eaoKni4+PN9dGjR4P+nN62HtPrxMTEoMc9Ho+ZZGAdk9WYMWPk5MmT/otWoMPVI+t2pUsqGyIAAACEJsimpaVJTEyM+XjNmjXSrVs387G2APz8888SKmfPnjW9rIG0xcCbuUGAjuXSMKp9tIFtANr72qpVK3Nbr0+cOCE7duzwH7Nu3TrzHNpLmxP92nRUV+ClwEVmdH2wIQIAAEAIe2Tr169vxl3pNAEdaaWzXdWRI0ekfPnyEipdu3Y1PbHVq1c3n/PLL7+UadOmyQMPPGAe1z7dESNGyNNPPy21a9c2wVbnzuokgh49ephj6tatKx07dpRBgwaZ16whfNiwYabKa9uJBQGtBRkbIjB+CwAAICRBVue53nHHHfLCCy/IgAEDpFGjRub+ZcuW+VsOQkHHZGkwHTJkiGkP0OD5xz/+0WyAYHn88cflzJkzZpGZVl7btGljxm3Fxsb6j9GWBw2vbdu2NRXeXr16mdmztuYfv6UbIrCzFwAAQFYuX+A2WRdAJwjo2/hly5b13/fDDz9I8eLFs00JcDr9OnXRl/bLFlibwVtdRQ5ukEdSh8kK3/Xy/eSMWboAAACFWdIF5K6L6pE9d+6cGU9lhdgff/zRbEywf//+Qhdi7VCR1YJsOlVZAACA/AdZ3UVLd9BS+na+Lpp68cUXTV+qbimLEPbIutLNNdvUAgAAhCDI6kzXG264wXz8wQcfmA0ItCqr4db2vadO4d8QwWOuCbIAAAAhCLI6FqtUqVLm41WrVknPnj3NIqrrrrvOBFqEPsh6mCULAACQ/yB71VVXydKlS81GAZ988om0b9/e3K+TBcIyc7Uwsnb2orUAAAAgdEFWx189+uijUrNmTTNuy9p8QKuzTZo0uZinRC4V2VgryLLYCwAAIP9zZO+8804zr1V38bJmyCqd06rzZRG6IBudGWQ96WyKAAAAkO8gq3RrWL385z//MberVq0a0s0QirzM8VuxEUwtAAAACFlrgdfrlYkTJ5phtTVq1DCXMmXKmK1q9TGEQETG7xjRrozzmcZiLwAAgPxXZJ944gl544035LnnnpPWrVub+z7//HOZMGGCJCcnyzPPPHMxT4scKrLRVGQBAABCF2Tfeust+etf/yrdunXz39ewYUO5/PLLZciQIQTZEPbIxvinFlzUTsIAAACF1kW1Fhw7dkzq1KmT7X69Tx9DCOfIstgLAAAgdEFWJxXMnDkz2/16n1ZmEbo5sjH+nb2oyAIAAOS7tWDKlCnSpUsXWbNmjX+G7KZNm8wGCR999NHFPCV+pyKbxiI6AACA/Fdkb7rpJvnuu+/MzNgTJ06Yi25Tu3fvXvnb3/52MU+J3BZ7WUHWwzQIAACAkMyRrVKlSrZFXV999ZWZZjB37tyLfVpkGb8VJZkbIrCzFwAAQP4rsii4imyUy+qRpSILAAAQiCBr8x5Zd2ZFlsVeAAAAwQiydl/slTm1wENFFgAA4OJ7ZHVB1/nooi+EdvyW2z9+i9YCAACAiw6ypUuX/t3H+/fvfyFPidzQWgAAABC6IDtv3rwLORyhaC3wUZEFAADICT2yNm8tiLR6ZBm/BQAAEIQga/PxW24qsgAAADkiyNpVpDuoIstiLwAAgGAEWZtXZCN9mTt7pfvC/IIAAADshSBr9x5ZX5q5TmX8FgAAQBCCrM2nFkRm9shSkQUAAAhGkHVIkKVHFgAAwGFB9qeffpJ7771XypcvL8WKFZMGDRrI9u3b/Y/7fD4ZP368VK5c2Tzerl07OXDgQNBzHDt2TPr27StxcXFSpkwZGThwoJw+fVqc0FoQ4Q+y9MgCAAA4JsgeP35cWrduLVFRUfLxxx/LN998Iy+++KKULVvWf8yUKVNk+vTpMmfOHNmyZYuUKFFCOnToIMnJyf5jNMTu3btXVq9eLStWrJANGzbI4MGDxQmLvSJ86eISr3i8bFELAABw0Tt7FbTnn39eqlWrFrSjWK1atYKqsS+//LKMHTtWunfvbu57++23pVKlSrJ06VLp06eP7Nu3T1auXCnbtm2TZs2amWNmzJghnTt3lqlTp0qVKlXEzuO3VJSk01oAAADgpIrssmXLTPi86667pGLFitKkSRN5/fXX/Y8fPHhQEhISTDuBpXTp0tKyZUvZtGmTua3X2k5ghVilx0dERJgKrt0rsipKPLQWAAAAOCnIfv/99zJ79mypXbu2fPLJJ/LQQw/JI488Im+99ZZ5XEOs0gpsIL1tPabXGoIDud1uKVeunP+YrFJSUiQpKSnoEq4eWeWmIgsAAOCs1gKv12sqqc8++6y5rRXZPXv2mH7YAQMGXLLPO3nyZHnqqackrCIiRcSlDRSmtYDxWwAAAA6qyOokgnr16gXdV7duXTl06JD5OD4+3lwfPXo06Bi9bT2m14mJiUGPezweM8nAOiarMWPGyMmTJ/2Xw4cPS4FzufwjuLS1gA0RAAAAHBRkdWLB/v37g+777rvvpEaNGv6FXxpG165d639c2wC097VVq1bmtl6fOHFCduzY4T9m3bp1ptqrvbQ5iYmJMaO6Ai9hkdle4HZ5xMPOXgAAAM5pLRg5cqRcf/31prWgd+/esnXrVpk7d665KJfLJSNGjJCnn37a9NFqsB03bpyZRNCjRw9/Bbdjx44yaNAg05KQlpYmw4YNMxMNbDuxwKIV2TSRaPGIx8scWQAAAMcE2ebNm8uSJUvMW/0TJ040QVXHbelcWMvjjz8uZ86cMXNhtfLapk0bM24rNjbWf8yCBQtMeG3btq2ZVtCrVy8ze9b2MlsLdLFXqoc5sgAAAIFcPh3GivPSdgUd66X9sgXaZjCtnkjST3J7ytOSfFlDWTPqpoL73AAAADbPXbbukS3yIjIK5myIAAAAkB1B1s4yN0XQqQWM3wIAAAhGkHVCj6yLLWoBAACyIsjamX+OLEEWAAAgK4KsnWXOkaW1AAAAIDuCrFPGb7EhAgAAQBCCrAOCLBsiAAAAZEeQtTNri1rxSLrXJ1529wIAAPAjyDph/JYr3VynedndCwAAwEKQtbNIa0MEj7lmliwAAMBvCLIOqMjqYi+VxoIvAAAAP4KsQ8ZvqbR0X5hfEAAAgH0QZB3QWhAbkdEbS0UWAADgNwRZB7QWxLjokQUAAMiKIOuA1oJoqyLL1AIAAAA/gqwDNkSItcZvsdgLAADAjyDrgCBrzZFl/BYAAMBvCLJO6JGNyOiRTaUiCwAA4EeQdUKPrGT0yFKRBQAA+A1B1gGtBdH0yAIAAGRDkHVEj6y1IUJGZRYAAAAEWWfs7MViLwAAgGyoyDqhtUAYvwUAAJAVQdYJrQVWkPX6wvyCAAAA7IMg64DxW/4eWQ89sgAAABaCrJ1FuM2V25cRZD1sUQsAAOBHkHVARdbtH79FawEAAICFIOuEHtnMiizjtwAAAH5DkHVCa0HmYi929gIAAHBokH3uuefE5XLJiBEj/PclJyfL0KFDpXz58lKyZEnp1auXHD16NOjPHTp0SLp06SLFixeXihUrymOPPSYeT0aV0xGtBZJmrlPZEAEAAMB5QXbbtm3y2muvScOGDYPuHzlypCxfvlwWL14sn376qRw5ckR69uzpfzw9Pd2E2NTUVPniiy/krbfekvnz58v48ePFKa0FkT4qsgAAAI4MsqdPn5a+ffvK66+/LmXLlvXff/LkSXnjjTdk2rRpcuutt0rTpk1l3rx5JrBu3rzZHLNq1Sr55ptv5J133pHGjRtLp06dZNKkSTJr1iwTbp0QZK2KLD2yAAAADguy2jqgVdV27doF3b9jxw5JS0sLur9OnTpSvXp12bRpk7mt1w0aNJBKlSr5j+nQoYMkJSXJ3r17xQlb1FoV2TTGbwEAAPhlrCaysYULF8rOnTtNa0FWCQkJEh0dLWXKlAm6X0OrPmYdExhircetx3KSkpJiLhYNveHskY30ZVRkWewFAADgkIrs4cOHZfjw4bJgwQKJjY0tsM87efJkKV26tP9SrVo1CW+PLOO3AAAAHBVktXUgMTFRrr32WnG73eaiC7qmT59uPtbKqva5njhxIujP6dSC+Ph487FeZ51iYN22jslqzJgxpv/WumigDmeQjcgMsqlsUQsAAOCMINu2bVvZvXu37Nq1y39p1qyZWfhlfRwVFSVr1671/5n9+/ebcVutWrUyt/Van0MDsWX16tUSFxcn9erVy/HzxsTEmMcDL+Htkc0IsqdSHDAyDAAAoIDYuke2VKlScs011wTdV6JECTMz1rp/4MCBMmrUKClXrpwJnA8//LAJr9ddd515vH379iaw9uvXT6ZMmWL6YseOHWsWkGlgtTV/RTZdXOKVpHMZvbIAAACweZDNi5deekkiIiLMRgi6QEsnErz66qv+xyMjI2XFihXy0EMPmYCrQXjAgAEyceJEsb3MIKuiJF1OJVORBQAAsLh8Pp/Pfws50qkFuuhL+2ULtM0g9azIs5XNh/WS35TKl5WXtX+6me8SAAAotC4kd9m6R7bIyxy/paLEQ0UWAAAgAEHWziIi/R/SWgAAABCMIGtnLpd/UwS3eORcWjrb1AIAAGQiyNpd5giuKFfmCC4WfAEAABgEWbuLzBgsUSpzgMGpZEZwAQAAKIKs3WW2FpSJzhguQUUWAAAgA0HWIa0FcTEuc51ERRYAAMAgyDpkUwSrIpt0jk0RAAAAFEHWIUGWHlkAAIBgBFmH9MiWivaaa3pkAQAAMhBk7S4i69QCWgsAAABMTOI0OKMiW9Kd2SPLYi8AAACDIOuQHtmSUdb4LebIAgAAKIKsQ1oLSrjpkQUAAAhEkHVIa0HxSIIsAABAIIKsQ1oLikfSWgAAABCIIOuYIJtRkU1iagEAAIBBkHXIFrWx/tYCFnsBAACYmMRpcEaPbGxEurmmIgsAAJCBIGt3kRlTC2IjMiqyqR6vpHgyQi0AAEBRRpB1SGtBtOu38MruXgAAAARZx7QWRHjTpFRMRnU26Rx9sgAAAFRkHTK1QDTIxmYEWSqyAAAAVGSdE2TTNchmfEyQBQAAIMg6pkc2I8haFVlaCwAAAGgtcEiPrKSn0loAAAAQgCDrkPFb4vVIXLGM6mwSFVkAAACCrHMqsr+1FrApAgAAAEHWQT2yqQGLveiRBQAAsHVrweTJk6V58+ZSqlQpqVixovTo0UP2798fdExycrIMHTpUypcvLyVLlpRevXrJ0aNHg445dOiQdOnSRYoXL26e57HHHhOPxyNOay1g/BYAAIBDguynn35qQurmzZtl9erVkpaWJu3bt5czZ874jxk5cqQsX75cFi9ebI4/cuSI9OzZ0/94enq6CbGpqanyxRdfyFtvvSXz58+X8ePHi/MWe2X2yLIhAgAAgGSW++xp5cqVQbc1gGpFdceOHXLjjTfKyZMn5Y033pB3331Xbr31VnPMvHnzpG7duib8XnfddbJq1Sr55ptvZM2aNVKpUiVp3LixTJo0SUaPHi0TJkyQ6OjMoOiA8VtxbIgAAADgjIpsVhpcVbly5cy1Blqt0rZr185/TJ06daR69eqyadMmc1uvGzRoYEKspUOHDpKUlCR79+4VJ22IEGf1yKbQIwsAAGDrimwgr9crI0aMkNatW8s111xj7ktISDAV1TJlygQdq6FVH7OOCQyx1uPWYzlJSUkxF4uG3rBhi1oAAABnV2S1V3bPnj2ycOHCAllkVrp0af+lWrVqYqceWbaoBQAAcEiQHTZsmKxYsULWr18vVatW9d8fHx9vFnGdOHEi6HidWqCPWcdknWJg3baOyWrMmDGmjcG6HD58WMImIrNonq4bImTOkT2XJj6fL3yvCQAAwAZsHWQ1rGmIXbJkiaxbt05q1aoV9HjTpk0lKipK1q5d679Px3PpuK1WrVqZ23q9e/duSUxM9B+jExDi4uKkXr16OX7emJgY83jgxR6tBRkfe7w+SU7zhu81AQAA2IDb7u0EOpHgH//4h5kla/W06tv9xYoVM9cDBw6UUaNGmQVgGjgffvhhE151YoHScV0aWPv16ydTpkwxzzF27Fjz3BpYbS+gtaBEdKREuES8voxNEYpFR4b71QEAAISNrSuys2fPNm/t33zzzVK5cmX/ZdGiRf5jXnrpJbn99tvNRgg6kkvbBT788EP/45GRkaYtQa814N57773Sv39/mThxojiCf/yWR1wul5SMYZtaAAAA21dk89IHGhsbK7NmzTKX3NSoUUM++ugjcST/+K1Uc6XtBUnJHrapBQAARZ6tK7II7pFVccUyd/dKdsgWuwAAAJcIQdYxPbIZQbaUf3cvNkUAAABFG0HW7vzjtzIrsmxTCwAAYBBknVKRzWwt+G1TBCqyAACgaCPIOqZH1qOr3/ytBUnn6JEFAABFG0HWKa0FKj1N4qjIAgAAGARZp7QWqPTUgMVeVGQBAEDRRpB1SmtBlm1qGb8FAACKOoKsw1oLGL8FAACQgSBrdy5XwDa1aWyIAAAAkIkg66hNEQJ7ZBm/BQAAijaCrBNEZrYXeD1siAAAAJCJIOuwbWqtxV6nUzzi8/nC+7oAAADCiCDrBP4e2d9aC9K9Pjmbmh7e1wUAABBGBFmHtRYUi4oUd4TL3EyiTxYAABRhBFmHLfZyuVxsigAAAECQdYiA8VvK6pNlcgEAACjKqMg6aXcvf5DNaDVgdy8AAFCUEWSdFGS9wUH2VLInnK8KAAAgrAiyDhu/peIyWwuSzrEpAgAAKLoIsk4QkTm1ID01S48sFVkAAFB0EWQd1VqQEVzZphYAAIAg67jxWyqOHlkAAAAqss5qLcjskS2W2SPLhggAAKAIo7XAgYu9mFoAAABAkHXo+C02RAAAAKAi66gNEaypBcyRBQAAIMg6aotaa2oB47cAAAAIsk7qkc1sLbCmFrAhAgAAKMqKVJCdNWuW1KxZU2JjY6Vly5aydetWcWZrQcbt06ke8Xp94XxlAAAAYVNkguyiRYtk1KhR8uSTT8rOnTulUaNG0qFDB0lMTBTnjN8K3hDB58sIswAAAEVRkQmy06ZNk0GDBsn9998v9erVkzlz5kjx4sXlzTffFMe0Fpz4UeTcCYmNipToyIxvHdvUAgCAoiqz1Fe4paamyo4dO2TMmDH++yIiIqRdu3ayadMmsb3Y0hnX364Q2f+xSI3rZUh0TdmQfKUsWnpKqpcrIRVKRku5ktES647M9sddrgJ+vQX+CbO9gDC/pAL++nP44i7lK/Dl+PkK7mt2hf3nKzsbvqS8cxWZegaAfCpVLl4qxFcTOykSQfbXX3+V9PR0qVSpUtD9evvbb7/NdnxKSoq5WJKSkiSsru0ncvbXjBD7y7ciP3wmI+QzGREjIj9kXgAAAC6hTZffLxUGvSx2UiSC7IWaPHmyPPXUU2Krimy7CRmXYwdFDqyS5L0fiSdxv6R7fRkXn88s/NK+2dCw3yIyV5hfU7g/f2H6eovauQwlzh3nEgib6OK2O/lFIshWqFBBIiMj5ejRo0H36+34+Phsx2sLgi4MC6zIVqtmk1J6uVoiLf8osS3/GO5XAgAAipBWYj9FojkqOjpamjZtKmvXrvXf5/V6ze1WrbJ/W2JiYiQuLi7oAgAAAHspEhVZpRXWAQMGSLNmzaRFixby8ssvy5kzZ8wUAwAAADhPkQmyf/jDH+SXX36R8ePHS0JCgjRu3FhWrlyZbQEYAAAAnMHl84VueVBhpT2ypUuXlpMnT9JmAAAAYJPcVSR6ZAEAAFD4EGQBAADgSARZAAAAOBJBFgAAAI5EkAUAAIAjEWQBAADgSARZAAAAOBJBFgAAAI5UZHb2yg9rzwgd0AsAAIBLx8pbedmziyCbB6dOnTLX1apVy+/3BgAAAHnMX7rD1/mwRW0eeL1eOXLkiJQqVUpcLpcUxG8iGpoPHz7MlrgFgPNd8DjnnPPCjp9xznlhl3QJs4pWYjXEVqlSRSIizt8FS0U2D/QkVq1aVQqa/mCE+ocDnG874Wecc17Y8TPOOS/s4i5RVvm9SqyFxV4AAABwJIIsAAAAHIkga0MxMTHy5JNPmmtwvgsjfsY554UdP+Oc88IuxiZZhcVeAAAAcCQqsgAAAHAkgiwAAAAciSALAAAARyLI2sysWbOkZs2aEhsbKy1btpStW7eG+yUVGpMnT5bmzZubjS0qVqwoPXr0kP379wcdk5ycLEOHDpXy5ctLyZIlpVevXnL06NGwvebC5LnnnjMbiowYMcJ/H+c79H766Se59957zc9wsWLFpEGDBrJ9+/agQePjx4+XypUrm8fbtWsnBw4cuASvpPBLT0+XcePGSa1atcy5vPLKK2XSpElB22pyvvNnw4YN0rVrVzMYX//+WLp0adDjeTm/x44dk759+5pZp2XKlJGBAwfK6dOn8/nKit75TktLk9GjR5u/U0qUKGGO6d+/v9kwKpznmyBrI4sWLZJRo0aZVYA7d+6URo0aSYcOHSQxMTHcL61Q+PTTT01I3bx5s6xevdr8T9m+fXs5c+aM/5iRI0fK8uXLZfHixeZ4/R+0Z8+eYX3dhcG2bdvktddek4YNGwbdz/kOrePHj0vr1q0lKipKPv74Y/nmm2/kxRdflLJly/qPmTJlikyfPl3mzJkjW7ZsMf8g6d8z+ksFLszzzz8vs2fPlpkzZ8q+ffvMbT2/M2bM4HyHiP79rP8WapEnJ3n5edZQtXfvXvP3/ooVK0xYGzx4cKheYpE532fPnjXZRH950+sPP/zQFIO6desWdFyBn28fbKNFixa+oUOH+m+np6f7qlSp4ps8eXJYX1dhlZiYqGUT36effmpunzhxwhcVFeVbvHix/5h9+/aZYzZt2hTGV+psp06d8tWuXdu3evVq30033eQbPny4uZ/zHXqjR4/2tWnTJtfHvV6vLz4+3vfCCy/479PvQ0xMjO+99967BK+ocOvSpYvvgQceCLqvZ8+evr59+5qPOd+hpX8XL1myxH87L+f3m2++MX9u27Zt/mM+/vhjn8vl8v30008hfoWF+3znZOvWrea4H3/8MWznm4qsTaSmpsqOHTvM2yKBW+Pq7U2bNoX1tRVWJ0+eNNflypUz13r+tUob+D2oU6eOVK9ene9BPmgVvEuXLkHnlfN9aSxbtkyaNWsmd911l2mfadKkibz++uv+xw8ePCgJCQlB3wvdBlLbmPh75sJdf/31snbtWvnuu+/M7a+++ko+//xz6dSpE+e7AOTl51mv9e1t/f/Cosfrv69awUX+/x3VFgQ9x+E63+5L8qy4YL/++qvpt6pUqVLQ/Xr722+/5YyGmNfrNb2a+jbsNddcY+7TvxCjo6P9/0MGfg/0MVy4hQsXmregtLUgK8536H3//ffmrW5tUfrLX/5izvsjjzxifq4HDBjg/znO6e8ZfsYv3J///GdJSkoyv/BGRkaav8OfeeYZ89aq4nxfWnk5v3qtv9QFcrvdpoDBz3z+aPuG9szefffdph82XOebIIsiWyXcs2ePqZ7g0jh8+LAMHz7c9Enp4kUUzC9oWgl59tlnzW2tyOrPufYPapBFaL3//vuyYMECeffdd6V+/fqya9cu8wuyLoLhfKMwS0tLk969e5vFdvrLczjRWmATFSpUML/RZ10hr7fj4+PD9roKo2HDhpkG9PXr10vVqlX99+t51haPEydOBB3P9+DiaKuGLlS89tprzW/ketEFdLowQz/WqgnnO7R05Xa9evWC7qtbt64cOnTIfGz9XcLfM6Hx2GOPmapsnz59zErufv36mQWMOiGF833p5eXnWa+zLpj2eDxmZT3/tuYvxP7444+mUGFVY8N1vgmyNqFv/TVt2tT0WwVWV/R2q1atwvraCgv9zVFD7JIlS2TdunVmZE4gPf+62jvwe6ArMjUE8D24cG3btpXdu3ebKpV10Wqhvu1qfcz5Di1tlck6Uk77N2vUqGE+1p95/cck8Gdc3xrX3jV+xi+cruLW3r9AWpDQv7s535deXn6e9VqLE/qLtUX//tfvkfbS4uJCrI44W7NmjRnzFygs5/uSLCHDRVm4cKFZbTl//nyz8m/w4MG+MmXK+BISEjijIfDQQw/5Spcu7fvXv/7l+/nnn/2Xs2fP+o958MEHfdWrV/etW7fOt337dl+rVq3MBaEROLWA8x16uoLY7Xb7nnnmGd+BAwd8CxYs8BUvXtz3zjvv+I957rnnzN8r//jHP3xff/21r3v37r5atWr5zp07dwleUeE2YMAA3+WXX+5bsWKF7+DBg74PP/zQV6FCBd/jjz/uP4bznf+pJ19++aW5aGSZNm2a+dhaJZ+X89uxY0dfkyZNfFu2bPF9/vnnZorK3Xffnc9XVvTOd2pqqq9bt26+qlWr+nbt2hX072hKSkrYzjdB1mZmzJhhglR0dLQZx7V58+Zwv6RCQ/+nzOkyb948/zH6l9+QIUN8ZcuWNQHgjjvuMP+T4tIEWc536C1fvtx3zTXXmF+K69Sp45s7d27Q4zqyaNy4cb5KlSqZY9q2bevbv3//JXglhV9SUpL5eda/s2NjY31XXHGF74knngj6R53znT/r16/P8e9t/SUir+f3v//9rwlSJUuW9MXFxfnuv/9+E9hwYedbf1nL7d9R/XPhOt8u/c+lqfUCAAAAlw49sgAAAHAkgiwAAAAciSALAAAARyLIAgAAwJEIsgAAAHAkgiwAAAAciSALAAAARyLIAgAAwJEIsgCAIC6XS5YuXcpZAWB7BFkAsJH77rvPBMmsl44dO4b7pQGA7bjD/QIAAME0tM6bNy/ovpiYGE4TAGRBRRYAbEZDa3x8fNClbNmy5jGtzs6ePVs6deokxYoVkyuuuEI++OCDoD+/e/duufXWW83j5cuXl8GDB8vp06eDjnnzzTelfv365nNVrlxZhg0bFvT4r7/+KnfccYcUL15cateuLcuWLSuArxwALgxBFgAcZty4cdKrVy/56quvpG/fvtKnTx/Zt2+feezMmTPSoUMHE3y3bdsmixcvljVr1gQFVQ3CQ4cONQFXQ6+G1Kuuuiroczz11FPSu3dv+frrr6Vz587m8xw7dqzAv1YAOB+Xz+fznfcIAECB9si+8847EhsbG3T/X/7yF3PRiuyDDz5owqjluuuuk2uvvVZeffVVef3112X06NFy+PBhKVGihHn8o48+kq5du8qRI0ekUqVKcvnll8v9998vTz/9dI6vQT/H2LFjZdKkSf5wXLJkSfn444/p1QVgK/TIAoDN3HLLLUFBVZUrV87/catWrYIe09u7du0yH2tltlGjRv4Qq1q3bi1er1f2799vQqoG2rZt2573NTRs2ND/sT5XXFycJCYm5vtrA4BQIsgCgM1ocMz6Vn+oaN9sXkRFRQXd1gCsYRgA7IQeWQBwmM2bN2e7XbduXfOxXmvvrLYDWDZu3CgRERFy9dVXS6lSpaRmzZqydu3aAn/dABBqVGQBwGZSUlIkISEh6D632y0VKlQwH+sCrmbNmkmbNm1kwYIFsnXrVnnjjTfMY7oo68knn5QBAwbIhAkT5JdffpGHH35Y+vXrZ/pjld6vfbYVK1Y00w9OnTplwq4eBwBOQpAFAJtZuXKlGYkVSKup3377rX+iwMKFC2XIkCHmuPfee0/q1atnHtNxWZ988okMHz5cmjdvbm7rhINp06b5n0tDbnJysrz00kvy6KOPmoB85513FvBXCQD5x9QCAHAQ7VVdsmSJ9OjRI9wvBQDCjh5ZAAAAOBJBFgAAAI5EjywAOAh72ADAb6jIAgAAwJEIsgAAAHAkgiwAAAAciSALAAAARyLIAgAAwJEIsgAAAHAkgiwAAAAciSALAAAARyLIAgAAQJzo/wFqETfumKhF0gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55d6967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
