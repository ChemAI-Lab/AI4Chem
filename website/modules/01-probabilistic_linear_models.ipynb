{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cdfc7ec",
   "metadata": {},
   "source": [
    "# Probabilistic Linear Models\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChemAI-Lab/AI4Chem/blob/main/website/modules/01-probabilistic_linear_models.ipynb)\n",
    "\n",
    "\n",
    "**References:**\n",
    "1. **Chapters 3**: [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), C. M. Bishop.\n",
    "2. **Chapter 2**:  [Gaussian Processes for Machine LearningOpen Access](https://direct.mit.edu/books/oa-monograph-pdf/2514321/book_9780262256834.pdf), C. E. Rasmussen, C. K. I. Williams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f7bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, FloatSlider, IntSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361af3b6",
   "metadata": {},
   "source": [
    "# Bayesian Regression\n",
    "\n",
    "So far we have considered linear models of the form,\n",
    "$$\n",
    "f(\\mathbf{\\phi}(\\mathbf{x}),\\mathbf{w}) = \\mathbf{w}^\\top \\mathbf{\\phi}(\\mathbf{x})= \\sum_{i}^d w_i \\, \\phi_i(\\mathbf{x}),\n",
    "$$\n",
    "where the parameters $\\mathbf{w}$ are optimized either through, \n",
    "1. Mean Square Error function,\n",
    "   $$\n",
    "    {\\cal L}(\\mathbf{w}) = \\|\\mathbf{y} - \\Phi(\\mathbf{X})\\mathbf{w} \\|_2^2  = \\frac{1}{2n}\\sum_{i}^{n} (\\hat{y}_i - \\mathbf{w}^\\top\\phi(\\mathbf{x}_i))^2 \n",
    "    $$\n",
    "2. Ridge Regression function\n",
    "    $$\n",
    "    {\\cal L}(\\mathbf{w}) = \\|\\mathbf{y} - \\Phi(\\mathbf{X})\\mathbf{w} \\|_2^2 + \\lambda \\|\\mathbf{w}\\|_2^2 = \\frac{1}{2n}\\sum_{i}^{n} (\\hat{y}_i - \\mathbf{w}^\\top\\phi(\\mathbf{x}_i))^2 + \\lambda \\sum_j^d w_j^2\n",
    "    $$\n",
    "\n",
    "For both methods procedure a single best-fit line, but provides no notion of uncertainty or noise.\n",
    "So far, we treated $\\mathbf{w}$ as an unknown but fixed parameter, meaning it has a well determined value.\n",
    "One could argue that this should not be the case, and $\\mathbf{w}$ could be parameterized by a probability distribution. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e0eaa",
   "metadata": {},
   "source": [
    "# Bayesian Regression\n",
    "\n",
    "In Bayesian Regression we aim to find the distribution of the model's parameters condition on some data or information. For this we will use Bayes's rule\n",
    "$$\n",
    "p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}) \\propto p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}) p(\\mathbf{w}),\n",
    "$$\n",
    "where\n",
    "* $p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X})$ is the **likelihood**\n",
    "* $p(\\mathbf{w})$ is the **prior**\n",
    "\n",
    "## Likelihood of the Data\n",
    "Under the Gaussian noise assumption, the conditional distribution of each observation is\n",
    "$$\n",
    "p(y_i|\\mathbf{w}, \\mathbf{x}_i) = {\\cal N}(y_i|\\mathbf{w}^\\top\\mathbf{x}_i, \\sigma^2).\n",
    "$$\n",
    "Assuming i.i.d. data, the likelihood of the full dataset is\n",
    "$$\n",
    "p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}) = \\prod_i^N p(y_i|\\mathbf{w}, \\mathbf{x}_i) = \\prod_i^N {\\cal N}(y_i|\\mathbf{w}^\\top\\mathbf{x}_i, \\sigma^2).\n",
    "$$\n",
    "\n",
    "Taking the logarithm of the likelihood gives, \n",
    "$$\n",
    "\\log p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}) = \\frac{N}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_i^N \\left ( y_i - \\mathbf{w}^\\top\\mathbf{x}_i \\right )^2.\n",
    "$$\n",
    "Besides the constants, $\\log p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X})$ is the same error function as the MSE.\n",
    "\n",
    "$$\n",
    "\\arg\\max_{\\mathbf{w}}\\, \\log p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}) \\Longleftrightarrow\\, \\arg\\min_{\\mathbf{w}} \\frac{1}{2N} \\sum_i^N \\left ( y_i - \\mathbf{w}^\\top\\mathbf{x}_i \\right )^2\n",
    "$$\n",
    "\n",
    "* Minimizing the mean squared error is equivalent to maximum likelihood estimation under a Gaussian noise model.\n",
    "1. MSE is not arbitrary.\n",
    "2. It corresponds to a specific probabilistic assumption.\n",
    "3. Linear regression is already a probabilistic model in disguise.\n",
    "\n",
    "## Prior Distribution on the Weights\n",
    "\n",
    "In the Bayesian framework, model parameters are treated as random variables rather than fixed but unknown quantities.\n",
    "\n",
    "Instead of estimating a single value of $\\mathbf{w}$, we encode our beliefs about plausible values of $\\mathbf{w}$ before seeing the data using a prior distribution: \n",
    "$$\n",
    "p(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "\n",
    "**Gaussian Prior on the Weights**:\n",
    "A common and mathematically convenient choice is a Gaussian prior:\n",
    "$$\n",
    "p(\\mathbf{w}) = {\\cal N}(\\mathbf{w}|\\mathbf{0}, \\alpha I)\n",
    "$$\n",
    "This prior expresses the belief that:\n",
    "1. Weights are centered around zero\n",
    "2. Large weights are unlikely\n",
    "3. All weights are treated symmetrically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df509ab0",
   "metadata": {},
   "source": [
    "Let's assume $\\mathbf{w}$ are sampled from a gaussian distribution,\n",
    "$$\n",
    "m_i \\sim  {\\cal N}( \\mu_m, \\sigma) \\quad \\text{and} \\quad b_i \\sim  {\\cal N}( \\mu_b, \\sigma) \n",
    "$$\n",
    "\n",
    "In probabilistic methods, the **goal** is to find the distribution of the model's parameters that best describe our data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(m, b, x):\n",
    "    return m*x + b\n",
    "\n",
    "\n",
    "# generate som \"True\" data\n",
    "x = np.linspace(0, 2, 5)\n",
    "m = 2\n",
    "b = 1\n",
    "y = linear_model(m, b, x) + np.random.uniform(-1,\n",
    "                                              2, size=x.shape)  # add some noise\n",
    "\n",
    "\n",
    "def plot_samples(mu_m=2.0, mu_b=1.0, n_samples=12):\n",
    "    rng = np.random.default_rng()\n",
    "    sigma = 1.0\n",
    "    # n_samples = 12\n",
    "    m_samples = rng.normal(loc=mu_m, scale=sigma, size=n_samples)\n",
    "    b_samples = rng.normal(loc=mu_b, scale=sigma, size=n_samples)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for m_i, b_i in zip(m_samples, b_samples):\n",
    "        plt.plot(x, linear_model(m_i, b_i, x), color='tab:blue', alpha=0.2)\n",
    "    plt.scatter(x, y, color='k', zorder=3, label='data')\n",
    "    plt.xlabel('x', fontsize=18)\n",
    "    plt.ylabel('f(x)', fontsize=18)\n",
    "    # plt.title('Samples as means change')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "interact(\n",
    "    plot_samples,\n",
    "    mu_m=FloatSlider(min=-2.0, max=8.0, step=0.2, value=2.0),\n",
    "    mu_b=FloatSlider(min=-2.0, max=8.0, step=0.2, value=1.0),\n",
    "    n_samples=IntSlider(min=5, max=25, step=1, value=12),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d73d8f",
   "metadata": {},
   "source": [
    "# Posterior Distribution (Bayes’ Rule)\n",
    "Combining the likelihood and the prior using Bayes’ rule gives the posterior:\n",
    "$$\n",
    "p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}) \\propto p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}) p(\\mathbf{w}),\n",
    "$$\n",
    "The posterior distribution combines the likelihood and the prior, and captures everything we know about the parameters.\n",
    "\n",
    "## MAP Estimation and Regularization\n",
    "\n",
    "Let's take the logarithm of the posterior distribution, \n",
    "$$\n",
    "\\log p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y})\\ = \\log p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}) + \\log p(\\mathbf{w}) + C,\n",
    "$$\n",
    "If we use Gaussian likelihood and prior we get, \n",
    "$$\n",
    "\\log p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y})\\ \\propto  -\\frac{1}{2\\sigma^2} \\sum_i^N \\left ( y_i - \\mathbf{w}^\\top\\mathbf{x}_i \\right )^2 -\\frac{\\alpha}{2} \\mathbf{w}^\\top\\mathbf{w}.\n",
    "$$\n",
    "\n",
    "Maximizing the posterior is therefore equivalent to minimizing the **Ridge regression** loss function with an L2 regularization where $\\lambda = \\frac{-\\alpha}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f324915",
   "metadata": {},
   "source": [
    "## Close form solution of the Posterior Distribution \n",
    "By considering a Gaussian likelihood and prior, both, the posterior distribution has a closed form solution. (This is uncommon, usually in practice the posterior distribution is found through sampling methods; [Chapters 2, Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf).)\n",
    "\n",
    "The posterior distribution over the linear parameters condition on the training data\n",
    "$$\n",
    "p(\\mathbf{w} |\\mathbf{X},\\mathbf{w},\\lambda ) = {\\cal N}(\\mathbf{w}| \\Sigma \\mathbf{X}^\\top\\mathbf{y}, \\Sigma),\n",
    "$$\n",
    "$$\n",
    "\\Sigma = (\\mathbf{X}^\\top\\mathbf{X} + \\lambda \\mathbb{I})^{-1}\n",
    "$$\n",
    "\n",
    "Check [Bishop's book](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) Equation 2.116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd6a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_posterior_params(Xtr,ytr, l=0.001):\n",
    "\n",
    "    l_I = l * np.eye(Xtr.shape[1], Xtr.shape[1])\n",
    "    A = l_I + Xtr.T @ Xtr\n",
    "    A_inv = np.linalg.inv(A)\n",
    "\n",
    "    mean = A_inv @ (Xtr.T @ ytr)\n",
    "    sigma = A_inv\n",
    "    return mean, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef74e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate som \"True\" data\n",
    "from scipy.stats import multivariate_normal\n",
    "x = np.linspace(0, 2, 5)\n",
    "m = 2\n",
    "b = 1\n",
    "y = linear_model(m, b, x) + np.random.uniform(-1,\n",
    "                                              2, size=x.shape)  # add some noise\n",
    "X = np.column_stack((x, np.ones_like(x)))\n",
    "\n",
    "x_grid = np.linspace(-0.2, 2.2, 100)\n",
    "X_grid = np.column_stack((x_grid, np.ones_like(x_grid)))\n",
    "\n",
    "mean, std = calculate_posterior_params(X, y)\n",
    "random_theta = np.random.multivariate_normal(mean, std, 100)\n",
    "\n",
    "fig, (ax_left, ax_right) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax_left.scatter(x, y, label='data', zorder=3.5, s=45)\n",
    "ax_left.plot(x_grid, X_grid @ mean, c='r',\n",
    "             label='most probable model', ls='--', lw=2., zorder=2.5)\n",
    "for t in random_theta:\n",
    "    y_pred = X_grid @ t\n",
    "    ax_left.plot(x_grid, y_pred, c='grey', lw=0.4)\n",
    "\n",
    "ax_left.legend()\n",
    "ax_left.set_xlabel(r'$x$', fontsize=18)\n",
    "ax_left.set_ylabel(r'$f(x)$', fontsize=18)\n",
    "\n",
    "theta1, theta0 = np.meshgrid(\n",
    "    np.linspace(-0., 5, 50), np.linspace(-2, 4.1, 50))\n",
    "pos = np.empty(theta0.shape + (2,))\n",
    "pos[:, :, 0] = theta1\n",
    "pos[:, :, 1] = theta0\n",
    "\n",
    "rv = multivariate_normal(mean=mean, cov=std)\n",
    "\n",
    "ax_right.contourf(theta1, theta0, rv.pdf(pos))\n",
    "ax_right.plot(mean[0], mean[1], 'rx', label='mean')\n",
    "ax_right.set_xlabel(r'$m$', fontsize=15)\n",
    "ax_right.set_ylabel(r'$b$', fontsize=15)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00284110",
   "metadata": {},
   "source": [
    "*   **Blue Points:** These represent the actual observed data points.\n",
    "  \n",
    "*   **Black Lines:** Each of these represents a regression line sampled from the posterior distribution. They give a sense of the uncertainty in the regression model; where they are dense, there is more certainty about the model, and where they spread out, there is more uncertainty.\n",
    "\n",
    "*   **Red Dashed Line:** This is the most probable regression line based on the mean of the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e5ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "MAX_POINTS = 50\n",
    "rng_data = np.random.default_rng(42)\n",
    "x_all = rng_data.uniform(-1., 1., MAX_POINTS)\n",
    "y_all = 2.0 * x_all + 1.0 + rng_data.normal(0., 0.3, MAX_POINTS)\n",
    "X_all = np.column_stack((x_all, np.ones_like(x_all)))\n",
    "\n",
    "def frame(n_points=10):\n",
    "    X = X_all[:n_points]\n",
    "    y = y_all[:n_points]\n",
    "\n",
    "    mean, std = calculate_posterior_params(X, y)\n",
    "    rng_theta = np.random.default_rng(123)\n",
    "    random_theta = rng_theta.multivariate_normal(mean, std, 50)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # left panel\n",
    "    x_grid = np.linspace(-1., 1., 100)\n",
    "    X_grid = np.column_stack((x_grid, np.ones_like(x_grid)))\n",
    "    axs[0].scatter(X[:, 0], y, label='data', zorder=3.5, s=70)\n",
    "    axs[0].plot(x_grid, X_grid @ mean, c='r',\n",
    "                label=r'$\\mathbf{w}_{MAP}^{*}$', ls='--', lw=2., zorder=2.5)\n",
    "    for t in random_theta:\n",
    "        y_pred = X_grid @ t\n",
    "        axs[0].plot(x_grid, y_pred, c='grey', lw=0.4)\n",
    "\n",
    "    axs[0].legend(fontsize=15)\n",
    "    axs[0].set_xlabel(r'$x$', fontsize=18)\n",
    "    axs[0].set_ylabel(r'$f(x)$', fontsize=18)\n",
    "\n",
    "    # right panel\n",
    "    theta1, theta0 = np.meshgrid(\n",
    "        np.linspace(-5., 5., 50), np.linspace(-5., 5., 100))\n",
    "    pos = np.empty(theta0.shape + (2,))\n",
    "    pos[:, :, 0] = theta1\n",
    "    pos[:, :, 1] = theta0\n",
    "    rv = multivariate_normal(mean=mean, cov=std)\n",
    "\n",
    "    axs[1].contourf(theta1, theta0, rv.pdf(pos))\n",
    "    axs[1].set_xlabel(r'$m$', fontsize=15)\n",
    "    axs[1].set_ylabel(r'$b$', fontsize=15)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "interact(\n",
    "    frame,\n",
    "    n_points=widgets.IntSlider(min=1, max=MAX_POINTS, step=1, value=1),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0d7f9c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
