{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2d0461",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChemAI-Lab/AI4Chem/blob/main/website/modules/04-intro_to_torch.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5338f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89beefa8",
   "metadata": {},
   "source": [
    "# Tensors & Autograd\n",
    "\n",
    "1.1 Why Tensors?\n",
    "What to say (conceptual)\n",
    "\n",
    "In PyTorch, everything is a tensor: inputs, parameters, outputs, gradients.\n",
    "A tensor is just a multi-dimensional array plus information needed for differentiation and acceleration.\n",
    "\n",
    "Compare to NumPy:\n",
    "\n",
    "* Same idea as `np.ndarray`\n",
    "* But:\n",
    "\n",
    "    * Knows about gradients\n",
    "    * Can live on GPU\n",
    "    * Can participate in a computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd67ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "A = torch.randn(3, 3)\n",
    "\n",
    "print(x)\n",
    "print(x.shape, A.shape)\n",
    "print(x.dtype)\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2090c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Operations & Broadcasting\n",
    "\n",
    "x = torch.randn(5)\n",
    "y = torch.randn(5)\n",
    "\n",
    "z = x + y\n",
    "w = x * y\n",
    "print(z.shape,x.shape)\n",
    "\n",
    "# Matrix operations:\n",
    "A = torch.randn(4, 3)\n",
    "B = torch.randn(3, 2)\n",
    "C = A @ B\n",
    "print(C.shape)\n",
    "\n",
    "# Broadcasting\n",
    "x = torch.randn(4, 3)\n",
    "y = torch.randn(3)  \n",
    "z = x + y\n",
    "print(z.shape)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ee986",
   "metadata": {},
   "source": [
    "## Introducing Autograd\n",
    "In PyTorch, autograd is the built-in automatic differentiation engine that powers all neural network training. It allows for the automatic computation of the gradient of any scalar value (like a loss function) with respect to all variables (like model parameters) that contributed to its computation. <br>\n",
    "\n",
    "[Introduction to Autograd](https://docs.pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de401e9d",
   "metadata": {},
   "source": [
    "$$\n",
    "y = x^2 + 3x + 1\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = 2x + 3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267e3d6",
   "metadata": {},
   "source": [
    "* Every operation creates a node\n",
    "* The graph is built during the forward pass\n",
    "* `backward()` applies the chain rule automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2 + 3*x + 1\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76505dd",
   "metadata": {},
   "source": [
    "## Gradient Accumulation\n",
    "* Gradients accumulate\n",
    "* Torch assumes you want to sum gradients\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x^2}{\\partial x} = 2x \\quad \\text{and} \\quad \\frac{\\partial x^3}{\\partial x} = 3x^2  \n",
    "$$\n",
    "\n",
    "PyTorch literally does:<br>\n",
    "`x.grad += new_gradient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cdab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "y1 = x**2\n",
    "y1.backward()\n",
    "print(x.grad)  # 2x = 4\n",
    "\n",
    "# x.grad.zero_() # uncomment this line\n",
    "\n",
    "y2 = x**3\n",
    "y2.backward()\n",
    "print(x.grad)  # 4 + 3x^2 = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e1e1e",
   "metadata": {},
   "source": [
    "We can use gradient accumulation for multiple loss terms, \n",
    "$$\n",
    "{\\cal L} = {\\cal L}_{\\text{MSE}} + \\lambda {\\cal L}_{\\text{regularization}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9660b3d6",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Networks\n",
    "We will build a **PIP-NN**, permutationally invariant polynomial neural network.\n",
    "\n",
    "1. **Data loader**: provides an iterable over the data samples, simplifying and optimizing the process of feeding data to a model during training or evaluation\n",
    "2. **Model**: Feed Forward Neural Network\n",
    "3. **Training loop**: Main part of training stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391e1364",
   "metadata": {},
   "source": [
    "We will first set up data handling, then define the model, and finally train and evaluate it.\n",
    "Keep an eye on where we scale targets so training, validation, and plots stay consistent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a107166",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data using pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_file = 'https://raw.githubusercontent.com/ChemAI-Lab/Math4Chem/main/website/Assignments/CH4_data.csv'\n",
    "data = pd.read_csv(data_file)\n",
    "data.head()\n",
    "\n",
    "# 1. How many points does the dataset contains?\n",
    "n = data.shape[0]\n",
    "print('The dataset contains {} points.'.format(n))\n",
    "\n",
    "# Load PIPs representation per molecule\n",
    "y_all = data['energy'].to_numpy()\n",
    "print('Total energy points:', y_all.shape)\n",
    "X_all = data.drop(['energy', 'Unnamed: 0'], axis=1).to_numpy()\n",
    "print(\"Total Geometries:\", X_all.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=42)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n",
    "\n",
    "X0_train = X_train\n",
    "X0_test = X_test\n",
    "y0_train = y_train\n",
    "y0_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5410f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Dataset\n",
    "# -----------------------------\n",
    "class PIPDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47907142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2) Normalization\n",
    "# -----------------------------\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(y0_train)\n",
    "# --- Fit standardizer on TRAIN only (important!)\n",
    "y_train = scaler.transform(y_train)\n",
    "y_test  = scaler.transform(y_test)\n",
    "\n",
    "plt.hist(y_train, bins=30, alpha=0.5, label='Train')\n",
    "plt.hist(y_test, bins=30, alpha=0.5, label='Test')\n",
    "plt.xlabel('Standardized Energy')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab9f6f5",
   "metadata": {},
   "source": [
    "Note: losses are computed in standardized target space.\n",
    "We only inverse-transform for plotting or reporting in original units.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54883e",
   "metadata": {},
   "source": [
    "# Check your device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"\n",
    "    Automatically selects the best available device (CUDA, MPS, or CPU).\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dedd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tr = PIPDataset(X_train, y_train)\n",
    "dataset_test = PIPDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 264\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset_tr,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Test/validation loader (no shuffle)\n",
    "test_loader = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "for i,(X_batch, y_batch) in enumerate(loader):\n",
    "    print(X_batch.shape)  # (batch_size, D)\n",
    "    print(y_batch.shape)  # (batch_size, 1)\n",
    "    print(X_batch[:3])\n",
    "    print(y_batch[:3])\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c146dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: one forward pass shape\n",
    "X_batch, y_batch = next(iter(loader))\n",
    "print('X batch:', X_batch.shape, 'y batch:', y_batch.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2953855d",
   "metadata": {},
   "source": [
    "## Stochastic approximation of MSE\n",
    "Commonly in ML, we do not use \"clean\" gradient, we usually use a subset of the data (mini-batch) to approximate the gradient\n",
    "\n",
    "$$\n",
    "\\nabla {\\cal L} \\approx \\frac{1}{B} \\sum_i^B \\nabla {\\cal L}_i\n",
    "$$\n",
    "where ${\\cal L}_i$ is the mean square error for each point in the mini-batch ($B$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4dda0d",
   "metadata": {},
   "source": [
    "Model dimensions: `input_size` = number of features per sample, `hidden_size` = neurons per hidden layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16970ea0",
   "metadata": {},
   "source": [
    "```Python\n",
    "# build a simple n-layer neural network using PyTorch (Sequential in a class)\n",
    "class SimpleNNSequential(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1):\n",
    "        super(SimpleNNSequential, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size), # Linear projection to the number of neurons in first layer\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size), # Scalar output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "```\n",
    "\n",
    "We can automate the number of layers instead of manually adding them into `SimpleNNSequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e556ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNNSequential(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=2, output_size=1, n_layers=3, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_size), activation()]\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers += [nn.Linear(hidden_size, hidden_size), activation()]\n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173807d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model / Optim / Loss\n",
    "hidden_size = 264\n",
    "input_size = X_train.shape[1]\n",
    "n_layers = 3\n",
    "activation = nn.LeakyReLU\n",
    "activation_name = 'LeakyReLU'\n",
    "\n",
    "pipnn_model_info = {\"hidden_size\": hidden_size,\n",
    "                  \"input_size\": input_size,\n",
    "                  \"n_layers\": n_layers,\n",
    "                  \"activation\": activation_name\n",
    "                 }\n",
    "\n",
    "pipnn_model = SimpleNNSequential(input_size=input_size, \n",
    "                                 hidden_size=hidden_size, \n",
    "                                 output_size=1, \n",
    "                                 n_layers=n_layers, \n",
    "                                    activation=activation)\n",
    "\n",
    "pipnn_model.to(device)\n",
    "print(pipnn_model)\n",
    "print('Model: ', next(pipnn_model.parameters()).device)\n",
    "\n",
    "lr = 2E-3\n",
    "weight_decay = 1E-5\n",
    "optimizer = torch.optim.Adam(pipnn_model.parameters(), \n",
    "                              lr=lr, \n",
    "                              weight_decay=weight_decay)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# # --- LR scheduler (validation-driven)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162aad87",
   "metadata": {},
   "source": [
    "# Main training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87955fd",
   "metadata": {},
   "source": [
    "Validation frequency trades off speed vs. monitoring.\n",
    "For faster training, evaluate every 10â€“50 epochs instead of every epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad77bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 750\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    pipnn_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for i, (X_batch, y_batch) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_pred = pipnn_model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * X_batch.size(0)\n",
    "    epoch_loss /= len(dataset_tr)\n",
    "\n",
    "    if (epoch) % 1 == 0:\n",
    "        # Validation/test loss\n",
    "        pipnn_model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = pipnn_model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "        val_loss /= len(dataset_test)\n",
    "        train_losses.append(epoch_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "    if (epoch) % 50 == 0:\n",
    "        scheduler.step()  # step per epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc846828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss with EMA\n",
    "def ema(series, alpha=0.1):\n",
    "    values = []\n",
    "    ema_val = None\n",
    "    for x in series:\n",
    "        ema_val = x if ema_val is None else alpha * x + (1 - alpha) * ema_val\n",
    "        values.append(ema_val)\n",
    "    return values\n",
    "\n",
    "alpha = 0.05  # smoothing factor\n",
    "train_ema = ema(train_losses, alpha=alpha)\n",
    "val_ema = ema(val_losses, alpha=alpha)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(1*np.arange(len(train_losses)), train_losses, color='tab:blue',label='Training loss', alpha=0.15)\n",
    "plt.plot(1*np.arange(len(val_losses)), val_losses, color='tab:orange', label='Validation loss', alpha=0.15)\n",
    "plt.plot(1*np.arange(len(train_ema)), train_ema, ls='--', color='tab:blue', label=f'Training EMA (alpha={alpha})')\n",
    "plt.plot(1*np.arange(len(val_ema)), val_ema, ls='--',\n",
    "         color='tab:orange', label=f'Validation EMA (alpha={alpha})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "# plt.ylim(0, 0.5)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_to_numpy(tensor):\n",
    "    return np.array(tensor.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262eb6cc",
   "metadata": {},
   "source": [
    "Residuals help diagnose bias (mean shift) and spread (variance).\n",
    "A good model shows residuals centered near zero with similar width for train/test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55d6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "    y_test_pred = pipnn_model(X_test_tensor)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    y_train_pred = pipnn_model(X_train_tensor)\n",
    "\n",
    "    test_loss = loss_fn(y_test_pred, y_test_tensor).item()\n",
    "    train_loss = loss_fn(y_train_pred, y_train_tensor).item()\n",
    "\n",
    "print(f\"Test MSE Loss: {test_loss:.4f}\")\n",
    "print(f\"Train MSE Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Inverse-transform to original scale for plotting\n",
    "train_true = scaler.inverse_transform(y_train)\n",
    "test_true = scaler.inverse_transform(y_test)\n",
    "train_pred = scaler.inverse_transform(\n",
    "    torch_to_numpy(y_train_pred))\n",
    "test_pred = scaler.inverse_transform(\n",
    "    torch_to_numpy(y_test_pred))\n",
    "\n",
    "# Residuals\n",
    "train_res = train_pred - train_true\n",
    "test_res = test_pred - test_true\n",
    "\n",
    "_, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
    "axs[0].scatter(train_true, train_pred, alpha=0.5)\n",
    "axs[0].plot([train_true.min(), train_true.max()], [train_true.min(), train_true.max()], 'r--')\n",
    "axs[0].set_xlabel('True Energies')\n",
    "axs[0].set_ylabel('Predicted Energies')\n",
    "axs[0].set_title('Training Set')    \n",
    "\n",
    "axs[1].scatter(test_true, test_pred, alpha=0.5)\n",
    "axs[1].plot([test_true.min(), test_true.max()], [test_true.min(), test_true.max()], 'r--')\n",
    "axs[1].set_xlabel('True Energies')\n",
    "axs[1].set_ylabel('Predicted Energies')\n",
    "axs[1].set_title('Test Set')    \n",
    "\n",
    "axs[2].hist(train_res, bins=30, alpha=0.5, label='Train')\n",
    "axs[2].hist(test_res, bins=30, alpha=0.5, label='Test')\n",
    "axs[2].axhline(0, color='r', linestyle='--', linewidth=1)\n",
    "axs[2].set_xlabel('Residual (Pred - True)')\n",
    "axs[2].set_ylabel('Count')\n",
    "axs[2].set_title('Residuals')\n",
    "axs[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d533617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model parameters\n",
    "model_path = f'pipnn_model_{activation_name}.pt'\n",
    "ckpt = {\n",
    "    \"state_dict\": pipnn_model.state_dict(),\n",
    "    \"pipnn_model_info\": pipnn_model_info,\n",
    "}\n",
    "\n",
    "torch.save(ckpt, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b697bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model from URL\n",
    "model_url = 'https://github.com/ChemAI-Lab/AI4Chem/raw/main/website/modules/models/pipnn_model.pt'\n",
    "state_dict = torch.hub.load_state_dict_from_url(model_url, map_location='cpu')\n",
    "print(state_dict.keys())\n",
    "pipnn_model_info = state_dict['pipnn_model_info']\n",
    "pipnn_weights = state_dict['state_dict']\n",
    "\n",
    "hidden_size = pipnn_model_info[\"hidden_size\"]\n",
    "input_size = pipnn_model_info[\"input_size\"]\n",
    "n_layers = pipnn_model_info[\"n_layers\"]\n",
    "activation_name = pipnn_model_info[\"activation\"]\n",
    "activation = nn.Tanh if activation_name == 'Tanh' else nn.ReLU\n",
    "pipnn_model = SimpleNNSequential(input_size=input_size,\n",
    "                                 hidden_size=hidden_size,\n",
    "                                 output_size=1,\n",
    "                                 n_layers=n_layers,\n",
    "                                 activation=activation)\n",
    "\n",
    "pipnn_model.load_state_dict(pipnn_weights)\n",
    "pipnn_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195ca45b",
   "metadata": {},
   "source": [
    "# Let's change the Activation function\n",
    "1. We are going to load the same weights but use another activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a998db",
   "metadata": {},
   "source": [
    "Warning: changing activations changes the function class.\n",
    "Weights trained with one activation may not transfer well to another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf3de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = pipnn_model_info[\"hidden_size\"]\n",
    "input_size = pipnn_model_info[\"input_size\"]\n",
    "n_layers = pipnn_model_info[\"n_layers\"]\n",
    "activation_name = pipnn_model_info[\"activation\"]\n",
    "\n",
    "# new activation function\n",
    "activation = nn.ReLU\n",
    "\n",
    "pipnn_model_new = SimpleNNSequential(input_size=input_size,\n",
    "                                 hidden_size=hidden_size,\n",
    "                                 output_size=1,\n",
    "                                 n_layers=n_layers,\n",
    "                                 activation=activation)\n",
    "\n",
    "pipnn_model_new.load_state_dict(pipnn_weights)\n",
    "pipnn_model_new.to(device)\n",
    "pipnn_model_new.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2292dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and plot on original (unscaled) target units\n",
    "pipnn_model_new.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.tensor(X0_test, dtype=torch.float32).to(device)\n",
    "    X_train_tensor = torch.tensor(X0_train, dtype=torch.float32).to(device)\n",
    "\n",
    "    y_test_pred = pipnn_model_new(X_test_tensor)\n",
    "    y_test_pred = torch_to_numpy(y_test_pred)\n",
    "    y_train_pred = pipnn_model_new(X_train_tensor)\n",
    "    y_train_pred = torch_to_numpy(y_train_pred)\n",
    "\n",
    "print(f\"Test MSE Loss: {test_loss:.4f}\")\n",
    "print(f\"Train MSE Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Ensure 2D shape for inverse_transform\n",
    "if y_train_pred.ndim == 1:\n",
    "    y_train_pred = y_train_pred.reshape(-1, 1)\n",
    "if y_test_pred.ndim == 1:\n",
    "    y_test_pred = y_test_pred.reshape(-1, 1)\n",
    "\n",
    "# Inverse-transform predictions to original scale\n",
    "train_pred = scaler.inverse_transform(y_train_pred)\n",
    "test_pred = scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "# True targets already in original scale\n",
    "train_true = y0_train\n",
    "test_true = y0_test\n",
    "if train_true.ndim == 1:\n",
    "    train_true = train_true.reshape(-1, 1)\n",
    "if test_true.ndim == 1:\n",
    "    test_true = test_true.reshape(-1, 1)\n",
    "\n",
    "# Residuals (Pred - True)\n",
    "train_res = train_pred - train_true\n",
    "test_res = test_pred - test_true\n",
    "\n",
    "_, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
    "axs[0].scatter(train_true, train_pred, alpha=0.5)\n",
    "axs[0].plot([train_true.min(), train_true.max()], [train_true.min(), train_true.max()], 'r--')\n",
    "axs[0].set_xlabel('True Energies')\n",
    "axs[0].set_ylabel('Predicted Energies')\n",
    "axs[0].set_title('Training Set')\n",
    "\n",
    "axs[1].scatter(test_true, test_pred, alpha=0.5)\n",
    "axs[1].plot([test_true.min(), test_true.max()], [test_true.min(), test_true.max()], 'r--')\n",
    "axs[1].set_xlabel('True Energies')\n",
    "axs[1].set_ylabel('Predicted Energies')\n",
    "axs[1].set_title('Test Set')\n",
    "\n",
    "axs[2].hist(train_res, bins=30, alpha=0.5, label='Train')\n",
    "axs[2].hist(test_res, bins=30, alpha=0.5, label='Test')\n",
    "axs[2].axhline(0, color='r', linestyle='--', linewidth=1)\n",
    "axs[2].set_xlabel('Residual (Pred - True)')\n",
    "axs[2].set_ylabel('Count')\n",
    "axs[2].set_title('Residuals')\n",
    "axs[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9e19f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
