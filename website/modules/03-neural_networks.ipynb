{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808b4cc9",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron or Neural Networks\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChemAI-Lab/AI4Chem/blob/main/website/modules/03-neural_networks.ipynb)\n",
    "\n",
    "**References:**\n",
    "1. **Chapters 5**: [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), C. M. Bishop.\n",
    "2. **Chapter 2**: [Machine Learning in Quantum Sciences](https://arxiv.org/pdf/2204.04198)\n",
    "3. **Chapter 16**: [Probabilistic Machine Learning: An Introduction, K. P. Murphy.](https://probml.github.io/pml-book/book1.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b412f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.datasets import make_moons, make_classification\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b84f15",
   "metadata": {},
   "source": [
    "# Feed-Forward Network Functions\n",
    "\n",
    "Linear models for regression or classification tasks rely on the assumption of having a **fixed** representation of $\\mathbf{x}$ through the use of basis set functions $\\phi$.\n",
    "For binary classification, we saw that such models take the form of, \n",
    "$$\n",
    "y = f(\\mathbf{x},\\mathbf{w}) = \\sigma\\left (\\mathbf{w}^\\top\\phi(\\mathbf{x})\\right ),\n",
    "$$\n",
    "where the function $\\sigma$ carries adds additional non-linearity to $f$. We will discuss later that the chose of $\\sigma$ depends on the task at hand, some work best for classification and some for regression. <br>\n",
    "\n",
    "Obviously, the only possible \"improvement\" to this family of models is making the basis functions $\\phi_j(\\cdot)$ depend on some parameters, which can and will be optimized during the training. <br>\n",
    "In neural networks (NNs), each basis function can be defined as, \n",
    "> non-linear function of a linear combination of the inputs, where the coefficients in the linear combination are adaptive parameters.\n",
    "\n",
    "$$\n",
    "z_j =  \\sigma\\left (a_j \\right ), \\quad \\text{where} \\quad a_j = \\mathbf{\\omega}_j^\\top \\mathbf{x}\n",
    "$$\n",
    "We can see that $z_j$ comes from a composition of functions, first a linear function followed by the activation function ($\\sigma$). $z_j$ are known in NNs as **neurons** and the quantities $a_j$ as **activation**. We shall refer to the parameters $\\mathbf{\\omega}_j$ as **weights** (and **biases**).\n",
    "\n",
    "In the context of NNs, $z_j$ are called **hidden units**, and these values can be linearly combined again to create functions with a higher non-linear character. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0219e802",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "In the perceptron model, we assumed that $\\phi(\\mathbf{x})$ is a non linear differentiable transformation to generate the feature space representation. \n",
    "The jump to modern deep learning models is to assume $\\phi(\\mathbf{x})$ can be learned through some parameters, but for that we require a non-linear transformation, commonly known as **activation function**.\n",
    "Typically, a differentiable nonlinear activation function is used in the hidden layers of a neural network. This allows the model to learn more complex functions than a network trained using a linear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6496473c",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\sigma(x)=\\frac{1}{1+e^{-x}}$\\\n",
    "Derivative: $\\sigma'(x)=\\sigma(x)(1-\\sigma(x))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 400)\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "y = sigmoid(x)\n",
    "dy = y * (1 - y)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('Sigmoid')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "ax[0].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[1].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[0].set_ylabel(rf\"$\\sigma(x)$\", fontsize=14)\n",
    "ax[1].set_ylabel(r\"$\\frac{d\\sigma(x)}{dx}$\", fontsize=14,rotation=0,labelpad=25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d60c7",
   "metadata": {},
   "source": [
    "## Tanh\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$\\\n",
    "Derivative: $\\frac{d}{dx}\\tanh(x)=1-\\tanh^2(x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf73994",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 400)\n",
    "y = np.tanh(x)\n",
    "dy = 1 - y**2\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('Tanh')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "ax[0].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[1].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[0].set_ylabel(rf\"$\\sigma(x)$\", fontsize=14)\n",
    "ax[1].set_ylabel(r\"$\\frac{d\\sigma(x)}{dx}$\",\n",
    "                 fontsize=14, rotation=0, labelpad=25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b6a453",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\mathrm{ReLU}(x)=\\max(0, x)$\\\n",
    "Derivative: $\\mathrm{ReLU}'(x)=\\begin{cases}1,&x>0\\\\0,&x\\le 0\\end{cases}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 400)\n",
    "y = np.maximum(0, x)\n",
    "dy = (x > 0).astype(float)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('ReLU')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "ax[0].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[1].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[0].set_ylabel(rf\"$\\sigma(x)$\", fontsize=14)\n",
    "ax[1].set_ylabel(r\"$\\frac{d\\sigma(x)}{dx}$\",\n",
    "                 fontsize=14, rotation=0, labelpad=25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5140d",
   "metadata": {},
   "source": [
    "## Leaky ReLU\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\mathrm{LReLU}(x)=\\max(\\alpha x, x)$\\\n",
    "Derivative: $\\mathrm{LReLU}'(x)=\\begin{cases}1,&x>0\\\\\\alpha,&x\\le 0\\end{cases}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eece341",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 400)\n",
    "alpha = 0.01\n",
    "y = np.where(x > 0, x, alpha * x)\n",
    "dy = np.where(x > 0, 1.0, alpha)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('Leaky ReLU')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "ax[0].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[1].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[0].set_ylabel(rf\"$\\sigma(x)$\", fontsize=14)\n",
    "ax[1].set_ylabel(r\"$\\frac{d\\sigma(x)}{dx}$\",\n",
    "                 fontsize=14, rotation=0, labelpad=25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fefcf0",
   "metadata": {},
   "source": [
    "## ELU\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\mathrm{ELU}(x)=\\begin{cases}x,&x>0\\\\\\alpha(e^x-1),&x\\le 0\\end{cases}$\\\n",
    "Derivative: $\\mathrm{ELU}'(x)=\\begin{cases}1,&x>0\\\\\\alpha e^x,&x\\le 0\\end{cases}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f6179",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 400)\n",
    "alpha = 1.0\n",
    "y = np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "dy = np.where(x > 0, 1.0, y + alpha)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('ELU')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "ax[0].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[1].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[0].set_ylabel(rf\"$\\sigma(x)$\", fontsize=14)\n",
    "ax[1].set_ylabel(r\"$\\frac{d\\sigma(x)}{dx}$\",\n",
    "                 fontsize=14, rotation=0, labelpad=25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4269bf",
   "metadata": {},
   "source": [
    "## Softplus\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\mathrm{Softplus}(x)=\\ln(1+e^x)$\\\n",
    "Derivative: $\\frac{d}{dx}\\mathrm{Softplus}(x)=\\sigma(x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 400)\n",
    "y = np.log1p(np.exp(x))\n",
    "dy = 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('Softplus')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "ax[0].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[1].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[0].set_ylabel(rf\"$\\sigma(x)$\", fontsize=14)\n",
    "ax[1].set_ylabel(r\"$\\frac{d\\sigma(x)}{dx}$\",\n",
    "                 fontsize=14, rotation=0, labelpad=25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c960dc80",
   "metadata": {},
   "source": [
    "## GELU (approx)\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\mathrm{GELU}(x)=0.5x\\left(1+\\tanh\\left(\\sqrt{\\tfrac{2}{\\pi}}(x+0.044715x^3)\\right)\\right)$\\\n",
    "Derivative: $\\mathrm{GELU}'(x)=0.5(1+\\tanh u)+0.5x(1-\\tanh^2 u)u'$, $u=\\sqrt{\\tfrac{2}{\\pi}}(x+0.044715x^3)$, $u'=\\sqrt{\\tfrac{2}{\\pi}}(1+3\\cdot0.044715x^2)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f707d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 400)\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "y = gelu(x)\n",
    "# Numerical derivative for visualization\n",
    "dy = np.gradient(y, x)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('GELU (approx)')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative (numerical)')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "ax[0].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[1].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[0].set_ylabel(rf\"$\\sigma(x)$\", fontsize=14)\n",
    "ax[1].set_ylabel(r\"$\\frac{d\\sigma(x)}{dx}$\",\n",
    "                 fontsize=14, rotation=0, labelpad=25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a18cc4",
   "metadata": {},
   "source": [
    "# Single Layer Neural Network\n",
    "\n",
    "In its core a NN is a **composition of functions**. Let's build the simplest NN possible, where a single **layer** of hidden units its use for classification. We will use the Binary Cross-Entropy loss, \n",
    "$$\n",
    "{\\cal L}_{\\text{BCE}} = -\\frac{1}{N}\\sum_i^N \\left (y_i\\log(p_i) - (1 - y_i)\\log(1- p_i) \\right ), \n",
    "$$\n",
    "where $y_i$ is true binary label, $y_i = [0,1]$, $p_i$ is the predicted probability.<br>\n",
    "\n",
    "\n",
    "Recap of our previous model for classification, \n",
    "$$\n",
    "p_0 = \\sigma\\left ( a_j \\right )  = \\sigma\\left (\\mathbf{w}^\\top\\phi(\\mathbf{x}) \\right ),\n",
    "$$\n",
    "this model has a **single** hidden neuron. We the sigmoid activation function.\n",
    "\n",
    "Let's consider now a two hidden neuron representation for this same model, \n",
    "$$\n",
    "p_0 =  \\sigma\\left (\\mathbf{w}^\\top\\mathbf{z} \\right ),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbf{z} = \\underbrace{\\begin{bmatrix}\n",
    " z_1 \\\\\n",
    "z_2\n",
    "\\end{bmatrix}}_{\\text{hidden neurons}}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "z_j = \\sigma'(\\mathbf{\\omega}_j^\\top \\phi(\\mathbf{x})).\n",
    "$$\n",
    "For notation clearness, $\\sigma'$ represent an activation function different from the sigmoid one that we use to predict the probability of $p_0$.\n",
    "This function has the following diagram, <br>\n",
    "<img src=\"https://github.com/ChemAI-Lab/AI4Chem/raw/main/website/modules/Figures/single_layer_mlp.png\"\n",
    "     width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5132722a",
   "metadata": {},
   "source": [
    "## Gradient of the Loss (Two Hidden Neurons, Single Sample)\n",
    "\n",
    "For a single data point $(\\mathbf{x}, y)$, define the hidden pre-activations and activations\n",
    "$$\n",
    "a_j = \\boldsymbol{\\omega}_j^\\top \\boldsymbol{\\phi}(\\mathbf{x}),\n",
    "\\qquad\n",
    "z_j = \\sigma'(a_j),\n",
    "\\qquad j = 1,2.\n",
    "$$\n",
    "\n",
    "Collect the hidden activations as\n",
    "$$\n",
    "\\mathbf{z} =\n",
    "\\begin{bmatrix}\n",
    "z_1\\\\\n",
    "z_2\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The output pre-activation and predicted probability are\n",
    "$$\n",
    "s = \\mathbf{w}^\\top \\mathbf{z},\n",
    "\\qquad\n",
    "p_0 = \\sigma(s).\n",
    "$$\n",
    "\n",
    "The binary cross-entropy loss is\n",
    "$$\n",
    "\\mathcal{L}\n",
    "= -\\bigl[\n",
    "y \\log p_0\n",
    "+ (1-y)\\log(1-p_0)\n",
    "\\bigr].\n",
    "$$\n",
    "\n",
    "## Gradient with Respect to Output Weights\n",
    "\n",
    "Applying the chain rule,\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial p_0}\n",
    "\\frac{\\partial p_0}{\\partial s}\n",
    "\\frac{\\partial s}{\\partial \\mathbf{w}}.\n",
    "$$\n",
    "\n",
    "Each term is given by\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial p_0}\n",
    "= -\\left(\n",
    "\\frac{y}{p_0} - \\frac{1-y}{1-p_0}\n",
    "\\right),\n",
    "\\qquad\n",
    "\\frac{\\partial p_0}{\\partial s}\n",
    "= p_0(1-p_0),\n",
    "\\qquad\n",
    "\\frac{\\partial s}{\\partial \\mathbf{w}}\n",
    "= \\mathbf{z}.\n",
    "$$\n",
    "\n",
    "Combining terms yields\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}}\n",
    "=\n",
    "(p_0 - y)\\,\\mathbf{z}.\n",
    "$$\n",
    "\n",
    "## Gradient with Respect to Hidden Weights\n",
    "\n",
    "For the hidden weights $\\boldsymbol{\\omega}_j$, apply the chain rule\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\omega}_j}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial p_0}\n",
    "\\frac{\\partial p_0}{\\partial s}\n",
    "\\frac{\\partial s}{\\partial z_j}\n",
    "\\frac{\\partial z_j}{\\partial a_j}\n",
    "\\frac{\\partial a_j}{\\partial \\boldsymbol{\\omega}_j}.\n",
    "$$\n",
    "\n",
    "The individual derivatives are\n",
    "$$\n",
    "\\frac{\\partial s}{\\partial z_j} = w_j,\n",
    "\\qquad\n",
    "\\frac{\\partial z_j}{\\partial a_j}\n",
    "= \\sigma'(a_j)\\bigl(1-\\sigma'(a_j)\\bigr),\n",
    "\\qquad\n",
    "\\frac{\\partial a_j}{\\partial \\boldsymbol{\\omega}_j}\n",
    "= \\boldsymbol{\\phi}(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "Combining all terms gives\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\omega}_j}\n",
    "=\n",
    "(p_0 - y)\\, w_j \\,\n",
    "\\sigma'(a_j)\\bigl(1-\\sigma'(a_j)\\bigr)\\,\n",
    "\\boldsymbol{\\phi}(\\mathbf{x}),\n",
    "\\qquad j = 1,2.\n",
    "$$\n",
    "\n",
    "## Full Gradient for $N$ Data Points\n",
    "\n",
    "The gradients over a dataset of $N$ samples are obtained by averaging:\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\mathrm{BCE}}\n",
    "=\n",
    "\\frac{1}{N}\\sum_{i=1}^N\n",
    "\\bigl(p_{0,i} - y_i\\bigr)\\,\n",
    "\\mathbf{z}_i,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol{\\omega}_j} \\mathcal{L}_{\\mathrm{BCE}}\n",
    "=\n",
    "\\frac{1}{N}\\sum_{i=1}^N\n",
    "\\bigl(p_{0,i} - y_i\\bigr)\\,\n",
    "w_j\\,\n",
    "\\sigma'(a_{j,i})\\bigl(1-\\sigma'(a_{j,i})\\bigr)\\,\n",
    "\\boldsymbol{\\phi}(\\mathbf{x}_i),\n",
    "\\qquad j = 1,2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd15d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(name=\"linear\"):\n",
    "    if name == \"linear\":\n",
    "        X, y = make_classification(\n",
    "            n_features=2, n_redundant=0, n_informative=2,\n",
    "            n_clusters_per_class=1, class_sep=1.1, flip_y=0,\n",
    "            random_state=1\n",
    "        )\n",
    "\n",
    "        rng = np.random.RandomState(2)\n",
    "        X += 2 * rng.uniform(size=X.shape)\n",
    "        linearly_separable = (X, y)\n",
    "        dataset = linearly_separable\n",
    "    elif name == \"moons\":\n",
    "        X, y = make_moons(noise=0.1, n_samples=200, )\n",
    "        dataset = (X, y)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset_name = \"moons\"\n",
    "dataset = get_data(dataset_name)\n",
    "\n",
    "cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(\n",
    "    dataset[0][:, 0], dataset[0][:, 1],\n",
    "    c=dataset[1], cmap=cm_bright,\n",
    "    edgecolor='k', s=100\n",
    ")\n",
    "if dataset_name == \"linear\":\n",
    "    plt.title(\"Linearly separable data\", fontsize=16)\n",
    "elif dataset_name == \"moons\":\n",
    "    plt.title(\"Non-linearly separable data\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c00f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple two layer neural network classifier using PyTorch\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b58ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "X = torch.tensor(dataset[0], dtype=torch.float32)\n",
    "y = torch.tensor(dataset[1], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "hidden_neurons = 10\n",
    "model = SimpleNN(input_size=2, hidden_size=hidden_neurons, output_size=1)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "\n",
    "# Store snapshots in memory (no files)\n",
    "state_history = []\n",
    "bce_ = []\n",
    "misclassified_ = []\n",
    "snapshot_every = 10\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % snapshot_every == 0:\n",
    "        state_history.append({k: v.detach().cpu().clone() for k, v in model.state_dict().items()})\n",
    "        bce_.append(loss.item())\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        misclassified = (preds != y).sum().item()\n",
    "        misclassified_.append(misclassified)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Misclassified: {misclassified}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots of BCE and misclassified samples\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].plot(bce_, color=\"black\", linewidth=2)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"BCE\")\n",
    "\n",
    "axes[1].plot(misclassified_, color=\"red\", linewidth=2)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Misclassified pointst\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c5eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation using trained NN snapshots\n",
    "X_np = dataset[0]\n",
    "y_np = dataset[1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.scatter(X_np[:, 0], X_np[:, 1], c=y_np, s=30, cmap=cm_bright, edgecolors=\"k\")\n",
    "\n",
    "x_min, x_max = X_np[:, 0].min() * 1.1, X_np[:, 0].max() * 1.1\n",
    "y_min, y_max = X_np[:, 1].min() * 0.9, X_np[:, 1].max() * 1.1\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "\n",
    "x1 = np.linspace(x_min, x_max, 200)\n",
    "x2 = np.linspace(y_min, y_max, 200)\n",
    "x1_x2 = np.meshgrid(x1, x2)\n",
    "X12_grid = np.c_[x1_x2[0].ravel(), x1_x2[1].ravel()]\n",
    "\n",
    "contour = None\n",
    "\n",
    "def decision_probabilities_nn(grid):\n",
    "    with torch.no_grad():\n",
    "        grid_t = torch.tensor(grid, dtype=torch.float32)\n",
    "        probs = model(grid_t).view(-1).detach().cpu()\n",
    "        probs = np.array(probs.tolist())\n",
    "    return probs\n",
    "\n",
    "def init():\n",
    "    return ()\n",
    "\n",
    "def update(frame):\n",
    "    global contour\n",
    "    model.load_state_dict(state_history[frame])\n",
    "\n",
    "    prob_output = decision_probabilities_nn(X12_grid)\n",
    "    if contour is not None:\n",
    "        try:\n",
    "            for coll in contour.collections:\n",
    "                coll.remove()\n",
    "        except AttributeError:\n",
    "            contour.remove()\n",
    "    contour = ax.contourf(\n",
    "        x1_x2[0],\n",
    "        x1_x2[1],\n",
    "        prob_output.reshape(x1_x2[0].shape),\n",
    "        levels=[0.0, 0.5, 1.0],\n",
    "        alpha=0.2,\n",
    "        colors=[\"red\", \"blue\"],\n",
    "    )\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"NN BCE probability surface - step {frame}, BCE={bce_[frame]:.4f}\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    return ()\n",
    "\n",
    "ani_bce = animation.FuncAnimation(\n",
    "    fig, update, frames=len(state_history), init_func=init, interval=300, blit=False\n",
    ")\n",
    "HTML(ani_bce.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51863dd3",
   "metadata": {},
   "source": [
    "## Things to do:\n",
    "1. Play with the number of hidden neurons `hidden_neurons = XXXX`, for example `2, 4, 8, 16`\n",
    "2. Change the first activation function to `torch.nn.Tanh`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a5d73",
   "metadata": {},
   "source": [
    "# more than one Layer Neural Network\n",
    "\n",
    "We now extend the previous model by introducing two layers of hidden units, leading to a deeper nonlinear representation while retaining the same binary classification objective.\n",
    "\n",
    "As before, we use the Binary Cross-Entropy loss\n",
    "$$\n",
    "{\\cal L}_{\\text{BCE}} = -\\frac{1}{N}\\sum_{i=1}^N\n",
    "\\left(\n",
    "y_i \\log(p_i)\n",
    "+ (1-y_i)\\log(1-p_i)\n",
    "\\right),\n",
    "$$\n",
    "where $y_i \\in \\{0,1\\}$ is the true label and $p_i$ is the predicted probability.\n",
    "\n",
    "\\subsection*{Model Definition}\n",
    "\n",
    "The output probability is given by\n",
    "$$\n",
    "p_0 = \\sigma\\!\\left(\\mathbf{w}^\\top \\mathbf{z}^{(2)}\\right),\n",
    "$$\n",
    "where $\\sigma(\\cdot)$ is the sigmoid activation function and $\\mathbf{z}^{(2)}$ denotes the activations of the second hidden layer.\n",
    "\n",
    "\\subsection*{Second Hidden Layer}\n",
    "\n",
    "The second hidden layer is defined as\n",
    "$$\n",
    "\\mathbf{z}^{(2)} =\n",
    "\\begin{bmatrix}\n",
    "z^{(2)}_1\\\\\n",
    "z^{(2)}_2\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "z^{(2)}_k = \\sigma''\\!\\left(\n",
    "(\\boldsymbol{\\omega}^{(2)}_k)^\\top \\mathbf{z}^{(1)}\n",
    "\\right),\n",
    "\\qquad k=1,2.\n",
    "$$\n",
    "\n",
    "Here, $\\sigma''(\\cdot)$ is an activation function (not necessarily the sigmoid), and $\\boldsymbol{\\omega}^{(2)}_k$ are the weights connecting the first hidden layer to the second.\n",
    "\n",
    "## First Hidden Layer\n",
    "\n",
    "The first hidden layer is defined as\n",
    "$$\n",
    "\\mathbf{z}^{(1)} =\n",
    "\\begin{bmatrix}\n",
    "z^{(1)}_1\\\\\n",
    "z^{(1)}_2\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "z^{(1)}_j = \\sigma'\\!\\left(\n",
    "(\\boldsymbol{\\omega}^{(1)}_j)^\\top \\boldsymbol{\\phi}(\\mathbf{x})\n",
    "\\right),\n",
    "\\qquad j=1,2.\n",
    "$$\n",
    "\n",
    "Here, $\\boldsymbol{\\phi}(\\mathbf{x})$ denotes the input feature representation, and $\\sigma'(\\cdot)$ is an activation function distinct from the sigmoid used at the output layer.\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "This model can be interpreted as a nested composition of nonlinear transformations,\n",
    "$$\n",
    "\\mathbf{x}\n",
    "\\;\\xrightarrow{\\;\\boldsymbol{\\phi}\\;}\n",
    "\\mathbf{z}^{(1)}\n",
    "\\;\\xrightarrow{\\;\\sigma''\\;}\n",
    "\\mathbf{z}^{(2)}\n",
    "\\;\\xrightarrow{\\;\\sigma\\;}\n",
    "p_0,\n",
    "$$\n",
    "where each hidden layer learns a progressively more abstract representation of the input features.\n",
    "\n",
    "## Notational Remark\n",
    "\n",
    "To avoid confusion:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sigma(\\cdot) &&\\text{output sigmoid producing } p_0,\\\\\n",
    "&\\sigma'(\\cdot) &&\\text{activation function of the first hidden layer},\\\\\n",
    "&\\sigma''(\\cdot) &&\\text{activation function of the second hidden layer}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "This function has the following diagram, <br>\n",
    "<img src=\"https://github.com/ChemAI-Lab/AI4Chem/raw/main/website/modules/Figures/two_layers_mlp.png\"\n",
    "     width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c006a",
   "metadata": {},
   "source": [
    "## Chain Rule Interpretation of the Gradient\n",
    "\n",
    "The forward computation of the two-layer neural network can be written as a composition of functions,\n",
    "$$\n",
    "\\mathbf{x}\n",
    "\\;\\xrightarrow{\\;\\boldsymbol{\\phi}\\;}\n",
    "\\mathbf{z}^{(1)}\n",
    "\\;\\xrightarrow{\\;\\sigma''\\;}\n",
    "\\mathbf{z}^{(2)}\n",
    "\\;\\xrightarrow{\\;\\sigma\\;}\n",
    "p_0\n",
    "\\;\\xrightarrow{\\;\\mathcal{L}\\;}\n",
    "\\mathcal{L}.\n",
    "$$\n",
    "\n",
    "The gradient of the loss propagates in the reverse direction,\n",
    "$$\n",
    "\\mathcal{L}\n",
    "\\;\\xleftarrow{\\;}\n",
    "p_0\n",
    "\\;\\xleftarrow{\\;}\n",
    "\\mathbf{z}^{(2)}\n",
    "\\;\\xleftarrow{\\;}\n",
    "\\mathbf{z}^{(1)}\n",
    "\\;\\xleftarrow{\\;}\n",
    "\\boldsymbol{\\phi}(\\mathbf{x}),\n",
    "$$\n",
    "and is obtained by repeated application of the chain rule.\n",
    "\n",
    "## Output Layer\n",
    "\n",
    "Define the output pre-activation\n",
    "$$\n",
    "s = \\mathbf{w}^\\top \\mathbf{z}^{(2)},\n",
    "\\qquad\n",
    "p_0 = \\sigma(s).\n",
    "$$\n",
    "\n",
    "The loss gradient with respect to $s$ is\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial s}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial p_0}\n",
    "\\frac{\\partial p_0}{\\partial s}\n",
    "=\n",
    "p_0 - y.\n",
    "$$\n",
    "\n",
    "The gradient with respect to the output weights is\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial s}\n",
    "\\frac{\\partial s}{\\partial \\mathbf{w}}\n",
    "=\n",
    "(p_0 - y)\\,\\mathbf{z}^{(2)}.\n",
    "$$\n",
    "\n",
    "## Backpropagation to the Second Hidden Layer\n",
    "\n",
    "The gradient with respect to the second hidden activations is\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(2)}}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial s}\n",
    "\\frac{\\partial s}{\\partial \\mathbf{z}^{(2)}}\n",
    "=\n",
    "(p_0 - y)\\,\\mathbf{w}.\n",
    "$$\n",
    "\n",
    "For each unit $k$ in the second hidden layer,\n",
    "$$\n",
    "z^{(2)}_k = \\sigma''(a^{(2)}_k),\n",
    "\\qquad\n",
    "a^{(2)}_k = (\\boldsymbol{\\omega}^{(2)}_k)^\\top \\mathbf{z}^{(1)}.\n",
    "$$\n",
    "\n",
    "Applying the chain rule,\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a^{(2)}_k}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z^{(2)}_k}\n",
    "\\frac{\\partial z^{(2)}_k}{\\partial a^{(2)}_k}\n",
    "=\n",
    "(p_0 - y)\\, w_k \\,\n",
    "\\sigma''(a^{(2)}_k)\\bigl(1-\\sigma''(a^{(2)}_k)\\bigr).\n",
    "$$\n",
    "\n",
    "The gradient with respect to the second-layer weights is\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\omega}^{(2)}_k}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a^{(2)}_k}\n",
    "\\frac{\\partial a^{(2)}_k}{\\partial \\boldsymbol{\\omega}^{(2)}_k}\n",
    "=\n",
    "(p_0 - y)\\, w_k \\,\n",
    "\\sigma''(a^{(2)}_k)\\bigl(1-\\sigma''(a^{(2)}_k)\\bigr)\\,\n",
    "\\mathbf{z}^{(1)}.\n",
    "$$\n",
    "\n",
    "## Backpropagation to the First Hidden Layer\n",
    "\n",
    "The error signal arriving at the first hidden layer is\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(1)}}\n",
    "=\n",
    "\\sum_{k}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a^{(2)}_k}\n",
    "\\boldsymbol{\\omega}^{(2)}_k.\n",
    "$$\n",
    "\n",
    "For each unit $j$ in the first hidden layer,\n",
    "$$\n",
    "z^{(1)}_j = \\sigma'(a^{(1)}_j),\n",
    "\\qquad\n",
    "a^{(1)}_j = (\\boldsymbol{\\omega}^{(1)}_j)^\\top \\boldsymbol{\\phi}(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "Applying the chain rule,\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\boldsymbol{\\omega}^{(1)}_j}\n",
    "=\n",
    "\\left(\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z^{(1)}_j}\n",
    "\\right)\n",
    "\\sigma'(a^{(1)}_j)\\bigl(1-\\sigma'(a^{(1)}_j)\\bigr)\n",
    "\\boldsymbol{\\phi}(\\mathbf{x}),\n",
    "\\qquad j = 1,2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f74d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple two layer neural network classifier using PyTorch (Sequential in a class)\n",
    "class SimpleNNSequential(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=2, output_size=1):\n",
    "        super(SimpleNNSequential, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aefe83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "X = torch.tensor(dataset[0], dtype=torch.float32)\n",
    "y = torch.tensor(dataset[1], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "hidden_neurons = 5\n",
    "model = SimpleNNSequential(input_size=2, hidden_size=hidden_neurons, output_size=1)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "\n",
    "# Store snapshots in memory (no files)\n",
    "state_history = []\n",
    "bce_ = []\n",
    "misclassified_ = []\n",
    "snapshot_every = 10\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % snapshot_every == 0:\n",
    "        state_history.append({k: v.detach().cpu().clone()\n",
    "                             for k, v in model.state_dict().items()})\n",
    "        bce_.append(loss.item())\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        misclassified = (preds != y).sum().item()\n",
    "        misclassified_.append(misclassified)\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Misclassified: {misclassified}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bce123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots of BCE and misclassified samples\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].plot(bce_, color=\"black\", linewidth=2)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"BCE\")\n",
    "\n",
    "axes[1].plot(misclassified_, color=\"red\", linewidth=2)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Misclassified pointst\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338cdcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation using trained NN snapshots\n",
    "X_np = dataset[0]\n",
    "y_np = dataset[1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.scatter(X_np[:, 0], X_np[:, 1], c=y_np,\n",
    "           s=30, cmap=cm_bright, edgecolors=\"k\")\n",
    "\n",
    "x_min, x_max = X_np[:, 0].min() * 1.1, X_np[:, 0].max() * 1.1\n",
    "y_min, y_max = X_np[:, 1].min() * 0.9, X_np[:, 1].max() * 1.1\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "\n",
    "x1 = np.linspace(x_min, x_max, 200)\n",
    "x2 = np.linspace(y_min, y_max, 200)\n",
    "x1_x2 = np.meshgrid(x1, x2)\n",
    "X12_grid = np.c_[x1_x2[0].ravel(), x1_x2[1].ravel()]\n",
    "\n",
    "contour = None\n",
    "\n",
    "\n",
    "def decision_probabilities_nn(grid):\n",
    "    with torch.no_grad():\n",
    "        grid_t = torch.tensor(grid, dtype=torch.float32)\n",
    "        probs = model(grid_t).view(-1).detach().cpu()\n",
    "        probs = np.array(probs.tolist())\n",
    "    return probs\n",
    "\n",
    "\n",
    "def init():\n",
    "    return ()\n",
    "\n",
    "\n",
    "def update(frame):\n",
    "    global contour\n",
    "    model.load_state_dict(state_history[frame])\n",
    "\n",
    "    prob_output = decision_probabilities_nn(X12_grid)\n",
    "    if contour is not None:\n",
    "        try:\n",
    "            for coll in contour.collections:\n",
    "                coll.remove()\n",
    "        except AttributeError:\n",
    "            contour.remove()\n",
    "    contour = ax.contourf(\n",
    "        x1_x2[0],\n",
    "        x1_x2[1],\n",
    "        prob_output.reshape(x1_x2[0].shape),\n",
    "        levels=[0.0, 0.5, 1.0],\n",
    "        alpha=0.2,\n",
    "        colors=[\"red\", \"blue\"],\n",
    "    )\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"NN BCE probability surface - step {frame}, BCE={bce_[frame]:.4f}\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    return ()\n",
    "\n",
    "\n",
    "ani_bce = animation.FuncAnimation(\n",
    "    fig, update, frames=len(state_history), init_func=init, interval=300, blit=False\n",
    ")\n",
    "HTML(ani_bce.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651467fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
