{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808b4cc9",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron or Neural Networks\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChemAI-Lab/AI4Chem/blob/main/website/modules/03-neural_networks.ipynb)\n",
    "\n",
    "**References:**\n",
    "1. **Chapters 5**: [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), C. M. Bishop.\n",
    "2. **Chapter 2**: [Machine Learning in Quantum Sciences](https://arxiv.org/pdf/2204.04198)\n",
    "3. **Chapter 16**: [Probabilistic Machine Learning: An Introduction, K. P. Murphy.](https://probml.github.io/pml-book/book1.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b412f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.datasets import make_moons, make_classification\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b84f15",
   "metadata": {},
   "source": [
    "# Feed-Forward Network Functions\n",
    "\n",
    "Linear models for regression or classification tasks rely on the assumption of having a **fixed** representation of $\\mathbf{x}$ through the use of basis set functions $\\phi$.\n",
    "For binary classification, we saw that such models take the form of, \n",
    "$$\n",
    "y = f(\\mathbf{x},\\mathbf{w}) = \\sigma\\left (\\mathbf{w}^\\top\\phi(\\mathbf{x})\\right ),\n",
    "$$\n",
    "where the function $\\sigma$ carries adds additional non-linearity to $f$. We will discuss later that the chose of $\\sigma$ depends on the task at hand, some work best for classification and some for regression. <br>\n",
    "\n",
    "Obviously, the only possible \"improvement\" to this family of models is making the basis functions $\\phi_j(\\cdot)$ depend on some parameters, which can and will be optimized during the training. <br>\n",
    "In neural networks (NNs), each basis function can be defined as, \n",
    "> non-linear function of a linear combination of the inputs, where the coefficients in the linear combination are adaptive parameters.\n",
    "\n",
    "$$\n",
    "z_j =  \\sigma\\left (a_j \\right ), \\quad \\text{where} \\quad a_j = \\mathbf{\\omega}_j^\\top \\mathbf{x}\n",
    "$$\n",
    "We can see that $z_j$ comes from a composition of functions, first a linear function followed by the activation function ($\\sigma$). $z_j$ are known in NNs as **neurons** and the quantities $a_j$ as **activation**. We shall refer to the parameters $\\mathbf{\\omega}_j$ as **weights** (and **biases**).\n",
    "\n",
    "In the context of NNs, $z_j$ are called **hidden units**, and these values can be linearly combined again to create functions with a higher non-linear character. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0219e802",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "In the perceptron model, we assumed that $\\phi(\\mathbf{x})$ is a non linear differentiable transformation to generate the feature space representation. \n",
    "The jump to modern deep learning models is to assume $\\phi(\\mathbf{x})$ can be learned through some parameters, but for that we require a non-linear transformation, commonly known as **activation function**.\n",
    "Typically, a differentiable nonlinear activation function is used in the hidden layers of a neural network. This allows the model to learn more complex functions than a network trained using a linear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6496473c",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\sigma(x)=\\frac{1}{1+e^{-x}}$\\\n",
    "Derivative: $\\sigma'(x)=\\sigma(x)(1-\\sigma(x))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 400)\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "y = sigmoid(x)\n",
    "dy = y * (1 - y)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('Sigmoid')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "ax[0].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[1].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[0].set_ylabel(rf\"$\\sigma(x)$\", fontsize=14)\n",
    "ax[1].set_ylabel(r\"$\\frac{d\\sigma(x)}{dx}$\", fontsize=14,rotation=0,labelpad=25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d60c7",
   "metadata": {},
   "source": [
    "## Tanh\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$\\\n",
    "Derivative: $\\frac{d}{dx}\\tanh(x)=1-\\tanh^2(x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf73994",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 400)\n",
    "y = np.tanh(x)\n",
    "dy = 1 - y**2\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('Tanh')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "ax[0].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[1].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[0].set_ylabel(rf\"$\\sigma(x)$\", fontsize=14)\n",
    "ax[1].set_ylabel(r\"$\\frac{d\\sigma(x)}{dx}$\",\n",
    "                 fontsize=14, rotation=0, labelpad=25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b6a453",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\mathrm{ReLU}(x)=\\max(0, x)$\\\n",
    "Derivative: $\\mathrm{ReLU}'(x)=\\begin{cases}1,&x>0\\\\0,&x\\le 0\\end{cases}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 400)\n",
    "y = np.maximum(0, x)\n",
    "dy = (x > 0).astype(float)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('ReLU')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "ax[0].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[1].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[0].set_ylabel(rf\"$\\sigma(x)$\", fontsize=14)\n",
    "ax[1].set_ylabel(r\"$\\frac{d\\sigma(x)}{dx}$\",\n",
    "                 fontsize=14, rotation=0, labelpad=25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5140d",
   "metadata": {},
   "source": [
    "## Leaky ReLU\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\mathrm{LReLU}(x)=\\max(\\alpha x, x)$\\\n",
    "Derivative: $\\mathrm{LReLU}'(x)=\\begin{cases}1,&x>0\\\\\\alpha,&x\\le 0\\end{cases}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eece341",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 400)\n",
    "alpha = 0.01\n",
    "y = np.where(x > 0, x, alpha * x)\n",
    "dy = np.where(x > 0, 1.0, alpha)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('Leaky ReLU')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "ax[0].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[1].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[0].set_ylabel(rf\"$\\sigma(x)$\", fontsize=14)\n",
    "ax[1].set_ylabel(r\"$\\frac{d\\sigma(x)}{dx}$\",\n",
    "                 fontsize=14, rotation=0, labelpad=25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fefcf0",
   "metadata": {},
   "source": [
    "## ELU\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\mathrm{ELU}(x)=\\begin{cases}x,&x>0\\\\\\alpha(e^x-1),&x\\le 0\\end{cases}$\\\n",
    "Derivative: $\\mathrm{ELU}'(x)=\\begin{cases}1,&x>0\\\\\\alpha e^x,&x\\le 0\\end{cases}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f6179",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 400)\n",
    "alpha = 1.0\n",
    "y = np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "dy = np.where(x > 0, 1.0, y + alpha)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('ELU')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "ax[0].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[1].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[0].set_ylabel(rf\"$\\sigma(x)$\", fontsize=14)\n",
    "ax[1].set_ylabel(r\"$\\frac{d\\sigma(x)}{dx}$\",\n",
    "                 fontsize=14, rotation=0, labelpad=25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4269bf",
   "metadata": {},
   "source": [
    "## Softplus\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\mathrm{Softplus}(x)=\\ln(1+e^x)$\\\n",
    "Derivative: $\\frac{d}{dx}\\mathrm{Softplus}(x)=\\sigma(x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 400)\n",
    "y = np.log1p(np.exp(x))\n",
    "dy = 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('Softplus')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "ax[0].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[1].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[0].set_ylabel(rf\"$\\sigma(x)$\", fontsize=14)\n",
    "ax[1].set_ylabel(r\"$\\frac{d\\sigma(x)}{dx}$\",\n",
    "                 fontsize=14, rotation=0, labelpad=25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c960dc80",
   "metadata": {},
   "source": [
    "## GELU (approx)\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\mathrm{GELU}(x)=0.5x\\left(1+\\tanh\\left(\\sqrt{\\tfrac{2}{\\pi}}(x+0.044715x^3)\\right)\\right)$\\\n",
    "Derivative: $\\mathrm{GELU}'(x)=0.5(1+\\tanh u)+0.5x(1-\\tanh^2 u)u'$, $u=\\sqrt{\\tfrac{2}{\\pi}}(x+0.044715x^3)$, $u'=\\sqrt{\\tfrac{2}{\\pi}}(1+3\\cdot0.044715x^2)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f707d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 400)\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "y = gelu(x)\n",
    "# Numerical derivative for visualization\n",
    "dy = np.gradient(y, x)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('GELU (approx)')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative (numerical)')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "ax[0].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[1].set_xlabel(rf\"$x$\", fontsize=14)\n",
    "ax[0].set_ylabel(rf\"$\\sigma(x)$\", fontsize=14)\n",
    "ax[1].set_ylabel(r\"$\\frac{d\\sigma(x)}{dx}$\",\n",
    "                 fontsize=14, rotation=0, labelpad=25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a18cc4",
   "metadata": {},
   "source": [
    "# Single Layer Neural Network\n",
    "\n",
    "In its core a NN is a **composition of functions**. Let's build the simplest NN possible, where a single **layer** of hidden units its use for classification. We will use the Binary Cross-Entropy loss, \n",
    "$$\n",
    "{\\cal L}_{\\text{BCE}} = -\\frac{1}{N}\\sum_i^N \\left (y_i\\log(p_i) - (1 - y_i)\\log(1- p_i) \\right ), \n",
    "$$\n",
    "where $y_i$ is true binary label, $y_i = [0,1]$, $p_i$ is the predicted probability.<br>\n",
    "\n",
    "\n",
    "Recap of our previous model for classification, \n",
    "$$\n",
    "p_0 = \\sigma\\left ( a_j \\right )  = \\sigma\\left (\\mathbf{w}^\\top\\phi(\\mathbf{x}) \\right ),\n",
    "$$\n",
    "this model has a **single** hidden neuron. We the sigmoid activation function.\n",
    "\n",
    "Let's consider now a two hidden neuron representation for this same model, \n",
    "$$\n",
    "p_0 =  \\sigma\\left (\\mathbf{w}^\\top\\mathbf{z} \\right ),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbf{z} = \\underbrace{\\begin{bmatrix}\n",
    " z_1 \\\\\n",
    "z_2\n",
    "\\end{bmatrix}}_{\\text{hidden neurons}}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "z_j = \\sigma'(\\mathbf{\\omega}_j^\\top \\phi(\\mathbf{x})).\n",
    "$$\n",
    "For notation clearness, $\\sigma'$ represent an activation function different from the sigmoid one that we use to predict the probability of $p_0$.\n",
    "This function has the following diagram, <br>\n",
    "<img src=\"https://github.com/ChemAI-Lab/AI4Chem/raw/main/website/modules/Figures/single_layer_mlp.png\"\n",
    "     width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd15d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(name=\"linear\"):\n",
    "    if name == \"linear\":\n",
    "        X, y = make_classification(\n",
    "            n_features=2, n_redundant=0, n_informative=2,\n",
    "            n_clusters_per_class=1, class_sep=1.1, flip_y=0,\n",
    "            random_state=1\n",
    "        )\n",
    "\n",
    "        rng = np.random.RandomState(2)\n",
    "        X += 2 * rng.uniform(size=X.shape)\n",
    "        linearly_separable = (X, y)\n",
    "        dataset = linearly_separable\n",
    "    elif name == \"moons\":\n",
    "        X, y = make_moons(noise=0.1, n_samples=200, )\n",
    "        dataset = (X, y)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset_name = \"moons\"\n",
    "dataset = get_data(dataset_name)\n",
    "\n",
    "cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(\n",
    "    dataset[0][:, 0], dataset[0][:, 1],\n",
    "    c=dataset[1], cmap=cm_bright,\n",
    "    edgecolor='k', s=100\n",
    ")\n",
    "if dataset_name == \"linear\":\n",
    "    plt.title(\"Linearly separable data\", fontsize=16)\n",
    "elif dataset_name == \"moons\":\n",
    "    plt.title(\"Non-linearly separable data\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c00f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple two layer neural network classifier using PyTorch\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b58ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "X = torch.tensor(dataset[0], dtype=torch.float32)\n",
    "y = torch.tensor(dataset[1], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "hidden_neurons = 10\n",
    "model = SimpleNN(input_size=2, hidden_size=hidden_neurons, output_size=1)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "\n",
    "# Store snapshots in memory (no files)\n",
    "state_history = []\n",
    "bce_ = []\n",
    "misclassified_ = []\n",
    "snapshot_every = 10\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % snapshot_every == 0:\n",
    "        state_history.append({k: v.detach().cpu().clone() for k, v in model.state_dict().items()})\n",
    "        bce_.append(loss.item())\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        misclassified = (preds != y).sum().item()\n",
    "        misclassified_.append(misclassified)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Misclassified: {misclassified}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c5eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation using trained NN snapshots\n",
    "X_np = dataset[0]\n",
    "y_np = dataset[1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.scatter(X_np[:, 0], X_np[:, 1], c=y_np, s=30, cmap=cm_bright, edgecolors=\"k\")\n",
    "\n",
    "x_min, x_max = X_np[:, 0].min() * 1.1, X_np[:, 0].max() * 1.1\n",
    "y_min, y_max = X_np[:, 1].min() * 0.9, X_np[:, 1].max() * 1.1\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "\n",
    "x1 = np.linspace(x_min, x_max, 200)\n",
    "x2 = np.linspace(y_min, y_max, 200)\n",
    "x1_x2 = np.meshgrid(x1, x2)\n",
    "X12_grid = np.c_[x1_x2[0].ravel(), x1_x2[1].ravel()]\n",
    "\n",
    "contour = None\n",
    "\n",
    "def decision_probabilities_nn(grid):\n",
    "    with torch.no_grad():\n",
    "        grid_t = torch.tensor(grid, dtype=torch.float32)\n",
    "        probs = model(grid_t).view(-1).detach().cpu()\n",
    "        probs = np.array(probs.tolist())\n",
    "    return probs\n",
    "\n",
    "def init():\n",
    "    return ()\n",
    "\n",
    "def update(frame):\n",
    "    global contour\n",
    "    model.load_state_dict(state_history[frame])\n",
    "\n",
    "    prob_output = decision_probabilities_nn(X12_grid)\n",
    "    if contour is not None:\n",
    "        try:\n",
    "            for coll in contour.collections:\n",
    "                coll.remove()\n",
    "        except AttributeError:\n",
    "            contour.remove()\n",
    "    contour = ax.contourf(\n",
    "        x1_x2[0],\n",
    "        x1_x2[1],\n",
    "        prob_output.reshape(x1_x2[0].shape),\n",
    "        levels=[0.0, 0.5, 1.0],\n",
    "        alpha=0.2,\n",
    "        colors=[\"red\", \"blue\"],\n",
    "    )\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"NN BCE probability surface - step {frame}, BCE={bce_[frame]:.4f}\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    return ()\n",
    "\n",
    "ani_bce = animation.FuncAnimation(\n",
    "    fig, update, frames=len(state_history), init_func=init, interval=300, blit=False\n",
    ")\n",
    "HTML(ani_bce.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17445d18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
