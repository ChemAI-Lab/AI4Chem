{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808b4cc9",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron or Neural Networks\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChemAI-Lab/AI4Chem/blob/main/website/modules/03-neural_networks.ipynb)\n",
    "\n",
    "**References:**\n",
    "1. **Chapters 5**: [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), C. M. Bishop.\n",
    "2. **Chapter 2**: [Machine Learning in Quantum Sciences](https://arxiv.org/pdf/2204.04198)\n",
    "3. **Chapter 16**: [Probabilistic Machine Learning: An Introduction, K. P. Murphy.](https://probml.github.io/pml-book/book1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0219e802",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "In the perceptron model, we assumed that $\\phi(\\mathbf{x})$ is a non linear differentiable transformation to generate the feature space representation. \n",
    "The jump to modern deep learning models is to assume $\\phi(\\mathbf{x})$ can be learned through some parameters, but for that we require a non-linear transformation, commonly known as **activation function**.\n",
    "Typically, a differentiable nonlinear activation function is used in the hidden layers of a neural network. This allows the model to learn more complex functions than a network trained using a linear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6496473c",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\sigma(x)=\\frac{1}{1+e^{-x}}$\\\n",
    "Derivative: $\\sigma'(x)=\\sigma(x)(1-\\sigma(x))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(-6, 6, 400)\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "y = sigmoid(x)\n",
    "dy = y * (1 - y)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('Sigmoid')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d60c7",
   "metadata": {},
   "source": [
    "## Tanh\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$\\\n",
    "Derivative: $\\frac{d}{dx}\\tanh(x)=1-\\tanh^2(x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf73994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(-6, 6, 400)\n",
    "y = np.tanh(x)\n",
    "dy = 1 - y**2\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('Tanh')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b6a453",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\mathrm{ReLU}(x)=\\max(0, x)$\\\n",
    "Derivative: $\\mathrm{ReLU}'(x)=\\begin{cases}1,&x>0\\\\0,&x\\le 0\\end{cases}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(-6, 6, 400)\n",
    "y = np.maximum(0, x)\n",
    "dy = (x > 0).astype(float)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('ReLU')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5140d",
   "metadata": {},
   "source": [
    "## Leaky ReLU\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\mathrm{LReLU}(x)=\\max(\\alpha x, x)$\\\n",
    "Derivative: $\\mathrm{LReLU}'(x)=\\begin{cases}1,&x>0\\\\\\alpha,&x\\le 0\\end{cases}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eece341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(-6, 6, 400)\n",
    "alpha = 0.01\n",
    "y = np.where(x > 0, x, alpha * x)\n",
    "dy = np.where(x > 0, 1.0, alpha)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('Leaky ReLU')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fefcf0",
   "metadata": {},
   "source": [
    "## ELU\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\mathrm{ELU}(x)=\\begin{cases}x,&x>0\\\\\\alpha(e^x-1),&x\\le 0\\end{cases}$\\\n",
    "Derivative: $\\mathrm{ELU}'(x)=\\begin{cases}1,&x>0\\\\\\alpha e^x,&x\\le 0\\end{cases}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f6179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(-6, 6, 400)\n",
    "alpha = 1.0\n",
    "y = np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "dy = np.where(x > 0, 1.0, y + alpha)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('ELU')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4269bf",
   "metadata": {},
   "source": [
    "## Softplus\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\mathrm{Softplus}(x)=\\ln(1+e^x)$\\\n",
    "Derivative: $\\frac{d}{dx}\\mathrm{Softplus}(x)=\\sigma(x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(-6, 6, 400)\n",
    "y = np.log1p(np.exp(x))\n",
    "dy = 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('Softplus')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c960dc80",
   "metadata": {},
   "source": [
    "## GELU (approx)\n",
    "\n",
    "\n",
    "\n",
    "Equation: $\\mathrm{GELU}(x)=0.5x\\left(1+\\tanh\\left(\\sqrt{\\tfrac{2}{\\pi}}(x+0.044715x^3)\\right)\\right)$\\\n",
    "Derivative: $\\mathrm{GELU}'(x)=0.5(1+\\tanh u)+0.5x(1-\\tanh^2 u)u'$, $u=\\sqrt{\\tfrac{2}{\\pi}}(x+0.044715x^3)$, $u'=\\sqrt{\\tfrac{2}{\\pi}}(1+3\\cdot0.044715x^2)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f707d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(-6, 6, 400)\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "y = gelu(x)\n",
    "# Numerical derivative for visualization\n",
    "dy = np.gradient(y, x)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax[0].plot(x, y, color='k')\n",
    "ax[0].set_title('GELU (approx)')\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "ax[1].plot(x, dy, color='k')\n",
    "ax[1].set_title('Derivative (numerical)')\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
