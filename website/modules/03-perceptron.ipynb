{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbbdf4e2",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChemAI-Lab/AI4Chem/blob/main/website/modules/03-perceptron.ipynb)\n",
    "\n",
    "**References:**\n",
    "1. **Chapters 4**: [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), C. M. Bishop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7959c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_moons, make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117d09fa",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "To motivate the introduction to neural networks, we have to pay homage to the first \"architecture\", the **perceptron**. \n",
    "First introduced in 1962 by Rosenblatt as a linear discriminant model, this model occupies an important place in the history of pattern recognition algorithms.\n",
    "A discrimination model aims to predict the class of a data point, meaning the output of the model is a binary variable, \n",
    "$$\n",
    "y = \\sigma(\\mathbf{w}^\\top\\phi(\\mathbf{x})),\n",
    "$$\n",
    "where $y$ can only have two values, $y = {-1,1}$, e.g., $y=-1$ means class 1 and $y=1$ class 2.<br>\n",
    "$\\mathbf{w}$ are the parameters of the model.\n",
    "$\\phi(\\mathbf{x})$ the **feature space** representation of $\\mathbf{x}$ using a fixed nonlinear transformation $\\phi()$. <br>\n",
    "$\\sigma()$ has a special name, **activation function**. We will see later that $\\sigma$ will dictate the special properties of neural networks. \n",
    "\n",
    "For the perceptron model, $\\sigma$ has is a sep function,\n",
    "$$\n",
    "\\sigma(x) =  \\left\\{\\begin{matrix}\n",
    " +1,\\quad x \\geq 0\\\\\n",
    " -1,\\quad x < 0\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "From this setup, the only we can \"tune\" is $\\mathbf{w}$, depending on the value it will be if $y$ will assign class 1 or 2,\n",
    "$$\n",
    "y = \\left\\{\\begin{matrix}\n",
    " +1,\\quad \\mathbf{w}^\\top\\phi(\\mathbf{x}) \\geq 0 \\\\\n",
    " -1, \\quad \\mathbf{w}^\\top\\phi(\\mathbf{x}) < 0\n",
    "\\end{matrix}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(name=\"linear\"):\n",
    "    if name == \"linear\":\n",
    "        X, y = make_classification(\n",
    "            n_features=2, n_redundant=0, n_informative=2,\n",
    "            n_clusters_per_class=1, class_sep=1.2, flip_y=0,\n",
    "            random_state=1\n",
    "        )\n",
    "\n",
    "        rng = np.random.RandomState(2)\n",
    "        X += 2 * rng.uniform(size=X.shape)\n",
    "        linearly_separable = (X, y)\n",
    "        dataset = linearly_separable\n",
    "    elif name == \"moons\":\n",
    "        X, y = make_moons(noise=0.1, n_samples=200, )\n",
    "        dataset = (X, y)\n",
    "    return dataset\n",
    "\n",
    "dataset_name = \"linear\"\n",
    "dataset = get_data(dataset_name)\n",
    "\n",
    "cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(\n",
    "    dataset[0][:, 0], dataset[0][:, 1],\n",
    "    c=dataset[1], cmap=cm_bright,\n",
    "    edgecolor='k', s=100\n",
    ")\n",
    "if dataset_name == \"linear\":\n",
    "    plt.title(\"Linearly separable data\", fontsize=16)\n",
    "elif dataset_name == \"moons\":\n",
    "    plt.title(\"Non-linearly separable data\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e9423",
   "metadata": {},
   "source": [
    "# Total Number of Misclassified Points\n",
    "\n",
    "The perceptron algorithms uses the total number if wrongly classified points as the error function to adjust the value of $\\mathbf{w}$.\n",
    "However, this \"simple\" error function is hard to optimize as piecewise constant function of $\\mathbf{w}$, with discontinuities wherever a change in w causes the decision boundary to move across one of the data points. We will see in a bit that depending on the value of $\\mathbf{w}$ the number of misclassified points change, because of this we can not used standard gradient descent methods like commonly used in BCE. \n",
    "\n",
    "## perceptron Criterion\n",
    "The perceptron criterion associates zero error with any pattern that is correctly classified, whereas for a misclassified pattern $\\mathbf{x_i}$ it tries to minimize the quantity $-\\mathbf{w}^\\top\\phi(\\mathbf{x})$. The perceptron criterion is therefore given by, \n",
    "$$\n",
    "{\\cal L}(\\mathbf{w}) = - \\sum_j^M \\mathbf{w}^\\top\\phi(\\mathbf{x}_i)\\,y_i, \n",
    "$$\n",
    "where $\\sum_j^M$ is only over the misclassified points. For each point, $\\mathbf{w}^\\top\\phi(\\mathbf{x}_i)\\,y_i$ if correctly predicted is positive, meaning we aim to minimize the negative of $\\mathbf{w}^\\top\\phi(\\mathbf{x}_i)\\,y_i$.\n",
    "\n",
    "The optimization of $\\mathbf{w}$ can be carried using gradient-based methods, $\\nabla_{\\mathbf{w}}{\\cal L}(\\mathbf{w})$, where given the linearity of $\\mathbf{w}^\\top\\phi(\\mathbf{x}_i)\\,y_i$, \n",
    "$$\n",
    "\\nabla_{\\mathbf{w}}{\\cal L}(\\mathbf{w}) = - \\sum_j^M \\phi(\\mathbf{x}_i)\\,y_i.\n",
    "$$\n",
    "We can use $\\nabla_{\\mathbf{w}}{\\cal L}$ to \"**update**\" $\\mathbf{w}$ by moving in the negative direction, \n",
    "$$\n",
    "\\mathbf{w}_{t+1} = \\mathbf{w}_{i} - \\eta \\nabla_{\\mathbf{w}}{\\cal L},\n",
    "$$\n",
    "where $\\eta$ is a scalar parameter commonly known as the **learning rate parameter**.\n",
    "Note that, as the weight vector evolves during training, the set of patterns that are misclassified will change.\n",
    "\n",
    "\n",
    "## Perceptron Algorithm\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textbf{Perceptron Algorithm} \\\\ \\hline\n",
    "\\textbf{Input: } \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N,\\; y_i \\in \\{-1,+1\\},\\; \\eta,\\; T \\\\\n",
    "\\textbf{Initialize: } \\mathbf{w} \\\\[0.4em]\n",
    "\\textbf{for } t = 1 \\textbf{ to } T \\quad\\text{ // iterations} \\\\ \n",
    "\\quad \\textbf{for } i = 1 \\textbf{ to } N \\quad\\text{ // loop over dataset}\\\\\n",
    "\\quad\\quad \\textbf{if } y_i(\\mathbf{w}^\\top \\mathbf{x}_i) \\le 0  \\quad\\text{ // misclassified points}\\\\\n",
    "\\quad\\quad\\quad \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta y_i \\mathbf{x}_i  \\quad\\text{ // gradient update} \\\\\n",
    "\\quad\\quad \\textbf{end if} \\\\\n",
    "\\quad \\textbf{end for} \\\\\n",
    "\\textbf{end for} \\\\\n",
    "\\textbf{Output: } \\mathbf{w}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dbf5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_train(X_in, y_in, epochs=20, eta=0.01, seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    w = rng.randn(X_in.shape[1], 1)\n",
    "    history = []\n",
    "    errors = []\n",
    "    for epoch in range(epochs):\n",
    "        m_tot = 0\n",
    "        for i in range(X_in.shape[0]):\n",
    "            xi = X_in[i].reshape(-1, 1)\n",
    "            yi = y_in[i]\n",
    "            linear_output = np.dot(w.T, xi)[0, 0]\n",
    "            if yi * linear_output <= 0:\n",
    "                w += eta * yi * xi\n",
    "                m_tot += 1\n",
    "        history.append(w.copy())\n",
    "        errors.append(m_tot)\n",
    "        print(f\"Epoch {epoch}: Number of misclassified samples={m_tot}, w={w.flatten()}\")\n",
    "    return w, history, errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be33977",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[0]\n",
    "y = dataset[1]\n",
    "y_labels = np.where(y == 0, -1, 1)\n",
    "\n",
    "use_bias = True\n",
    "eta = 2E-3\n",
    "epochs = 100\n",
    "\n",
    "if use_bias:\n",
    "    X_train = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "else:\n",
    "    X_train = X\n",
    "\n",
    "w, w_history, error_ = perceptron_train(\n",
    "    X_train, y_labels, epochs=epochs, eta=eta\n",
    ")\n",
    "y_pred = np.sign(np.dot(X_train, w)).flatten()\n",
    "\n",
    "def decision_values(grid, w, use_bias):\n",
    "    if use_bias:\n",
    "        grid_aug = np.hstack((grid, np.ones((grid.shape[0], 1))))\n",
    "        return np.dot(grid_aug, w)\n",
    "    return np.dot(grid, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(X[:, 0].min() * 1.1, X[:, 0].max() * 1.1, 200)\n",
    "x2 = np.linspace(X[:, 1].min() * 0.9, X[:, 1].max() * 1.1, 200)\n",
    "x1_x2 = np.meshgrid(x1, x2)\n",
    "X12_grid = np.c_[x1_x2[0].ravel(), x1_x2[1].ravel()]\n",
    "linear_output = decision_values(X12_grid, w, use_bias)\n",
    "\n",
    "fig, (ax_left, ax_right) = plt.subplots(1, 2, figsize=(16, 4), gridspec_kw={\"width_ratios\": [2, 1]})\n",
    "\n",
    "ax_left.scatter(X[:, 0], X[:, 1], c=y_labels, s=30, cmap=cm_bright, edgecolors=\"k\")\n",
    "ax_left.contourf(\n",
    "    x1_x2[0],\n",
    "    x1_x2[1],\n",
    "    linear_output.reshape(x1_x2[0].shape),\n",
    "    levels=[-1e5, 0, 1e5],\n",
    "    alpha=0.2,\n",
    "    colors=[\"red\", \"blue\"],\n",
    ")\n",
    "ax_left.set_xlabel(rf\"$x_1$\", fontsize=16)\n",
    "ax_left.set_ylabel(rf\"$x_2$\", fontsize=16)\n",
    "\n",
    "mis_idx = np.where(y_labels != y_pred)[0]\n",
    "if mis_idx.size:\n",
    "    ax_left.scatter(\n",
    "        X[mis_idx, 0], X[mis_idx, 1],\n",
    "        c=\"none\", edgecolors=\"k\", s=100, linewidths=2, marker=\"o\"\n",
    "    )\n",
    "\n",
    "ax_left.set_title(\"Perceptron decision regions (final)\", fontsize=14)\n",
    "\n",
    "ax_right.plot(range(len(error_)), error_, marker=\"o\", color=\"k\")\n",
    "ax_right.set_xlabel(\"Epoch\", fontsize=14)\n",
    "ax_right.set_ylabel(\"Misclassified samples\", fontsize=14)\n",
    "ax_right.grid(True, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d6b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_left = plt.subplots(figsize=(8, 4))\n",
    "ax_left.scatter(X[:, 0], X[:, 1], c=y_labels, s=30, cmap=cm_bright, edgecolors=\"k\")\n",
    "\n",
    "x_min, x_max = X[:, 0].min() * 1.1, X[:, 0].max() * 1.1\n",
    "y_min, y_max = X[:, 1].min() * 0.9, X[:, 1].max() * 1.1\n",
    "ax_left.set_xlim(x_min, x_max)\n",
    "ax_left.set_ylim(y_min, y_max)\n",
    "\n",
    "x1 = np.linspace(x_min, x_max, 200)\n",
    "x2 = np.linspace(y_min, y_max, 200)\n",
    "x1_x2 = np.meshgrid(x1, x2)\n",
    "X12_grid = np.c_[x1_x2[0].ravel(), x1_x2[1].ravel()]\n",
    "\n",
    "line, = ax_left.plot([], [], \"k-\", linewidth=2)\n",
    "contour = None\n",
    "mis_scatter = None\n",
    "\n",
    "def line_from_w(w):\n",
    "    if use_bias:\n",
    "        w0, w1, b = w.flatten()\n",
    "    else:\n",
    "        w0, w1 = w.flatten()\n",
    "        b = 0.0\n",
    "    if abs(w1) < 1e-12:\n",
    "        x_vert = -b / w0 if abs(w0) > 1e-12 else 0.0\n",
    "        return np.array([x_vert, x_vert]), np.array([y_min, y_max])\n",
    "    x_line = np.array([x_min, x_max])\n",
    "    y_line = -(w0 * x_line + b) / w1\n",
    "    return x_line, y_line\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    return (line,)\n",
    "\n",
    "def update(frame):\n",
    "    global contour, mis_scatter\n",
    "    w_frame = w_history[frame]\n",
    "    x_line, y_line = line_from_w(w_frame)\n",
    "    line.set_data(x_line, y_line)\n",
    "\n",
    "    linear_output = decision_values(X12_grid, w_frame, use_bias)\n",
    "    if contour is not None:\n",
    "        contour.remove()\n",
    "    contour = ax_left.contourf(\n",
    "        x1_x2[0],\n",
    "        x1_x2[1],\n",
    "        linear_output.reshape(x1_x2[0].shape),\n",
    "        levels=[-1e5, 0, 1e5],\n",
    "        alpha=0.2,\n",
    "        colors=[\"red\", \"blue\"],\n",
    "    )\n",
    "\n",
    "    y_pred_frame = np.sign(np.dot(X_train, w_frame)).flatten()\n",
    "    mis_idx = np.where(y_labels != y_pred_frame)[0]\n",
    "    if mis_scatter is not None:\n",
    "        mis_scatter.remove()\n",
    "        mis_scatter = None\n",
    "    if mis_idx.size:\n",
    "        mis_scatter = ax_left.scatter(\n",
    "            X[mis_idx, 0], X[mis_idx, 1],\n",
    "            c=\"none\", edgecolors=\"k\", s=100, linewidths=2, marker=\"o\"\n",
    "        )\n",
    "\n",
    "    ax_left.set_title(f\"Perceptron boundary - epoch {frame}\", fontsize=14)\n",
    "    return (line,)\n",
    "\n",
    "ani_line = animation.FuncAnimation(\n",
    "    fig, update, frames=len(w_history), init_func=init, interval=300, blit=False\n",
    ")\n",
    "HTML(ani_line.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f1950",
   "metadata": {},
   "source": [
    "# Sigmoid and BCE\n",
    "\n",
    "In binary classification, we commonly used the Binary Cross-Entropy error,\n",
    "$$\n",
    "{\\cal L}_{\\text{BCE}} = -\\frac{1}{N}\\sum_i^N \\left (y_i\\log(p_i) - (1 - y_i)\\log(1- p_i) \\right ), \n",
    "$$\n",
    "where $y_i$ is true binary label, $y_i = [0,1]$, $p_i$ is the predicted probability. \n",
    "Commonly we can use the **sigmoid function**, \n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}.\n",
    "$$\n",
    "\n",
    "Activation functions are one of the **key** components in modern deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15b77fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "x_grid = np.linspace(-10, 10, 200)\n",
    "y = sigmoid(x_grid)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x_grid, y, color='k', lw=2)\n",
    "plt.text(6., 0.9, \"Class 2\", fontsize=16)\n",
    "plt.text(-7.5, .05, \"Class 1\", fontsize=16)\n",
    "plt.title(\"Sigmoid function\", fontsize=16)\n",
    "plt.ylabel(rf\"$\\sigma(x)$\", fontsize=16)\n",
    "plt.xlabel(\"x\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56505ed",
   "metadata": {},
   "source": [
    "## Gradient of the Loss (Single Sample)\n",
    "\n",
    "For a single data point $(\\mathbf{x}, y)$, define\n",
    "$$\n",
    "z = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}),\n",
    "\\qquad\n",
    "p = \\sigma(z),\n",
    "\\qquad\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}.\n",
    "$$\n",
    "\n",
    "The binary cross-entropy loss is\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{w})\n",
    "= -\\bigl[\n",
    "y \\log p\n",
    "+ (1-y)\\log(1-p)\n",
    "\\bigr].\n",
    "$$\n",
    "\n",
    "Using the chain rule,\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial p}\n",
    "\\frac{\\partial p}{\\partial z}\n",
    "\\frac{\\partial z}{\\partial \\mathbf{w}}.\n",
    "$$\n",
    "\n",
    "Each term is given by\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial p}\n",
    "= -\\left(\n",
    "\\frac{y}{p} - \\frac{1-y}{1-p}\n",
    "\\right),\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial p}{\\partial z}\n",
    "= p(1-p),\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial \\mathbf{w}}\n",
    "= \\boldsymbol{\\phi}(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "Combining terms yields\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}}\n",
    "= \\bigl(p - y\\bigr)\\,\\boldsymbol{\\phi}(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "The full gradient is simply for $N$ data points is, \n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\text{BCE}}\n",
    "= \\frac{1}{N}\\sum_{i=1}^N\n",
    "\\bigl(\\sigma(z_i) - y_i\\bigr)\\,\n",
    "\\boldsymbol{\\phi}(\\mathbf{x}_i),\n",
    "\\qquad\n",
    "z_i = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0e2063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross-entropy (BCE) optimization with a single gradient step\n",
    "\n",
    "def bce_loss(y_true, y_prob, eps=1e-12):\n",
    "    y_prob = np.clip(y_prob, eps, 1 - eps)\n",
    "    return -np.mean(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob))\n",
    "\n",
    "def bce_grad_step(Phi_X, y, w, lr=0.1):\n",
    "    # Single gradient descent step for logistic regression\n",
    "    z = X @ w\n",
    "    y_prob = sigmoid(z)\n",
    "    error = y_prob - y\n",
    "    grad_w = Phi_X.T @ error / X.shape[0]\n",
    "    w = w - lr * grad_w # gradient step\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b7c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "X = dataset[0]\n",
    "y = dataset[1]\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "use_bias = True\n",
    "\n",
    "if use_bias:\n",
    "    X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "\n",
    "# initialize weights\n",
    "\n",
    "# w = np.zeros((X.shape[1],1))\n",
    "seed = 0\n",
    "rng = np.random.RandomState(seed)\n",
    "w = rng.randn(X.shape[1], 1)\n",
    "\n",
    "bce_ = []\n",
    "w_ = []\n",
    "for epoch in range(epochs):\n",
    "    w = bce_grad_step(X, y, w, lr=0.1)\n",
    "    y_prob = sigmoid(np.dot(X, w))\n",
    "    loss = bce_loss(y, y_prob)\n",
    "    bce_.append(loss)\n",
    "    w_.append(w.copy())\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: BCE Loss={loss:.4f}\")\n",
    "\n",
    "# plot BCE\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(len(bce_)), bce_, marker=\"o\", color=\"k\")\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Binary Cross-Entropy Loss\", fontsize=14)\n",
    "plt.grid(True, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation using precomputed weights w_ (bias is included in X)\n",
    "\n",
    "# Expect w_ to be a list/array of shape (steps, n_features)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cm_bright, edgecolors=\"k\")\n",
    "\n",
    "x_min, x_max = X[:, 0].min() * 1.1, X[:, 0].max() * 1.1\n",
    "y_min, y_max = X[:, 1].min() * 0.9, X[:, 1].max() * 1.1\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "\n",
    "x1 = np.linspace(x_min, x_max, 200)\n",
    "x2 = np.linspace(y_min, y_max, 200)\n",
    "x1_x2 = np.meshgrid(x1, x2)\n",
    "X12_grid = np.c_[x1_x2[0].ravel(), x1_x2[1].ravel()]\n",
    "\n",
    "line, = ax.plot([], [], \"k-\", linewidth=2)\n",
    "contour = None\n",
    "\n",
    "def line_from_w(w):\n",
    "    if use_bias:\n",
    "        w0, w1, b = w.flatten()\n",
    "    else:\n",
    "        w0, w1 = w.flatten()\n",
    "        b = 0.0\n",
    "    if abs(w1) < 1e-12:\n",
    "        x_vert = -b / w0 if abs(w0) > 1e-12 else 0.0\n",
    "        return np.array([x_vert, x_vert]), np.array([y_min, y_max])\n",
    "    x_line = np.array([x_min, x_max])\n",
    "    y_line = -(w0 * x_line + b) / w1\n",
    "    return x_line, y_line\n",
    "\n",
    "def decision_probabilities(grid, w, use_bias):\n",
    "    if use_bias:\n",
    "        grid_aug = np.hstack((grid, np.ones((grid.shape[0], 1))))\n",
    "        scores = np.dot(grid_aug, w)\n",
    "    else:\n",
    "        scores = np.dot(grid, w)\n",
    "    return sigmoid(scores)\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    return (line,)\n",
    "\n",
    "def update(frame):\n",
    "    global contour\n",
    "    w_frame = np.asarray(w_[frame])\n",
    "    bce_value = bce_[frame]\n",
    "    x_line, y_line = line_from_w(w_frame)\n",
    "    line.set_data(x_line, y_line)\n",
    "\n",
    "    prob_output = decision_probabilities(X12_grid, w_frame, use_bias)\n",
    "    if contour is not None:\n",
    "        contour.remove()\n",
    "    contour = ax.contourf(\n",
    "        x1_x2[0],\n",
    "        x1_x2[1],\n",
    "        prob_output.reshape(x1_x2[0].shape),\n",
    "        levels=[0.0, 0.5, 1.0],\n",
    "        alpha=0.2,\n",
    "        colors=[\"red\", \"blue\"],\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"BCE probability surface - step {frame}, BCE={bce_value:.4f}\", fontsize=14)\n",
    "    return (line,)\n",
    "\n",
    "ani_bce = animation.FuncAnimation(\n",
    "    fig, update, frames=len(w_), init_func=init, interval=300, blit=False\n",
    ")\n",
    "HTML(ani_bce.to_jshtml())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
