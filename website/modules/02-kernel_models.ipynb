{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71415a16",
   "metadata": {},
   "source": [
    "# Kernel models\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChemAI-Lab/AI4Chem/blob/main/website/modules/02-kernel_models.ipynb)\n",
    "\n",
    "**References:**\n",
    "1. **Chapters 6**: [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), C. M. Bishop.\n",
    "2. **Chapter 2**:  [Gaussian Processes for Machine LearningOpen Access](https://direct.mit.edu/books/oa-monograph-pdf/2514321/book_9780262256834.pdf), C. E. Rasmussen, C. K. I. Williams\n",
    "3. **Chapter 4**: [Machine Learning in Quantum Sciences](https://arxiv.org/pdf/2204.04198)\n",
    "4. [**The Kernel Cookbook**](https://www.cs.toronto.edu/~duvenaud/cookbook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b42d8",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "So far we have covered regression models of the form, \n",
    "$$\n",
    "f(\\mathbf{\\phi}(\\mathbf{x}),\\mathbf{w}) = \\mathbf{w}^\\top \\mathbf{\\phi}(\\mathbf{x})= \\sum_{i}^d w_i \\, \\phi_i(\\mathbf{x}),\n",
    "$$\n",
    "where the set of non-linear transformations $\\phi_i$ when chosen properly can be powerful regression models.\n",
    "The change from \n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\underbrace{\\begin{bmatrix}\n",
    " x_1 \\\\\n",
    " \\vdots \\\\\n",
    "x_d\n",
    "\\end{bmatrix}}_{\\text{input space}} \\;\\;  \\to \\;\\; \\mathbf{\\phi}(\\mathbf{x}) = \\underbrace{\\begin{bmatrix}\n",
    "\\phi_1(\\mathbf{x}) \\\\\n",
    " \\vdots \\\\\n",
    "\\phi_{d'}(\\mathbf{x})\n",
    "\\end{bmatrix}}_{\\text{feature space}}\n",
    "\n",
    "$$\n",
    "is also know as **feature transformation**. For example, polynomial expansion. <br>\n",
    "\n",
    "\n",
    "<!-- There is another class of models, where prediction is done through a linear combination of a **kernel function** evaluated on at the training data points.  -->\n",
    "\n",
    "* We can construct another alternative approach to build a regression model using the **distance** or **similarity** between training data points; known as the **kernel** function. \n",
    "* The kernel function allows us to *implicitly* use a \"high-dimensional\" feature space; e.g., infinite polynomial expansion. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc181039",
   "metadata": {},
   "source": [
    "# Kernel trick\n",
    "Here, we present a~derivation of the kernel trick following [Appendix B in Machine Learning in Quantum Sciences](https://arxiv.org/pdf/2204.04198).\n",
    "\n",
    "1. Let's define the ridge regression loss function\n",
    "$$\n",
    "\\begin{equation}\n",
    "    {\\mathcal L}(\\mathbf{w},\\mathbf{X},\\mathbf{y}) = \\left\\| \\mathbf{X}\\mathbf{w} - \\mathbf{y} \\right\\|_2^2  + \\lambda \\left\\| \\mathbf{w} \\right\\|_2^2,\n",
    "\\end{equation}\n",
    "$$\n",
    "where, \n",
    "$$\n",
    "\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "\\mathbf{x}_1^\\top \\\\\n",
    "\\mathbf{x}_2^\\top \\\\\n",
    "\\vdots \\\\\n",
    " \\mathbf{x}_{N}^\\top \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} &\\cdots & x_{1,d}\\\\\n",
    "x_{2,1} & x_{2,2}&\\cdots & x_{2,d}\\\\\n",
    "\\vdots & \\vdots &\\ddots & \\vdots\\\\\n",
    "x_{N,1} & x_{N,2}&\\cdots & x_{N,d}\\\\\n",
    "\\end{bmatrix} \\text{ and  } \\mathbf{y} = \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_{N}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "The optimal set of parameters $\\mathbf{w}^*$ is found by minimizing ${\\mathcal L}(\\mathbf{w},\\mathbf{X},\\mathbf{y})$ with respect to $\\mathbf{w}$, \n",
    "$$\n",
    "    \\mathbf{w}^* = \\arg \\min_{\\mathbf{w}} {\\mathcal L}(\\mathbf{w},\\mathbf{X},\\mathbf{y}) = \\arg \\min_{\\mathbf{w}} \\left\\| \\mathbf{X}\\mathbf{w} - \\mathbf{y} \\right\\|_2^2  + \\lambda \\left\\| \\mathbf{w} \\right\\|_2^2.\n",
    "$$\n",
    "\n",
    "1. We find the value of $\\mathbf{w}$ where  $\\nabla_{\\mathbf{w}}{\\mathcal L} = \\mathbf{0}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb8d839",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
