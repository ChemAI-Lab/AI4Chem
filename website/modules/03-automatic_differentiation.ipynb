{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ca213b",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChemAI-Lab/AI4Chem/blob/main/website/modules/03-automatic_differentiation.ipynb)\n",
    "   \n",
    "\n",
    "**References:**\n",
    "1. **Automatic Differentiation in Machine Learning: a Survey** [Paper PDF](https://www.jmlr.org/papers/volume18/17-468/17-468.pdf)\n",
    "1. **Chapters 5**: [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), C. M. Bishop.\n",
    "<!-- 2. **Chapter 2**: [Machine Learning in Quantum Sciences](https://arxiv.org/pdf/2204.04198) UPDATE THIS! -->\n",
    "3. **Chapter 7**: [Probabilistic Machine Learning: An Introduction, K. P. Murphy.](https://probml.github.io/pml-book/book1.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8993640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b04354",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Numerical differentiation is a computational field which \"sole\" aim is to estimate the derivate of a mathematical function or subroutine using values of the function. \n",
    "For example, one could use an approximation of the **derivative** to compute the gradient of a function,\n",
    "$$\n",
    "\\frac{\\partial f(\\mathbf{x})}{\\partial x_i} \\approx \\frac{f(\\mathbf{x} - h\\mathbf{e}_i) - f(\\mathbf{x})}{h},\n",
    "$$\n",
    "where $\\mathbf{e}_i$ is a unit vector in the $i$-th element, and $h$ is a small positive step size.  \n",
    "From this equation one can observe two things, \n",
    "1. The estimation of the gradient is \"simple\", it only depends on the evaluation of the function itself. \n",
    "2. We require $2N$ total evaluations to ensemble the full jacobian, $\\nabla f(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0656df9a",
   "metadata": {},
   "source": [
    "## 1D Example\n",
    "$$\n",
    "f(x) = \\sin(x) + 0.1x^2\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\frac{\\partial f(x)}{\\partial x} = \\cos(x) + 0.2x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9987de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1D example: finite difference vs exact derivative\n",
    "# f(x) = sin(x) + 0.1 x^2\n",
    "# f'(x) = cos(x) + 0.2 x\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(x) + 0.1 * x**2\n",
    "\n",
    "\n",
    "def fp_exact(x):\n",
    "    return np.cos(x) + 0.2 * x\n",
    "\n",
    "\n",
    "def fp_forward_fd(x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "\n",
    "# Domain\n",
    "x = np.linspace(-6, 6, 1200)\n",
    "y_exact = fp_exact(x)\n",
    "\n",
    "# Step sizes to animate through (log-spaced, typical slide-friendly range)\n",
    "h_values = np.logspace(0, -8, 50)\n",
    "\n",
    "# Precompute errors for y-axis scaling stability (optional but makes animation nicer)\n",
    "all_err = []\n",
    "all_approx = []\n",
    "for h in h_values:\n",
    "    ya = fp_forward_fd(x, h)\n",
    "    all_approx.append(ya)\n",
    "    all_err.append(np.abs(ya - y_exact))\n",
    "\n",
    "all_approx = np.array(all_approx)\n",
    "all_err = np.array(all_err)\n",
    "\n",
    "# Figure: side-by-side panels\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    1, 2, figsize=(11, 4.2), constrained_layout=True)\n",
    "\n",
    "# Panel 1: derivatives\n",
    "ax1.set_title(\"Derivative: exact vs forward finite difference\")\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"f'(x)\")\n",
    "(line_exact,) = ax1.plot(x, y_exact, lw=2, label=\"Exact\", alpha=0.85)\n",
    "(line_fd,) = ax1.plot(x, all_approx[0], lw=2, label=\"Forward FD\", alpha=0.9)\n",
    "ax1.legend(loc=4)\n",
    "ax1.grid(True, alpha=0.25)\n",
    "\n",
    "# Set reasonable y-limits based on exact +/- margin\n",
    "ymin = min(y_exact.min(), all_approx.min())\n",
    "ymax = max(y_exact.max(), all_approx.max())\n",
    "pad = 0.05 * (ymax - ymin + 1e-12)\n",
    "ax1.set_ylim(ymin - pad, ymax + pad)\n",
    "\n",
    "# Panel 2: absolute error\n",
    "ax2.set_xlabel(\"x\")\n",
    "ax2.set_ylabel(\"Absolute error |FD - exact|\")\n",
    "(line_err,) = ax2.plot(x, all_err[0], lw=2)\n",
    "ax2.set_yscale(\"log\")  # log scale makes the tradeoff pop\n",
    "ax2.grid(True, alpha=0.25)\n",
    "\n",
    "# Stable y-limits for log plot (avoid zero)\n",
    "err_min = np.maximum(all_err.min(), 1e-16)\n",
    "err_max = np.maximum(all_err.max(), 1e-16)\n",
    "ax2.set_ylim(err_min, err_max)\n",
    "\n",
    "# Text annotation for current h\n",
    "h_text = ax1.text(\n",
    "    0.02, 0.95, \"\", transform=ax1.transAxes, va=\"top\",\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.85, edgecolor=\"0.8\")\n",
    ")\n",
    "\n",
    "\n",
    "def init():\n",
    "    line_fd.set_ydata(all_approx[0])\n",
    "    line_err.set_ydata(all_err[0])\n",
    "    h_text.set_text(f\"h = {h_values[0]:.1e}\")\n",
    "    return line_fd, line_err, h_text\n",
    "\n",
    "\n",
    "def update(frame):\n",
    "    ya = all_approx[frame]\n",
    "    ea = all_err[frame]\n",
    "\n",
    "    line_fd.set_ydata(ya)\n",
    "    line_err.set_ydata(np.maximum(ea, 1e-16))  # keep strictly positive for log\n",
    "    h_text.set_text(f\"h = {h_values[frame]:.1e}\")\n",
    "\n",
    "    return line_fd, line_err, h_text\n",
    "\n",
    "\n",
    "ani_fd = animation.FuncAnimation(\n",
    "    fig,\n",
    "    update,\n",
    "    frames=len(h_values),\n",
    "    init_func=init,\n",
    "    interval=120,   # adjust speed\n",
    "    blit=False\n",
    ")\n",
    "\n",
    "plt.close(fig)   \n",
    "HTML(ani_fd.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae12f0c",
   "metadata": {},
   "source": [
    "## 2D Example\n",
    "$$\n",
    "f(x_1,x_2) = \\sin(x_1)\\cos(x_2) + 0.1(x_1^2 + x_2^2)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\nabla f(x_1,x_2) = \\begin{pmatrix}\n",
    "\\frac{\\partial f(x_1,x_2) }{\\partial x_1} \\\\\n",
    "\\frac{\\partial f(x_1,x_2) }{\\partial x_2}\n",
    "\\end{pmatrix} =  \\begin{pmatrix}\n",
    " \\cos(x_1) \\cos(x_2) + 0.2x_1 \\\\\n",
    " -\\sin(x_1) \\sin(x_2) + 0.2x_2 \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ecb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2D example: finite difference gradient vs exact gradient\n",
    "# f(x,y) = sin(x)cos(y) + 0.1(x^2 + y^2)\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def f(x, y):\n",
    "    return np.sin(x) * np.cos(y) + 0.1 * (x**2 + y**2)\n",
    "\n",
    "\n",
    "def grad_exact(x, y):\n",
    "    gx = np.cos(x) * np.cos(y) + 0.2 * x\n",
    "    gy = -np.sin(x) * np.sin(y) + 0.2 * y\n",
    "    return gx, gy\n",
    "\n",
    "\n",
    "def grad_forward_fd(x, y, h):\n",
    "    gx = (f(x + h, y) - f(x, y)) / h\n",
    "    gy = (f(x, y + h) - f(x, y)) / h\n",
    "    return gx, gy\n",
    "\n",
    "\n",
    "# Grid (keep modest for animation performance)\n",
    "x = np.linspace(-4, 4, 41)\n",
    "y = np.linspace(-4, 4, 41)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Z = f(X, Y)\n",
    "Gx_true, Gy_true = grad_exact(X, Y)\n",
    "\n",
    "# Step sizes to animate (big -> tiny so you see truncation -> roundoff)\n",
    "h_values = np.logspace(0, -5, 50)\n",
    "\n",
    "# Precompute FD gradients + error magnitude for smooth animation\n",
    "Gx_fd_all = []\n",
    "Gy_fd_all = []\n",
    "Err_all = []\n",
    "for h in h_values:\n",
    "    Gx_fd, Gy_fd = grad_forward_fd(X, Y, h)\n",
    "    Gx_fd_all.append(Gx_fd)\n",
    "    Gy_fd_all.append(Gy_fd)\n",
    "    Err_all.append(np.sqrt((Gx_fd - Gx_true) ** 2 + (Gy_fd - Gy_true) ** 2))\n",
    "\n",
    "Gx_fd_all = np.array(Gx_fd_all)\n",
    "Gy_fd_all = np.array(Gy_fd_all)\n",
    "Err_all = np.array(Err_all)\n",
    "\n",
    "# Figure: 2x2 layout (nice for slides)\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 9), constrained_layout=True)\n",
    "(ax_contour, ax_true), (ax_fd, ax_err) = axs\n",
    "\n",
    "# --- Top-left: contours of f(x,y)\n",
    "ax_contour.set_title(\"f(x, y) contours\")\n",
    "ax_contour.set_xlabel(\"x\")\n",
    "ax_contour.set_ylabel(\"y\")\n",
    "cs = ax_contour.contour(X, Y, Z, levels=18)\n",
    "ax_contour.clabel(cs, inline=True, fontsize=8)\n",
    "\n",
    "# --- Top-right: true gradient field\n",
    "ax_true.set_title(\"True gradient ∇f(x, y)\")\n",
    "ax_true.set_xlabel(\"x\")\n",
    "ax_true.set_ylabel(\"y\")\n",
    "q_true = ax_true.quiver(X, Y, Gx_true, Gy_true)\n",
    "ax_true.set_aspect(\"equal\")\n",
    "\n",
    "# --- Bottom-left: FD gradient field (animated)\n",
    "ax_fd.set_title(\"Forward finite-difference gradient (animated)\")\n",
    "ax_fd.set_xlabel(\"x\")\n",
    "ax_fd.set_ylabel(\"y\")\n",
    "q_fd = ax_fd.quiver(X, Y, Gx_fd_all[0], Gy_fd_all[0])\n",
    "ax_fd.set_aspect(\"equal\")\n",
    "\n",
    "# --- Bottom-right: error magnitude heatmap (animated)\n",
    "ax_err.set_title(\"Error magnitude ‖∇f_FD − ∇f‖₂ (animated)\")\n",
    "ax_err.set_xlabel(\"x\")\n",
    "ax_err.set_ylabel(\"y\")\n",
    "\n",
    "# Use log scale on color via LogNorm-ish manual trick: plot log10(error)\n",
    "# (avoids importing matplotlib.colors; keeps code minimal)\n",
    "log_err0 = np.log10(np.maximum(Err_all[0], 1e-16))\n",
    "im = ax_err.imshow(\n",
    "    log_err0,\n",
    "    origin=\"lower\",\n",
    "    extent=[x.min(), x.max(), y.min(), y.max()],\n",
    "    aspect=\"equal\"\n",
    ")\n",
    "cbar = fig.colorbar(im, ax=ax_err, shrink=0.85)\n",
    "cbar.set_label(\"log10(error)\")\n",
    "\n",
    "# Annotation for h\n",
    "h_text = ax_fd.text(\n",
    "    0.02, 0.98, \"\", transform=ax_fd.transAxes, va=\"top\",\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.85, edgecolor=\"0.8\")\n",
    ")\n",
    "\n",
    "\n",
    "def init():\n",
    "    q_fd.set_UVC(Gx_fd_all[0], Gy_fd_all[0])\n",
    "    im.set_data(np.log10(np.maximum(Err_all[0], 1e-16)))\n",
    "    h_text.set_text(f\"h = {h_values[0]:.1e}\")\n",
    "    return q_fd, im, h_text\n",
    "\n",
    "\n",
    "def update(frame):\n",
    "    q_fd.set_UVC(Gx_fd_all[frame], Gy_fd_all[frame])\n",
    "    im.set_data(np.log10(np.maximum(Err_all[frame], 1e-16)))\n",
    "    h_text.set_text(f\"h = {h_values[frame]:.1e}\")\n",
    "    return q_fd, im, h_text\n",
    "\n",
    "\n",
    "ani_2d = animation.FuncAnimation(\n",
    "    fig, update, frames=len(h_values), init_func=init, interval=140, blit=False\n",
    ")\n",
    "\n",
    "plt.close(fig)\n",
    "HTML(ani_2d.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e42d0",
   "metadata": {},
   "source": [
    "Various techniques have been developed to mitigate approximation errors in numerical differentiation, such as using a **center difference approximation**, \n",
    "$$\n",
    "\\frac{\\partial f(\\mathbf{x})}{\\partial x_i} \\approx \\frac{f(\\mathbf{x} + h\\mathbf{e}_i) - f(\\mathbf{x} - h\\mathbf{e}_i)}{2h} + {\\cal O}(h^2),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6558f0",
   "metadata": {},
   "source": [
    "# Forward Mode differentiation\n",
    "AD in forward accumulation mode is how \"physicist\" see the chain rule. Let's consider the function $f(x_1,x_2) = \\sin(x_1)\\cos(x_2) + 0.1(x_1^2 + x_2^2)$, and compute its jacobian. First, we will associate each intermediate variable $v_i$ as $ \\dot{v}_i = \\frac{\\partial v_i}{\\partial x_j}$. Similar to finite differences, we will initialized with only one of the $\\dot{v}_i$ variables being non-zero; e.g., $\\dot{v}_i = \\frac{\\partial v_i}{\\partial x_1} = 1$ and $\\dot{v}_i = \\frac{\\partial v_i}{\\partial x_2} = 0$.\n",
    "\n",
    "| Forward Primal Trace      | Forward Tangent (Derivative) Trace|\n",
    "| ----------- | ----------- |\n",
    "| $v_{-1} = x_1$     | $\\dot{v}_{-1} = \\dot{x}_1$ = 1       |\n",
    "| $v_{0} = x_2$   | $\\dot{v}_{0} = \\dot{x}_2$ = 0           |\n",
    "| ----------- | ----------- |\n",
    "| $v_{1} = \\sin(x_1)$     | $\\dot{v}_{1} = \\cos(v_{-1})\\dot{v}_{-1}$       |\n",
    "| $v_{2} = \\cos(x_0)$     | $\\dot{v}_2 = -\\sin(v_0)\\dot{v}_0$       |\n",
    "| $v_{3} = v_{-1}^2$     | $\\dot{v}_3 = 2v_{-1}\\dot{v}_{-1}$      |\n",
    "| $v_{4} = v_{0}^2$     | $\\dot{v}_4 = 2v_0\\dot{v}_0$       |\n",
    "| $v_{5} = v_{1} \\times v_{2}$     | $\\dot{v}_5 = \\dot{v}_1 \\, v_{2} + v_{1}\\,\\dot{v}_2$       |\n",
    "| $v_{6} = v_{3} + v_{4}$     | $\\dot{v}_6 = \\dot{v}_3 + \\dot{v}_4$       |\n",
    "| $v_{7} = 0.1*v_{6}$     | $\\dot{v}_7 = 0.1 \\dot{v}_{6}$       |\n",
    "| $v_{8} = v_{7} + v_{7}$     | $\\dot{v}_2 = \\dot{v}_5 + \\dot{v}_7$       |\n",
    "| ----------- | ----------- |\n",
    "| $y = v_{8}$ |    $\\dot{y} = \\dot{v}_{8}$        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf71d33",
   "metadata": {},
   "source": [
    "Forward mode AD provides a very efficient and matrix-free way of computing Jacobian–vector products,\n",
    "$$\n",
    "\\mathbf{J}_{f}\\mathbf{r}  = \\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_n} \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "r_1\\\\\n",
    "\\vdots \\\\\n",
    "r_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where $\\dot{\\mathbf{x}} = \\mathbf{r}$. Thus, we can compute the Jacobian–vector product in\n",
    "just one forward pass.\n",
    "\n",
    "* Compared to finite differences, forward mode AD only requires $N$ evaluations of the function $f$ to compute the jacobian, one for each variable, and its the exact gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83a901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_mode_trace_np(x1, x2, seed=(1.0, 0.0), verbose=True):\n",
    "    \"\"\"\n",
    "    Forward-mode AD for:\n",
    "        f(x1, x2) = sin(x1)*cos(x2) + 0.1*(x1^2 + x2^2)\n",
    "\n",
    "    seed = (dx1, dx2):\n",
    "        (1,0) -> ∂f/∂x1\n",
    "        (0,1) -> ∂f/∂x2\n",
    "        (a,b) -> directional derivative a*∂f/∂x1 + b*∂f/∂x2\n",
    "    \"\"\"\n",
    "    dx1, dx2 = seed\n",
    "\n",
    "    # --- Inputs (your v_-1 and v_0) ---\n",
    "    v_m1, dv_m1 = np.array(x1, dtype=float), float(dx1)   # v_-1 = x1\n",
    "    v_0,  dv_0 = np.array(x2, dtype=float), float(dx2)   # v_0  = x2\n",
    "\n",
    "    # --- Primal + tangent trace ---\n",
    "    v1 = np.sin(v_m1)\n",
    "    dv1 = np.cos(v_m1) * dv_m1\n",
    "    v2 = np.cos(v_0)\n",
    "    dv2 = -np.sin(v_0) * dv_0\n",
    "\n",
    "    v3 = v_m1**2\n",
    "    dv3 = 2.0 * v_m1 * dv_m1\n",
    "    v4 = v_0**2\n",
    "    dv4 = 2.0 * v_0 * dv_0\n",
    "\n",
    "    v5 = v1 * v2\n",
    "    dv5 = dv1 * v2 + v1 * dv2\n",
    "    v6 = v3 + v4\n",
    "    dv6 = dv3 + dv4\n",
    "    v7 = 0.1 * v6\n",
    "    dv7 = 0.1 * dv6\n",
    "    y = v5 + v7\n",
    "    dy = dv5 + dv7\n",
    "\n",
    "    if verbose:\n",
    "        rows = [\n",
    "            (\"v_-1 = x1\", v_m1, \"dv_-1 = dx1\", dv_m1),\n",
    "            (\"v_0  = x2\", v_0,  \"dv_0  = dx2\", dv_0),\n",
    "            (\"v1 = sin(v_-1)\", v1, \"dv1 = cos(v_-1)*dv_-1\", dv1),\n",
    "            (\"v2 = cos(v_0)\",  v2, \"dv2 = -sin(v_0)*dv_0\",  dv2),\n",
    "            (\"v3 = v_-1^2\",    v3, \"dv3 = 2*v_-1*dv_-1\",    dv3),\n",
    "            (\"v4 = v_0^2\",     v4, \"dv4 = 2*v_0*dv_0\",      dv4),\n",
    "            (\"v5 = v1*v2\",     v5, \"dv5 = dv1*v2 + v1*dv2\", dv5),\n",
    "            (\"v6 = v3+v4\",     v6, \"dv6 = dv3 + dv4\",       dv6),\n",
    "            (\"v7 = 0.1*v6\",    v7, \"dv7 = 0.1*dv6\",         dv7),\n",
    "            (\"y = v5+v7\",      y,  \"dy = dv5 + dv7\",        dy),\n",
    "        ]\n",
    "        for a, av, b, bv in rows:\n",
    "            print(f\"{a:<18} = {av: .8f}   |   {b:<28} = {bv: .8f}\")\n",
    "\n",
    "    return float(y), float(dy)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "x1, x2 = 1.2, -0.7\n",
    "\n",
    "# ∂f/∂x1 at (x1,x2)\n",
    "y, dfdx1 = forward_mode_trace_np(x1, x2, seed=(1.0, 0.0), verbose=True)\n",
    "print(\"\\nForward-mode AD\")\n",
    "print(\"∂f/∂x1 =\", dfdx1)\n",
    "\n",
    "# ∂f/∂x2 at (x1,x2)\n",
    "_, dfdx2 = forward_mode_trace_np(x1, x2, seed=(0.0, 1.0), verbose=False)\n",
    "print(\"∂f/∂x2 =\", dfdx2)\n",
    "\n",
    "print(\"\\nExact gradient:\")\n",
    "dfdx1_true, dfdx2_true = grad_exact(x1, x2)\n",
    "print(\"∂f/∂x1 =\", dfdx1_true)\n",
    "print(\"∂f/∂x2 =\", dfdx2_true)\n",
    "\n",
    "# finite difference check\n",
    "h_fd = 1e-2\n",
    "dfdx1_fd, dfdx2_fd = grad_forward_fd(x1, x2, h_fd)\n",
    "print(\"\\nFinite difference gradient (h = 1e-6):\")\n",
    "print(\"∂f/∂x1 =\", dfdx1_fd)\n",
    "print(\"∂f/∂x2 =\", dfdx2_fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb5d18b",
   "metadata": {},
   "source": [
    "## Forward AD as Dual Numbers \n",
    "Commonly, one does not \"program\" the derivative either use the **dual numbers** representation, $v + \\dot{v}\\epsilon$. \n",
    "$\\epsilon$ is a **nilpotent number**, meaning $\\epsilon^2 = 0$ and $\\epsilon \\neq 0$. <br>\n",
    "$$\n",
    "f(v + \\dot{v}\\epsilon) = f(v) + f'(v)\\dot{v}\\epsilon,\n",
    "$$\n",
    "where $f'(v)$ is the derivative of $f$ with respect to $v$. This will allows us to overload the operations and expand simple functions to also aggregate the gradient.\n",
    "\n",
    "```Python\n",
    "import numpy as np\n",
    "\n",
    "class Dual:\n",
    "    def __init__(self, val, dot):\n",
    "        self.val = val      # primal\n",
    "        self.dot = dot      # tangent\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if not isinstance(other, Dual):\n",
    "            other = Dual(other, 0.0)\n",
    "        return Dual(self.val + other.val,\n",
    "                    self.dot + other.dot)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if not isinstance(other, Dual):\n",
    "            other = Dual(other, 0.0)\n",
    "        return Dual(self.val * other.val,\n",
    "                    self.dot * other.val + self.val * other.dot)\n",
    "\n",
    "    def __pow__(self, p):\n",
    "        return Dual(self.val ** p,\n",
    "                    p * self.val ** (p - 1) * self.dot)\n",
    "\n",
    "# overload simple functions\n",
    "def sin(x):\n",
    "    return Dual(np.sin(x.val),\n",
    "                np.cos(x.val) * x.dot)\n",
    "\n",
    "def cos(x):\n",
    "    return Dual(np.cos(x.val),\n",
    "                -np.sin(x.val) * x.dot)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b33f73",
   "metadata": {},
   "source": [
    "# Reverse Mode AD\n",
    "Reverse AD corresponds to a generalized backpropagation algorithm as it propagates the derivatives from the outputs to the inputs.\n",
    "This is done using intermediate variables $v_i$ named **adjoint**, \n",
    "$$\n",
    "\\bar{v}_i = \\frac{\\partial y_j}{\\partial v_i}\n",
    "$$\n",
    "Derivatives are computed in the second phase of a two-phase process.\n",
    "In the first phase, the original function code is run forward, populating intermediate variables vi and recording the dependencies in the computational graph through a book-keeping procedure. In the second phase, derivatives are calculated by propagating adjoints $\\bar{v}_i$ in reverse, from the outputs to the inputs.\n",
    "\n",
    "Reverse-mode AD is literally computing:\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial v_i} = \\sum_{j\\in\\text{children(i)}} \\frac{\\partial y}{\\partial v_j}\\frac{\\partial v_j}{\\partial v_i}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49fb4a2",
   "metadata": {},
   "source": [
    "| Forward Primal Trace      | Reverse Adjoint (Derivative) Trace |\n",
    "| ----------- | ----------- |\n",
    "| $v_{-1} = x_1$     | $\\bar{x}_1 = \\bar{v}_{-1}$  |\n",
    "| $v_{0} = x_2$   |  $\\bar{x}_0 = \\bar{v}_{0} $         |\n",
    "| ----------- | ----------- |\n",
    "| $v_{1} = \\sin(v_{-1})$     | $\\bar{v}_{-1} = \\bar{v}_{-1} + \\bar{v}_1\\frac{\\partial v_1}{\\partial v_{-1}}$         |\n",
    "| $v_{2} = \\cos(v_0)$     | $\\bar{v}_{0} = \\bar{v}_{0} + \\bar{v}_2\\frac{\\partial v_2}{\\partial v_{0}}$       |\n",
    "| $v_{3} = v_{-1}^2$     | $\\bar{v}_{-1} = \\bar{v}_3\\frac{\\partial v_3}{\\partial v_{-1}}$     |\n",
    "| $v_{4} = v_{0}^2$     | $\\bar{v}_0 = \\bar{v}_4\\frac{\\partial v_4}{\\partial v_0}$      |\n",
    "| $v_{5} = v_{1} \\times v_{2}$     |  $\\bar{v}_2 = \\bar{v}_5\\frac{\\partial v_5}{\\partial v_2}$   and $\\bar{v}_1 = \\bar{v}_5\\frac{\\partial v_5}{\\partial v_1}$     |\n",
    "| $v_{6} = v_{3} + v_{4}$     |  $\\bar{v}_4 = \\bar{v}_6\\frac{\\partial v_6}{\\partial v_4}$   and $\\bar{v}_3 = \\bar{v}_6\\frac{\\partial v_6}{\\partial v_3}$        |\n",
    "| $v_{7} = 0.1*v_{6}$     |  $\\bar{v}_6 = \\bar{v}_6\\frac{\\partial v_7}{\\partial v_6}$     |\n",
    "| $v_{8} = v_{5} + v_{7}$     | $\\bar{v}_7 = \\bar{v}_8\\frac{\\partial v_8}{\\partial v_7}$ and $\\bar{v}_5 = \\bar{v}_8\\frac{\\partial v_8}{\\partial v_5}$       |\n",
    "| ----------- | ----------- |\n",
    "| $y = v_{8}$ |    $\\dot{y} = \\dot{v}_{8}$        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8a59d7",
   "metadata": {},
   "source": [
    "Similar to the matrix-free computation of Jacobian–vector products with forward mode, reverse mode can be used for computing the transposed Jacobian–vector product\n",
    "$$\n",
    "\\mathbf{J}^\\top_{f}\\mathbf{r}  = \\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_1}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_1}{\\partial x_n} & \\cdots & \\frac{\\partial y_m}{\\partial x_n} \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "r_1\\\\\n",
    "\\vdots \\\\\n",
    "r_m\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "by initializing the reverse phase with $\\bar{\\mathbf{y}} = \\bar{\\mathbf{r}}$.\n",
    "\n",
    "* Reverse mode AD can build the full jacobian for a scalar function with a single forward pass, with the additional cost to store the adjoints in the way. (This is the reason why bigger GPUs are needed during training than inference).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
