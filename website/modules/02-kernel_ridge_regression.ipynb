{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f77dc67",
   "metadata": {},
   "source": [
    "# Kernel Ridge Regression\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChemAI-Lab/AI4Chem/blob/main/website/modules/02-kernel_ridge_regression.ipynb)\n",
    "\n",
    "**References:**\n",
    "1. **Chapters 6**: [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), C. M. Bishop.\n",
    "2. **Chapter 2**:  [Gaussian Processes for Machine LearningOpen Access](https://direct.mit.edu/books/oa-monograph-pdf/2514321/book_9780262256834.pdf), C. E. Rasmussen, C. K. I. Williams\n",
    "3. **Chapter 4**: [Machine Learning in Quantum Sciences](https://arxiv.org/pdf/2204.04198)\n",
    "4. [**The Kernel Cookbook**](https://www.cs.toronto.edu/~duvenaud/cookbook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15676fc5",
   "metadata": {},
   "source": [
    "# Potential Energy Surface "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d14c4",
   "metadata": {},
   "source": [
    "The following data is based on a Polynomial expansion to predict the electronic energy of methane for different geometries. <br>\n",
    "We will follow the work presented in the paper, [paper link](https://pubs.acs.org/doi/full/10.1021/acs.jctc.9b00043)<br>\n",
    "\"*Using Gradients in Permutationally Invariant Polynomial Potential Fitting: A Demonstration for CH4 Using as Few as 100 Configurations*\".\n",
    "\n",
    "The main idea is to expand the inter-atomic distance of the 5 atoms in terms on a polynomial expansion,\n",
    "$$\n",
    "y = \\sum^{np}_{i} w_i \\phi_i(\\mathbf{r})\n",
    "$$\n",
    "where $w_i$ are the weights, $\\phi_i$ are the Permutationally Invariant Polynomial expansion, and $\\mathbf{r}$ are the inter-atomic distances. \n",
    "\n",
    "In the dataset provided, the values of the vector \n",
    "$$\n",
    "\\mathbf{\\phi}(\\mathbf{r})^\\top = [\\phi_0(\\mathbf{r}),\\cdots,\\phi_{np}(\\mathbf{r})],\n",
    "$$ \n",
    "are in each column of a raw, with the respective energy in the last column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8776995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bb173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data using pandas\n",
    "data_file = 'https://raw.githubusercontent.com/ChemAI-Lab/Math4Chem/main/website/Assignments/CH4_data.csv'\n",
    "data = pd.read_csv(data_file)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b86e41f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35012feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. How many points does the dataset contains?\n",
    "n = data.shape[0]\n",
    "print('The dataset contains {} points.'.format(n))\n",
    "\n",
    "# 2. What is the point with the lowest energy and what is its value?\n",
    "lowest_energy = data['energy'].min()\n",
    "lowest_energy_point = data[data['energy'] == lowest_energy]\n",
    "print('The point with the lowest energy is {} with a value of {} eV.'.format(\n",
    "    lowest_energy_point.iloc[0, 0], lowest_energy))\n",
    "\n",
    "# 3. What is the range of energies in the dataset? The energy is reported in Hartrees\n",
    "energy_range = data['energy'].max() - data['energy'].min()\n",
    "print('The range of energies in the dataset is {} Ha.'.format(energy_range))\n",
    "\n",
    "# 4. Do a histogram of the energy.\n",
    "plt.hist(data['energy'], bins=20)\n",
    "plt.xlabel('Energy (Ha)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c06db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PIPs representation per molecule\n",
    "y_all = data['energy'].to_numpy()\n",
    "print('Total energy points:', y_all.shape)\n",
    "X_all = data.drop(['energy', 'Unnamed: 0'], axis=1).to_numpy()\n",
    "print(\"Total Geometries:\", X_all.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.3, random_state=42)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dfb9bd",
   "metadata": {},
   "source": [
    "# Kernel Ridge Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12115d22",
   "metadata": {},
   "source": [
    "Kernel ridge regression (KRR) combines ridge regression with the kernel trick, enabling linear regression to be performed in a (possibly high-dimensional) feature space without explicitly constructing the feature map. The model is trained by minimizing the squared error with an  L2 regularization term, controlled by the regularization parameter $\\lambda$; in scikit-learn this is named $\\alpha$.\n",
    "\n",
    "When a linear kernel is used, KRR is mathematically equivalent to standard ridge regression. Nevertheless, framing the problem in the kernel setting is useful, as it allows us to apply the same training and validation machinery used for more general kernels. In particular, the regularization strength $\\lambda$ can be selected via cross-validation.\n",
    "\n",
    "Below, we use cross-validation to choose the optimal value of $\\lambda$ for linear KRR and then evaluate the resulting model on a held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11934e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Linear kernel ridge regression with cross-validated regularization strength\n",
    "param_grid = {\"alpha\": np.logspace(-8, 2, 11)}\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "krr = KernelRidge(kernel=\"linear\")\n",
    "grid = GridSearchCV(krr, param_grid=param_grid, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha: {}\".format(grid.best_params_[\"alpha\"]))\n",
    "best_krr = grid.best_estimator_\n",
    "y_pred = best_krr.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Test RMSE: {:.6f}\".format(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_scatter, ax_hist) = plt.subplots(1, 2, figsize=(10, 4), gridspec_kw={'width_ratios': [2, 1]})\n",
    "\n",
    "ax_scatter.scatter(y_test, y_pred, alpha=0.5)\n",
    "ax_scatter.plot([y_all.min(), y_all.max()], [y_all.min(), y_all.max()], 'k--')\n",
    "ax_scatter.set_xlabel('True Energies (Ha)')\n",
    "ax_scatter.set_ylabel('Predicted Energies (Ha)')\n",
    "\n",
    "residuals = y_test - y_pred\n",
    "ax_hist.hist(residuals, bins=30, orientation='horizontal', color='tab:gray', alpha=0.7)\n",
    "ax_hist.axhline(0.0, color='k', linestyle='--', linewidth=1)\n",
    "ax_hist.set_xlabel('Count')\n",
    "ax_hist.set_ylabel('Residuals (Ha)')\n",
    "\n",
    "plt.title(\"Linear KRR:\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc047be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# RBF kernel ridge regression with cross-validated alpha and gamma\n",
    "param_grid = {\n",
    "    \"alpha\": np.logspace(-8, 2, 11),\n",
    "    \"gamma\": np.logspace(-2, 5, 13),\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "krr = KernelRidge(kernel=\"rbf\")\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    krr,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha:\", grid.best_params_[\"alpha\"])\n",
    "print(\"Best gamma:\", grid.best_params_[\"gamma\"])\n",
    "\n",
    "best_krr = grid.best_estimator_\n",
    "y_pred = best_krr.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Test RMSE: {:.6f}\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3998ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_scatter, ax_hist) = plt.subplots(\n",
    "    1, 2, figsize=(10, 4), gridspec_kw={'width_ratios': [2, 1]})\n",
    "\n",
    "ax_scatter.scatter(y_test, y_pred, alpha=0.5)\n",
    "ax_scatter.plot([y_all.min(), y_all.max()], [y_all.min(), y_all.max()], 'k--')\n",
    "ax_scatter.set_xlabel('True Energies (Ha)')\n",
    "ax_scatter.set_ylabel('Predicted Energies (Ha)')\n",
    "\n",
    "residuals = y_test - y_pred\n",
    "ax_hist.hist(residuals, bins=30, orientation='horizontal',\n",
    "             color='tab:gray', alpha=0.7)\n",
    "ax_hist.axhline(0.0, color='k', linestyle='--', linewidth=1)\n",
    "ax_hist.set_xlabel('Count')\n",
    "ax_hist.set_ylabel('Residuals (Ha)')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64884d6",
   "metadata": {},
   "source": [
    "## RBF Kernel: Effect of $\\gamma$ (1D Example)\n",
    "\n",
    "The RBF kernel parameter $\\gamma$ controls the smoothness of the fitted function. Small $\\gamma$ produces smoother functions, while large $\\gamma$ can overfit the training data.\n",
    "This is why $\\gamma$ should be selected using cross-validation rather than by optimizing training error alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff0831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Fixed 1D dataset for comparing different gamma values\n",
    "rng = np.random.default_rng(42)\n",
    "X_1d = rng.uniform(0.0, 1.0, 25)[:, None]\n",
    "y_1d = np.sin(2 * np.pi * X_1d[:, 0]) + 0.2 * rng.normal(size=25)\n",
    "\n",
    "X_train_1d, X_test_1d, y_train_1d, y_test_1d = train_test_split(\n",
    "    X_1d, y_1d, test_size=0.3, random_state=42\n",
    ")\n",
    "x_grid = np.linspace(0.0, 1.0, 200)[:, None]\n",
    "\n",
    "def plot_gamma(gamma=1.0):\n",
    "    model = KernelRidge(kernel=\"rbf\", alpha=1e-2, gamma=gamma)\n",
    "    model.fit(X_train_1d, y_train_1d)\n",
    "\n",
    "    y_grid = model.predict(x_grid)\n",
    "    y_train_pred = model.predict(X_train_1d)\n",
    "    y_test_pred = model.predict(X_test_1d)\n",
    "\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train_1d, y_train_pred))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test_1d, y_test_pred))\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.scatter(X_train_1d[:, 0], y_train_1d, c='tab:blue', label='train')\n",
    "    plt.scatter(X_test_1d[:, 0], y_test_1d, c='tab:orange', label='test')\n",
    "    plt.plot(x_grid[:, 0], y_grid, c='k', lw=2, label='KRR fit')\n",
    "    plt.title(f'RBF KRR with gamma = {gamma:.3g} | RMSE train={rmse_train:.3f}, test={rmse_test:.3f}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "gamma_slider = widgets.FloatLogSlider(base=10, min=-2, max=3, step=0.1, value=1.0)\n",
    "out = widgets.interactive_output(plot_gamma, {\"gamma\": gamma_slider})\n",
    "display(gamma_slider, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ede47b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
