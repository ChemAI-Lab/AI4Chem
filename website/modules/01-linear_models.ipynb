{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba87ace9",
   "metadata": {},
   "source": [
    "# Liner models\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChemAI-Lab/AI4Chem/blob/main/website/modules/01-linear_models.ipynb)\n",
    "\n",
    "**References:**\n",
    "1. **Chapters 1 and 3**: Pattern Recognition and Machine Learning, C. M. Bishop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c12eeff",
   "metadata": {},
   "source": [
    "## Linear Models\n",
    "The simplest models for regression, are the ones that involved a linear combination of input variables,\n",
    "$$\n",
    "f(\\mathbf{x},\\mathbf{w}) = \\underbrace{\\mathbf{w}^\\top \\mathbf{x}}_{\\text{dot product}}= \\sum_{i}^d w_i \\, x_i,\n",
    "$$\n",
    "where $\\mathbf{w}$ are the linear weights, also known as **parameters**, and $\\mathbf{x}$ is the input. $d$ is known as the number of features that represent $\\mathbf{x}$. <br>\n",
    "\n",
    "The family of \"linear models\" is not unique, it can be extended to **fixed** non-linear functions, \n",
    "$$\n",
    "f(\\mathbf{\\phi}(\\mathbf{x}),\\mathbf{w}) = \\mathbf{w}^\\top \\mathbf{\\phi}(\\mathbf{x})= \\sum_{i}^d w_i \\, \\phi_i(\\mathbf{x}),\n",
    "$$\n",
    "where $\\phi_i(\\mathbf{x})$ is a \"basis function\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c80df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as f_mse\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ebd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random data over f(x) = sin(x) + x - 1\n",
    "def get_data(N, bool_biased=True):\n",
    "    # This creates an array x of N linearly spaced values between -1 and 1.\n",
    "    # bool_biased: if True, adds a bias term (column of ones) to the input data X\n",
    "\n",
    "    x = np.linspace(-1., 1., N) + np.random.uniform(low=-.15, high=.15, size=N)\n",
    "    y = 1.2*np.sin(2*x) + x - 1.\n",
    "    # Adds random noise to each y value.\n",
    "    y = y + np.random.uniform(low=-.35, high=.35, size=x.shape)\n",
    "    if bool_biased:\n",
    "        X = np.column_stack((np.ones_like(x), x))\n",
    "    else:\n",
    "        X = x[:, None]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f980edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(15, bool_biased=True)\n",
    "\n",
    "plt.plot(X[:, 1], y, 'o', label='Data')\n",
    "plt.xlabel('x', fontsize=16)\n",
    "plt.ylabel('f(x)', fontsize=16)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5bb30e",
   "metadata": {},
   "source": [
    "## Least Square Problem\n",
    "**Notes from CHEM-3PC3**: <br> \n",
    "* Least Square:  [![Download PDF](https://img.shields.io/badge/Download_PDF-Click_Here-blue.svg)](https://github.com/ChemAI-Lab/Math4Chem/raw/main/website/Lecture_Notes/Notes/Linear_Regression.pdf)  <br>\n",
    "\n",
    "**Mean Square Error**\n",
    "$$\n",
    "{\\cal L}(\\mathbf{w}) = \\frac{1}{2n}\\sum_{i}^{n} (\\hat{y}_i - \\mathbf{w}^\\top\\phi(\\mathbf{x}_i)) = \\frac{1}{2n} \\left (\\mathbf{y} - \\Phi(\\mathbf{X})\\mathbf{w} \\right)^\\top \\left (\\mathbf{y} - \\Phi(\\mathbf{X})\\mathbf{w} \\right)\n",
    "$$\n",
    "\n",
    "**Gradient of a function equal to zero**  \n",
    "$$\n",
    "    \\nabla {\\cal L}(\\mathbf{w}) \\Big\\rvert_{\\mathbf{w}^{*}} = \\frac{1}{2n} \\nabla_{\\mathbf{w}} \\left [ \\left (\\mathbf{y} - \\Phi(\\mathbf{X})\\mathbf{w} \\right)^\\top \\left (\\mathbf{y} - \\Phi(\\mathbf{X})\\mathbf{w} \\right) \\right ]= 0\n",
    "$$\n",
    "\n",
    "To solve for $\\mathbf{w}^*$, let's expand $ \\left (\\mathbf{y} - \\Phi(\\mathbf{X})\\mathbf{w} \\right)^\\top \\left (\\mathbf{y} - \\Phi(\\mathbf{X})\\mathbf{w} \\right)$,\n",
    "\n",
    "$$\n",
    "    \\left (\\mathbf{y} - \\Phi(\\mathbf{X})\\mathbf{w} \\right)^\\top \\left (\\mathbf{y} - \\Phi(\\mathbf{X})\\mathbf{w} \\right) = \\mathbf{y}^\\top \\mathbf{y}  - \\mathbf{y}^\\top \\Phi(\\mathbf{X})\\mathbf{w} -  \\mathbf{w}^\\top\\Phi(\\mathbf{X})^\\top\\mathbf{y} +   \\mathbf{w}^\\top\\Phi(\\mathbf{X})^\\top \\Phi(\\mathbf{X})\\mathbf{w}\n",
    "$$\n",
    "$$\n",
    "    \\nabla_{\\mathbf{w}} {\\cal L}(\\mathbf{w}) = \\frac{1}{2n}\\left(  -2 \\Phi(\\mathbf{X})^\\top\\mathbf{y} + 2\\Phi(\\mathbf{X})^\\top\\Phi(\\mathbf{X})\\mathbf{w} \\right) = 0\n",
    "$$\n",
    "\n",
    "**Optimal parameters**\n",
    "\n",
    "$$\n",
    " \\mathbf{w}^* = \\left ( \\Phi(\\mathbf{X})^\\top \\Phi(\\mathbf{X}) \\right ) ^{-1} \\Phi(\\mathbf{X})^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "$\\,$<br>\n",
    "\n",
    " \n",
    "**Extra:**\n",
    "1. Proof the above equations.\n",
    "2. [Equations from Sections 2.4.1 and 2.4.2](https://www2.imm.dtu.dk/pubdb/edoc/imm3274.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa408e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trining a linear model\n",
    "def linear_model_solver(X, y):\n",
    "    Xt = X.T  # transpose of X\n",
    "    A = Xt @ X  # A = X^T X\n",
    "    z = Xt @ y  # z = X^T y\n",
    "    A_inv = np.linalg.inv(A)  # inverse of A\n",
    "    w_opt = A_inv @ z  # w = A^-1 z\n",
    "    return w_opt  # optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the optimal parameters\n",
    "w_opt = linear_model_solver(X, y)\n",
    "print(\"Optimal parameters\")\n",
    "for i,wi in enumerate(w_opt):\n",
    "    print(f\"w_{i} = {wi:.3f}, \", end='')\n",
    "print( )\n",
    "\n",
    "y_data_pred = X@w_opt\n",
    "r2_value = r2_score(y,y_data_pred)\n",
    "print(f'R-squared score: {r2_value:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f8796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot our model\n",
    "x_grid = np.linspace(-1.1, 1.1, 100)\n",
    "X_grid = np.column_stack((np.ones_like(x_grid), x_grid))  # add bias term\n",
    "print(\"Grid with bias: \", X_grid.shape)\n",
    "y_pred = X_grid@w_opt\n",
    "\n",
    "# plot the data\n",
    "fig, ax = plt.subplots()\n",
    "ax.text(0.05, 0.8, r'$R^2 = $' +\n",
    "        f'{r2_value:0.3f}', transform=ax.transAxes, fontsize=14)\n",
    "ax.text(0.05, 0.7, f\"m = {w_opt[1]:0.3f}, b = {w_opt[0]:0.3f}\", transform=ax.transAxes, fontsize=14)\n",
    "ax.scatter(X[:, 1], y, label='data', s=75)\n",
    "ax.plot(X_grid[:, 1], y_pred, c='k', label='Linear model', markersize=5)\n",
    "ax.set_xlabel('x', fontsize=18)\n",
    "ax.set_ylabel('f(x)', fontsize=18)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505602c1",
   "metadata": {},
   "source": [
    "## Beyond Linear Models\n",
    "Instead of considering for each data point, $\\mathbf{x}^\\top = [1, x]$, we can assume any other representation using a \"basis set\" $\\phi$, for example, a polynomial one, \n",
    "This is simply a new representation of $x$\n",
    "$$\n",
    "\\phi(\\mathbf{x})^\\top = [1, x, x^2, x^3, \\cdots,x^p].\n",
    "$$\n",
    "\n",
    "Instead of coding how to create the polynomial representation, we will use Sklearn. <br>\n",
    "Read the documentation: [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3915297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(X, p):\n",
    "    # transform the input data to polynomial features up to degree p\n",
    "    poly = PolynomialFeatures(p)\n",
    "    Phi = poly.fit_transform(X)\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a4b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the training data to polynomial features\n",
    "p = 3  # degree\n",
    "\n",
    "X = X[:, -1:]  # select the last column only (without bias term)\n",
    "\n",
    "# transform the input data to polynomial features\n",
    "X_phi = polynomial_features(X, p)\n",
    "print(\"phi(x): \", X_phi[0])\n",
    "\n",
    "# find the optimal parameters for the polynomial model\n",
    "w_phi_opt = linear_model_solver(X_phi, y)\n",
    "print(\"Optimal parameters\")\n",
    "for i, wi in enumerate(w_phi_opt):\n",
    "    print(f\"w_{i} = {wi:.3f}, \", end='')\n",
    "print()\n",
    "\n",
    "y_data_phi_pred = X_phi@w_phi_opt\n",
    "r2_score_poly = r2_score(y, y_data_phi_pred)\n",
    "print(f'R2 polynomial model: {r2_score_poly:.3f}')\n",
    "\n",
    "X_grid = np.linspace(-1.25, 1.25, 100)[:, None]\n",
    "X_grid_phi = polynomial_features(X_grid, p)\n",
    "y_pred_phi = X_grid_phi@w_phi_opt\n",
    "\n",
    "# plot the data and the polynomial model\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "ax.scatter(X[:, 0], y, label='data', s=75)\n",
    "ax.plot(X_grid[:, 0], y_pred, c='green', ls='--',\n",
    "        label='linear model, ' + r'$R^2 = $' +\n",
    "        f'{r2_value:0.3f}', markersize=5)\n",
    "ax.plot(X_grid[:, 0], y_pred_phi, c='k',\n",
    "        label=f'polynomial model (p={p}), '+ r'$R^2 = $' + f'{r2_score_poly:0.3f}', markersize=5)\n",
    "ax.set_xlabel('x', fontsize=18)\n",
    "ax.set_ylabel('f(x)', fontsize=18)\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c72e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 as a function of the polynomial degree\n",
    "\n",
    "plt.figure(1)\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "ax.scatter(X[:, 0], y, label='data', s=105,zorder=4)\n",
    "\n",
    "r2_ = []    \n",
    "for p in range(1,15,2):# loop over polynomial degrees p = [2, 4, .., 10]\n",
    "    X_phi = polynomial_features(X, p)\n",
    "    w_phi_opt = linear_model_solver(X_phi, y) # we can reuse our function\n",
    "    # print(f'Optimal parameters\\n{w_phi_opt}')\n",
    "\n",
    "    y_data_phi_pred = X_phi@w_phi_opt\n",
    "    r2_score_poly = r2_score(y, y_data_phi_pred)\n",
    "    r2_.append(r2_score_poly)\n",
    "    print(f'p: {p}, R2= {r2_score_poly:.4f}')\n",
    "\n",
    "    X_grid_phi = polynomial_features(X_grid, p)\n",
    "    y_pred_phi = X_grid_phi@w_phi_opt\n",
    "# plot the data and the polynomial model\n",
    "\n",
    "    ax.plot(X_grid[:, 0], y_pred_phi,\n",
    "            label=f'p={p} ' +  r' $R^2 = $' +\n",
    "            f'{r2_score_poly:0.4f}',  markersize=5, alpha=0.95*p/15)\n",
    "ax.set_xlabel('x', fontsize=18)\n",
    "ax.set_ylabel('f(x)', fontsize=18)\n",
    "ax.set_ylim(-5,3)\n",
    "plt.legend(fontsize=10,loc=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e61e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "plt.plot(range(1, 15, 2), r2_, '-o')\n",
    "plt.hlines(y=1.0, xmin=1, xmax=15, colors='k', linestyles='dashed')\n",
    "plt.xticks(range(1, 15, 2))\n",
    "plt.xlabel('Polynomial Degree', fontsize=16)\n",
    "plt.ylabel(r'$R^2$', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a409c768",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "From the exercise above, we can see that increasing the polynomial degree often improves the fit.\n",
    "However, if you look closely, the models start to look \"funky\" and oscillate between data points,\n",
    "especially when $p > 8$. This \"fake high accuracy\" is known as **overfilling**.\n",
    "<br>\n",
    "\n",
    "The degree of a polynomial is a **hyperparameter**: a choice we control that strongly affects\n",
    "model accuracy. To pick an \"optimal\" value, we cannot evaluate on the same data used for training;\n",
    "we need additional, unseen data to assess true performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ff44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation sets\n",
    "X, y = get_data(35, bool_biased=False)\n",
    "X_tr,X_val, y_tr,y_val = train_test_split(X,y,test_size=0.25, random_state=0)\n",
    "print(\"Training data:\", X_tr.shape, y_tr.shape)\n",
    "print(\"Validation data:\", X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b7ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the train/validation split\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.scatter(X_tr, y_tr, c='tab:blue', label='train', s=75, alpha=0.9)\n",
    "ax.scatter(X_val, y_val, c='tab:orange', label='validation', s=75, alpha=0.9)\n",
    "ax.set_xlabel('x', fontsize=16)\n",
    "ax.set_ylabel('f(x)', fontsize=16)\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d43c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ = np.arange(1,15,2) # polynomial degrees\n",
    "r2_train = [] # list to store R2 scores for training set\n",
    "r2_val = [] # list to store R2 scores for validation set\n",
    "for p in p_:\n",
    "    # transform the training data to polynomial features\n",
    "    X_tr_phi = polynomial_features(X_tr, p)\n",
    "    w_phi_opt = linear_model_solver(X_tr_phi, y_tr)\n",
    "\n",
    "    # evaluate on training data\n",
    "    y_tr_phi_pred = X_tr_phi@w_phi_opt\n",
    "    r2_score_tr = r2_score(y_tr, y_tr_phi_pred)\n",
    "    r2_train.append(r2_score_tr)\n",
    "    mse_tr = f_mse(y_tr, y_tr_phi_pred)\n",
    "    rmse_tr = np.sqrt(mse_tr)\n",
    "\n",
    "    # transform the validation data to polynomial features\n",
    "    X_val_phi = polynomial_features(X_val, p)\n",
    "    y_val_phi_pred = X_val_phi@w_phi_opt\n",
    "    r2_score_val = r2_score(y_val, y_val_phi_pred)\n",
    "    r2_val.append(r2_score_val)\n",
    "    mse_val = f_mse(y_val, y_val_phi_pred)\n",
    "    rmse_val = np.sqrt(mse_val)\n",
    "\n",
    "    print(f'p: {p}, R2 train= {r2_score_tr:.4f}, R2 val= {r2_score_val:.4f}', end=', ')\n",
    "    print(f' RMSE train= {rmse_tr:.4f}, RMSE val= {rmse_val:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda6c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train = np.array(r2_train)\n",
    "r2_val = np.array(r2_val)\n",
    "print(r2_train)\n",
    "plt.figure(3)\n",
    "plt.plot(p_, r2_train, '-o', label='Train ' + r'$R^2$')\n",
    "plt.plot(p_, r2_val, '-o', label='Validation ' + r'$R^2$')\n",
    "plt.hlines(y=1.0, xmin=1, xmax=15, colors='k', linestyles='dashed')\n",
    "plt.xticks(p_)\n",
    "plt.xlabel('Polynomial Degree', fontsize=16)\n",
    "plt.ylabel(r'$R^2$', fontsize=16)\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b9855",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "Overfitting happens when a model learns noise in the training data instead of the underlying pattern.\n",
    "It can look very accurate on the training set but generalizes poorly to new, unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47489a73",
   "metadata": {},
   "source": [
    "# Ridge Regression to avoid Overfitting\n",
    "\n",
    "When we solve the least square problem, the linear weights can have high values, this is the reason behind the highly osculating functions that fit the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid of polynomial weights across degrees\n",
    "p_grid = np.arange(1, 15, 2)\n",
    "w_grid = np.full((len(p_grid), p_grid.max() + 1), np.nan)\n",
    "\n",
    "for i, p in enumerate(p_grid):\n",
    "    X_tr_phi = polynomial_features(X_tr, p)\n",
    "    w_phi_opt = linear_model_solver(X_tr_phi, y_tr)\n",
    "    w_grid[i, :p + 1] = w_phi_opt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "im = ax.imshow(w_grid, aspect='auto', cmap='coolwarm')\n",
    "ax.hlines(np.arange(-0.5, len(p_grid), 1), -0.5, p_grid.max() + 0.5, colors='white', linewidth=0.5)\n",
    "ax.set_xlabel('Weight index', fontsize=14)\n",
    "ax.set_ylabel('Polynomial degree', fontsize=14)\n",
    "ax.set_xticks(np.arange(p_grid.max() + 1))\n",
    "ax.set_yticks(np.arange(len(p_grid)))\n",
    "ax.set_yticklabels(p_grid)\n",
    "fig.colorbar(im, ax=ax, label='Weight value')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda7d47e",
   "metadata": {},
   "source": [
    "add Ridge regression text here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed9e12",
   "metadata": {},
   "source": [
    "# Ridge Regression to avoid Overfitting\n",
    "\n",
    "Ridge regression adds an $L_2$ penalty to the least-squares objective.\n",
    "\n",
    "$$\n",
    "\n",
    "\\hat{w} = \\arg\\min_w \\|\\mathbf{y} - \\Phi(\\mathbf{X})\\mathbf{w} \\|_2^2 + \\lambda \\|\\mathbf{w}\\|_2^2 = \\arg\\min_w \\frac{1}{2n}\\sum_{i}^{n} (\\hat{y}_i - \\mathbf{w}^\\top\\phi(\\mathbf{x}_i)) + \\lambda \\sum_j^d w_j^2\n",
    "\n",
    "$$\n",
    "\n",
    "This yields the closed-form solution\n",
    "\n",
    "$$\n",
    "\\hat{w} = (\\Phi(\\mathbf{X})^T \\Phi(\\mathbf{X}) + \\lambda I)^{-1} \\Phi(\\mathbf{X})^T \\mathbf{y}.\n",
    "$$\n",
    "\n",
    "You can derive this equation using the same procedure as in least square without the ridge term $ \\lambda \\|\\mathbf{w}\\|_2^2$.<br>\n",
    "\n",
    "\n",
    "The $\\lambda$ parameter is also a **hyperparameter**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_solver(X, y, lambda_):\n",
    "    # Ridge regression solver\n",
    "    Xt = X.T\n",
    "    n_features = X.shape[1]\n",
    "    A = Xt @ X + lambda_ * np.eye(n_features)  # A = X^T X + λI\n",
    "    z = Xt @ y  # z = X^T y\n",
    "    A_inv = np.linalg.inv(A)  # inverse of A\n",
    "    w_ridge_opt = A_inv @ z  # w = A^-1 z\n",
    "    return w_ridge_opt  # optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde29bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan \\lambda for Ridge regression and p = 8\n",
    "lambda_values = np.logspace(-6, 2, 11)\n",
    "p = 8  # polynomial degree\n",
    "r2_ridge_train = []\n",
    "r2_ridge_val = []\n",
    "for lambda_ in lambda_values:\n",
    "    # transform the training data to polynomial features\n",
    "    X_tr_phi = polynomial_features(X_tr, p)\n",
    "    w_ridge_opt = ridge_regression_solver(X_tr_phi, y_tr, lambda_)\n",
    "\n",
    "    # evaluate on training data\n",
    "    y_tr_phi_pred = X_tr_phi@w_ridge_opt\n",
    "    r2_score_tr = r2_score(y_tr, y_tr_phi_pred)\n",
    "    r2_ridge_train.append(r2_score_tr)\n",
    "\n",
    "    # transform the validation data to polynomial features\n",
    "    X_val_phi = polynomial_features(X_val, p)\n",
    "    y_val_phi_pred = X_val_phi@w_ridge_opt\n",
    "    r2_score_val = r2_score(y_val, y_val_phi_pred)\n",
    "    r2_ridge_val.append(r2_score_val)\n",
    "\n",
    "    print(f'λ: {lambda_:.1e}, R2 train= {r2_score_tr:.4f}, R2 val= {r2_score_val:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf69049",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lambda_values, r2_ridge_train, '-o', label='Train ' + r'$R^2$')\n",
    "plt.plot(lambda_values, r2_ridge_val, '-o', label='Validation ' + r'$R^2$')\n",
    "plt.ylim(0.8,1.0)\n",
    "plt.xscale('log')\n",
    "plt.xlabel(r'Ridge parameter $\\lambda$', fontsize=16)\n",
    "plt.ylabel(r'$R^2$', fontsize=16)\n",
    "plt.legend(loc=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ba6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid of polynomial weights across ridge lambdas (p=8)\n",
    "p = 8\n",
    "X_tr_phi = polynomial_features(X_tr, p)\n",
    "w_lambda_grid = np.zeros((len(lambda_values), p + 1))\n",
    "\n",
    "for i, lambda_ in enumerate(lambda_values):\n",
    "    w_ridge_opt = ridge_regression_solver(X_tr_phi, y_tr, lambda_)\n",
    "    w_lambda_grid[i, :] = w_ridge_opt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "im = ax.imshow(w_lambda_grid, aspect='auto', cmap='coolwarm')\n",
    "ax.hlines(np.arange(-0.5, len(lambda_values), 1), -0.5, p + 0.5, colors='white', linewidth=0.5)\n",
    "ax.set_xlabel('Weight index', fontsize=14)\n",
    "ax.set_ylabel(r'$\\lambda$', fontsize=14)\n",
    "ax.set_xticks(np.arange(p + 1), [r'$w_{%s}$'%(i) for i in range(p + 1)])\n",
    "ax.set_yticks(np.arange(len(lambda_values)))\n",
    "ax.set_yticklabels([f'{v:.1e}' for v in lambda_values])\n",
    "fig.colorbar(im, ax=ax, label='Weight value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fd6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive ridge fit for a single lambda (p=8)\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "p = 8\n",
    "X_tr_phi = polynomial_features(X_tr, p)\n",
    "x_grid = np.linspace(X_tr.min(), X_tr.max(), 200)[:, None]\n",
    "X_grid_phi = polynomial_features(x_grid, p)\n",
    "\n",
    "\n",
    "def plot_ridge(lambda_):\n",
    "    w_ridge_opt = ridge_regression_solver(X_tr_phi, y_tr, lambda_)\n",
    "    y_pred = X_grid_phi @ w_ridge_opt\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.scatter(X_tr, y_tr, s=40, alpha=0.5, label='train data')\n",
    "    ax.scatter(X_val, y_val, s=40, alpha=0.5, label='validation data', c='orange')\n",
    "    ax.plot(x_grid[:, 0], y_pred, c='k', label=f'λ={lambda_:.3e}')\n",
    "    ax.set_xlabel('x', fontsize=16)\n",
    "    ax.set_ylabel('f(x)', fontsize=16)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "slider = widgets.FloatLogSlider(value=1e-2, base=10, min=-6, max=2, step=0.2, description='lambda')\n",
    "widgets.interact(plot_ridge, lambda_=slider)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8257f",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "A single random split can give a misleading performance estimate because results depend on which points land in the train/validation sets.\n",
    "Cross-validation repeats the split across multiple folds and averages the scores, giving a more reliable estimate for model selection.\n",
    "\n",
    "\n",
    "![Cross-validation schematic](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
    "\n",
    "Reference:\n",
    "[Cross-Validation Scikit Learn](https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86195136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation with scikit-learn to select degree and lambda\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# X and y come from the earlier train/val split setup\n",
    "X_all = X.reshape(-1, 1)\n",
    "y_all = y\n",
    "\n",
    "model = make_pipeline(\n",
    "    PolynomialFeatures(include_bias=True),\n",
    "    Ridge()\n",
    ")\n",
    "\n",
    "# Grid of polynomial degrees and lambdas (alpha in scikit-learn) to search\n",
    "param_grid = {\n",
    "    \"polynomialfeatures__degree\": [1, 2, 3, 4, 6, 8, 10, 12],\n",
    "    \"ridge__alpha\": np.logspace(-3, 2, 25),\n",
    "}\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "search = GridSearchCV(\n",
    "    model,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_squared_error\"\n",
    ")\n",
    "search.fit(X_all, y_all)\n",
    "\n",
    "best_degree = search.best_params_[\"polynomialfeatures__degree\"]\n",
    "best_lambda = search.best_params_[\"ridge__alpha\"]\n",
    "best_mse = -search.best_score_\n",
    "\n",
    "print(f\"Best degree: {best_degree}\")\n",
    "print(f\"Best lambda: {best_lambda:.3e}\")\n",
    "print(f\"CV MSE at best: {best_mse:.4f}\")\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "ridge = best_model.named_steps[\"ridge\"]\n",
    "w = ridge.coef_\n",
    "b = ridge.intercept_\n",
    "\n",
    "print(\"coef:\", w[1:])\n",
    "print(\"intercept:\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebebaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions from the best cross-validated model\n",
    "best_model = search.best_estimator_\n",
    "x_grid = np.linspace(X_all.min(), X_all.max(), 200)[:, None]\n",
    "y_pred = best_model.predict(x_grid)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.scatter(X_all, y_all, s=40, alpha=0.5, label=\"data\")\n",
    "ax.plot(x_grid[:, 0], y_pred, c=\"k\", label=\"best CV model\")\n",
    "ax.set_xlabel(\"x\", fontsize=16)\n",
    "ax.set_ylabel(\"f(x)\", fontsize=16)\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d713ae",
   "metadata": {},
   "source": [
    "### Model Diagnostics\n",
    "Quick checks to see if residuals look random and centered around zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220599ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual diagnostics for the best CV model\n",
    "y_fit = best_model.predict(X_all)\n",
    "residuals = y_all - y_fit\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].scatter(y_fit, residuals, s=30, alpha=0.6)\n",
    "axes[0].axhline(0, color=\"k\", linewidth=1)\n",
    "axes[0].set_xlabel(\"fitted values\", fontsize=12)\n",
    "axes[0].set_ylabel(\"residuals\", fontsize=12)\n",
    "axes[0].set_title(\"Residuals vs fitted\", fontsize=12)\n",
    "\n",
    "axes[1].hist(residuals, bins=20, alpha=0.7, edgecolor=\"k\")\n",
    "axes[1].set_xlabel(\"residual\", fontsize=12)\n",
    "axes[1].set_title(\"Residual histogram\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e25f25",
   "metadata": {},
   "source": [
    "## Bias–Variance Tradeoff\n",
    "\n",
    "As model complexity increases, training error typically decreases, but validation error can rise after a point due to overfitting.\n",
    "Plotting train vs. cross-validated error across polynomial degrees makes this tradeoff visible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d800578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train vs CV error across polynomial degree (selecting best lambda per degree)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "degrees = [1, 2, 3, 4, 5, 6, 8, 10, 12]\n",
    "lambdas = np.logspace(-6, 2, 25)\n",
    "\n",
    "train_mse = []\n",
    "cv_mse = []\n",
    "best_lambdas = []\n",
    "\n",
    "for d in degrees:\n",
    "    # inner loop: pick lambda with lowest CV MSE for this degree\n",
    "    lambda_scores = []\n",
    "    for a in lambdas:\n",
    "        model = make_pipeline(\n",
    "            PolynomialFeatures(degree=d, include_bias=True),\n",
    "            Ridge(alpha=a)\n",
    "        )\n",
    "        scores = cross_val_score(\n",
    "            model, X_all, y_all, cv=cv, scoring=\"neg_mean_squared_error\"\n",
    "        )\n",
    "        lambda_scores.append(-scores.mean())\n",
    "\n",
    "    best_idx = int(np.argmin(lambda_scores))\n",
    "    best_a = lambdas[best_idx]\n",
    "    best_lambdas.append(best_a)\n",
    "    cv_mse.append(lambda_scores[best_idx])\n",
    "\n",
    "    # fit on full data with best lambda to get training MSE\n",
    "    best_model = make_pipeline(\n",
    "        PolynomialFeatures(degree=d, include_bias=True),\n",
    "        Ridge(alpha=best_a)\n",
    "    )\n",
    "    best_model.fit(X_all, y_all)\n",
    "    y_fit = best_model.predict(X_all)\n",
    "    train_mse.append(mean_squared_error(y_all, y_fit))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(degrees, train_mse, marker=\"o\", label=\"train MSE\")\n",
    "ax.plot(degrees, cv_mse, marker=\"o\", label=\"CV MSE (best lambda)\")\n",
    "ax.set_xlabel(\"polynomial degree\", fontsize=12)\n",
    "ax.set_ylabel(\"MSE\", fontsize=12)\n",
    "ax.set_title(\"Bias–variance tradeoff\", fontsize=12)\n",
    "ax.set_xticks(degrees)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Best lambda per degree:\")\n",
    "for d, a in zip(degrees, best_lambdas):\n",
    "    print(f\"degree {d}: lambda={a:.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c2c897",
   "metadata": {},
   "source": [
    "### Extra: Gaussian (RBF) Feature Expansion\n",
    "We can also expand features using a Gaussian (radial basis) function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b39f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "def gaussian_features(X, n_gaussians, length_scale=0.2):\n",
    "    # transform the input data to Gaussian basis functions\n",
    "    # X: input data (N x D)\n",
    "    # n_gaussians: number of Gaussian basis functions\n",
    "    # length_scale: length scale of the Gaussians\n",
    "\n",
    "    # Use the raw x values (without the bias column)\n",
    "    x_min, x_max = np.min(X[:, 1]), np.max(X[:, 1])\n",
    "    x_grid_features = np.linspace(x_min, x_max, n_gaussians)[:, None]\n",
    "\n",
    "    dists = cdist(X[:, 1:] / length_scale, x_grid_features /\n",
    "                  length_scale, metric=\"sqeuclidean\")\n",
    "    Phi = np.exp(-0.5 * dists)\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbbe69",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
