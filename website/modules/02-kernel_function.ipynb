{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71415a16",
   "metadata": {},
   "source": [
    "# Kernel Function\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChemAI-Lab/AI4Chem/blob/main/website/modules/02-kernel_function.ipynb)\n",
    "\n",
    "**References:**\n",
    "1. **Chapters 6**: [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf), C. M. Bishop.\n",
    "2. **Chapter 2**:  [Gaussian Processes for Machine LearningOpen Access](https://direct.mit.edu/books/oa-monograph-pdf/2514321/book_9780262256834.pdf), C. E. Rasmussen, C. K. I. Williams\n",
    "3. **Chapter 4**: [Machine Learning in Quantum Sciences](https://arxiv.org/pdf/2204.04198)\n",
    "4. [**The Kernel Cookbook**](https://www.cs.toronto.edu/~duvenaud/cookbook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b42d8",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "So far we have covered regression models of the form, \n",
    "$$\n",
    "f(\\mathbf{\\phi}(\\mathbf{x}),\\mathbf{w}) = \\mathbf{w}^\\top \\mathbf{\\phi}(\\mathbf{x})= \\sum_{i}^d w_i \\, \\phi_i(\\mathbf{x}),\n",
    "$$\n",
    "where the set of non-linear transformations $\\phi_i$ when chosen properly can be powerful regression models.\n",
    "The change from \n",
    "$$\n",
    "\\mathbf{x} = \\underbrace{\\begin{bmatrix}\n",
    " x_1 \\\\\n",
    " \\vdots \\\\\n",
    "x_d\n",
    "\\end{bmatrix}}_{\\text{input space}} \\;\\;  \\to \\;\\; \\mathbf{\\phi}(\\mathbf{x}) = \\underbrace{\\begin{bmatrix}\n",
    "\\phi_1(\\mathbf{x}) \\\\\n",
    " \\vdots \\\\\n",
    "\\phi_{d'}(\\mathbf{x})\n",
    "\\end{bmatrix}}_{\\text{feature space}}\n",
    "$$\n",
    "is also know as **feature transformation**. For example, polynomial expansion. <br>\n",
    "\n",
    "\n",
    "<!-- There is another class of models, where prediction is done through a linear combination of a **kernel function** evaluated on at the training data points.  -->\n",
    "\n",
    "* We can construct another alternative approach to build a regression model using the **distance** or **similarity** between training data points; known as the **kernel** function. \n",
    "* The kernel function allows us to *implicitly* use a \"high-dimensional\" feature space; e.g., infinite polynomial expansion. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc181039",
   "metadata": {},
   "source": [
    "# Kernel trick\n",
    "Here, we present a~derivation of the kernel trick following [Appendix B in Machine Learning in Quantum Sciences](https://arxiv.org/pdf/2204.04198).\n",
    "\n",
    "1. Let's define the ridge regression loss function\n",
    "$$\n",
    "\\begin{equation}\n",
    "    {\\mathcal L}(\\mathbf{w},\\mathbf{X},\\mathbf{y}) = \\left\\| \\mathbf{X}\\mathbf{w} - \\mathbf{y} \\right\\|_2^2  + \\lambda \\left\\| \\mathbf{w} \\right\\|_2^2,\n",
    "\\end{equation}\n",
    "$$\n",
    "where, \n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "\\mathbf{x}_1^\\top \\\\\n",
    "\\mathbf{x}_2^\\top \\\\\n",
    "\\vdots \\\\\n",
    " \\mathbf{x}_{N}^\\top \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} &\\cdots & x_{1,d}\\\\\n",
    "x_{2,1} & x_{2,2}&\\cdots & x_{2,d}\\\\\n",
    "\\vdots & \\vdots &\\ddots & \\vdots\\\\\n",
    "x_{N,1} & x_{N,2}&\\cdots & x_{N,d}\\\\\n",
    "\\end{bmatrix} \\text{ and  } \\mathbf{y} = \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_{N}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "The optimal set of parameters $\\mathbf{w}^*$ is found by minimizing ${\\mathcal L}(\\mathbf{w},\\mathbf{X},\\mathbf{y})$ with respect to $\\mathbf{w}$, \n",
    "$$\n",
    "    \\mathbf{w}^* = \\arg \\min_{\\mathbf{w}} {\\mathcal L}(\\mathbf{w},\\mathbf{X},\\mathbf{y}) = \\arg \\min_{\\mathbf{w}} \\left\\| \\mathbf{X}\\mathbf{w} - \\mathbf{y} \\right\\|_2^2  + \\lambda \\left\\| \\mathbf{w} \\right\\|_2^2.\n",
    "$$\n",
    "\n",
    "2. Find the value of $\\mathbf{w}$ where  $\\nabla_{\\mathbf{w}}{\\mathcal L} = \\mathbf{0}$. Following the same procedure from the least square problem we find that, \n",
    "$$\n",
    "\\mathbf{w}^* = \\left ( \\mathbf{X}^\\top\\mathbf{X} + \\lambda I\\right )^{-1}  \\mathbf{X}^\\top\\mathbf{y}.\n",
    "$$\n",
    "\n",
    "Before we proceed further, let us examine the $\\mathbf{X}^\\top\\mathbf{X}$ term\n",
    "$$\n",
    "\\mathbf{X}^\\top\\mathbf{X} = \\begin{bmatrix}\n",
    "\\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_N \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\\mathbf{x}_1^\\top \\\\\\mathbf{x}_2^\\top \\\\\\vdots \\\\ \\mathbf{x}_N^\\top \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "x_{1,1} &  \\cdots  & x_{N,1} \\\\\n",
    "x_{1,2} &  \\cdots  & x_{N,2}  \\\\\n",
    "\\vdots & \\ddots  & \\vdots \\\\\n",
    "x_{1,d} & \\cdots  & x_{N,d}  \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}x_{1,1} & \\cdots & x_{1,d}\\\\x_{2,1} & \\cdots & x_{2,d}\\\\\\vdots & \\ddots & \\vdots\\\\x_{N,1} & \\cdots & x_{N,d}\\\\\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "4. Use the following identity matrix,\n",
    "$$\n",
    "\\left ( \\mathbf{A}\\mathbf{B} +  I\\right )^{-1} \\mathbf{A} = \\mathbf{A}\\left ( \\mathbf{B}\\mathbf{A} +  I\\right )^{-1}.\n",
    "$$\n",
    "\n",
    "5. Optimal parameters\n",
    "$$\n",
    "\\mathbf{w}^* =  \\mathbf{X}^\\top\\left ( \\mathbf{X}\\mathbf{X}^\\top + \\lambda I\\right )^{-1} \\mathbf{y}.\n",
    "$$\n",
    "Let's look closer to $\\mathbf{X}\\mathbf{X}^\\top$\n",
    "$$\n",
    "\\mathbf{X}\\mathbf{X}^\\top = \\begin{bmatrix}x_{1,1} & \\cdots & x_{1,d}\\\\x_{2,1} & \\cdots & x_{2,d}\\\\\\vdots & \\ddots & \\vdots\\\\x_{N,1} & \\cdots & x_{N,d}\\\\\\end{bmatrix} \\begin{bmatrix}\n",
    "x_{1,1} & \\cdots  & x_{N,1} \\\\\n",
    "x_{1,2} & \\cdots  & x_{N,2}  \\\\\n",
    "\\vdots & \\ddots  & \\vdots \\\\\n",
    "x_{1,d} & \\cdots  & x_{N,d}  \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix} \\mathbf{x}_1^\\top \\mathbf{x}_1 & \\mathbf{x}_1^\\top \\mathbf{x}_2 & \\cdots &\\mathbf{x}_1^\\top \\mathbf{x}_N \\\\\n",
    "\\mathbf{x}_2^\\top \\mathbf{x}_1 & \\mathbf{x}_2^\\top \\mathbf{x}_2 & \\cdots & \\mathbf{x}_2^\\top \\mathbf{x}_N \\\\\n",
    "\\vdots &  & \\ddots & \\vdots \\\\\n",
    "\\mathbf{x}_N^\\top \\mathbf{x}_1 & \\mathbf{x}_N^\\top \\mathbf{x}_2 & \\cdots & \\mathbf{x}_N^\\top \\mathbf{x}_N \\\\\n",
    "\\end{bmatrix}, \n",
    "$$\n",
    "where, $\\mathbf{x}_i^\\top \\mathbf{x}_j$ is the **dot product** between the $\\mathbf{x}_i$ and $\\mathbf{x}_j$ points.\n",
    "\n",
    "6. Regression model with optimal parameters, \n",
    "$$\n",
    "f(\\mathbf{x}_{\\text{new}},\\mathbf{w}^*) = {\\mathbf{w}^*}^\\top \\mathbf{x}_{\\text{new}}= \\mathbf{x}_{\\text{new}}^\\top \\mathbf{w}^* = \\mathbf{x}_{\\text{new}}^\\top \\left ( \\mathbf{X}^\\top\\left ( \\mathbf{X}\\mathbf{X}^\\top + \\lambda I\\right )^{-1} \\mathbf{y} \\right ), \n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbf{x}_{\\text{new}}^\\top \\mathbf{X}^\\top = \\begin{bmatrix} \n",
    "x_1 & x_2 & \\cdots & x_d\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_{1,1} & \\cdots  & x_{N,1} \\\\\n",
    "x_{1,2} & \\cdots  & x_{N,2}  \\\\\n",
    "\\vdots & \\ddots  & \\vdots \\\\\n",
    "x_{1,d} & \\cdots  & x_{N,d}  \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix} \n",
    "\\mathbf{x}_{\\text{new}}^\\top \\mathbf{x}_1 & \\mathbf{x}_{\\text{new}}^\\top \\mathbf{x}_2 & \\cdots & \\mathbf{x}_{\\text{new}}^\\top \\mathbf{x}_N\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "which is the **dot product** between the point where the function is evaluated and the training data.\n",
    "\n",
    "7. The final model.\n",
    "$$\n",
    "f(\\mathbf{x}_{\\text{new}},\\mathbf{w}^*) = \\mathbf{x}_{\\text{new}}^\\top \\mathbf{X}^\\top \\underbrace{\\left ( \\mathbf{X}\\mathbf{X}^\\top + \\lambda I\\right )^{-1} \\mathbf{y}}_{\\mathbf{a}}  = k(\\mathbf{x}_{\\text{new}}, \\mathbf{X})^\\top \\;\\mathbf{a}.\n",
    "$$\n",
    "\n",
    "This equation can be interpreted as a linear model on the feature space $k$ and the linear weights $\\mathbf{a}$.\n",
    "\n",
    "\n",
    "## Kernel trick in feature space\n",
    "You can carry exactly the same procedure and find that the model for $f(\\mathbf{x},\\mathbf{w}) = \\mathbf{w}^\\top \\phi(\\mathbf{x})$, \n",
    "1. Optimal parameters,\n",
    "   $$\n",
    "    \\mathbf{w}^* =  \\Phi(\\mathbf{X})^\\top\\left ( \\Phi(\\mathbf{X})\\Phi(\\mathbf{X})^\\top + \\lambda I\\right )^{-1} \\mathbf{y},\n",
    "   $$\n",
    "   where $\\Phi(\\mathbf{X})$ is the data represented in the feature space. \n",
    "2. Optimal model\n",
    "   $$   \n",
    "    f(\\phi(\\mathbf{x}_{\\text{new}}),\\mathbf{w}^*) = \\underbrace{\\phi(\\mathbf{x}_{\\text{new}})^\\top \\Phi(\\mathbf{X})^\\top}_{[\\phi(\\mathbf{x}_{\\text{new}})^\\top \\phi(\\mathbf{x}_1),\\cdots,\\phi(\\mathbf{x}_{\\text{new}})^\\top \\phi(\\mathbf{x}_N)]} \\underbrace{\\left ( \\Phi(\\mathbf{X})\\Phi(\\mathbf{X})^\\top + \\lambda I\\right )^{-1} \\mathbf{y}}_{\\mathbf{a}} \n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd1ec2b",
   "metadata": {},
   "source": [
    "## Kernel in Feature Space\n",
    "\n",
    "The kernel function is defined for a one-dimensional input space by, \n",
    "\n",
    "$$\n",
    "k(x_i,x_j) = \\phi(x_i)^\\top \\phi(x_j) = \\sum_\\ell^m \\phi_\\ell(x_i)\\phi_\\ell(x_j)\n",
    "$$\n",
    "\n",
    "Let's consider two different feature spaces, \n",
    "1. Polynomial, $\\phi(\\mathbf{x})^\\top = [x, x^2, x^3, \\cdots, x^p]$\n",
    "2. Gaussian, $\\phi(\\mathbf{x})^\\top = [\\sigma(\\mathbf{x} - \\mathbf{\\mu}_1), \\sigma(\\mathbf{x} - \\mathbf{\\mu}_2), \\cdots, \\sigma(\\mathbf{x} - \\mathbf{\\mu}_m)]$ where, $\\sigma(\\mathbf{x} - \\mathbf{\\mu}_i) = e^{\\frac{-(\\mathbf{x} - \\mathbf{\\mu}_i)^2}{2\\ell}}$\n",
    "3. Sigmoid; $\\phi(\\mathbf{x})^\\top = [\\sigma(\\mathbf{x} - \\mathbf{\\mu}_1), \\sigma(\\mathbf{x} - \\mathbf{\\mu}_2), \\cdots, \\sigma(\\mathbf{x} - \\mathbf{\\mu}_m)]$ where, $\\sigma(\\mathbf{x} - \\mathbf{\\mu}_i) = \\frac{1}{1 + e^{-(\\mathbf{x} - \\mathbf{\\mu}_i)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Grid and settings\n",
    "x = np.linspace(-1, 1, 1600)\n",
    "dx = x[1] - x[0]\n",
    "xprime = 0.0\n",
    "\n",
    "M = 10\n",
    "centers = np.linspace(-1, 1, M)\n",
    "\n",
    "def gram_inverse_kernel(raw_basis, x, dx, xprime=0.0):\n",
    "    \"\"\"\n",
    "    raw_basis: list of arrays b_i(x) sampled on x-grid\n",
    "    returns k(x, xprime) using k(x,x') = b(x)^T G^{-1} b(x')\n",
    "    with G_ij = ∫ b_i(x) b_j(x) dx  (approximated on grid).\n",
    "    \"\"\"\n",
    "    B = np.stack(raw_basis, axis=1)             # shape (N, M)\n",
    "    G = (B.T @ B) * dx                          # Gram matrix\n",
    "    Ginv = np.linalg.pinv(G)                    # stable inverse\n",
    "\n",
    "    b_xprime = np.array([np.interp(xprime, x, bi) for bi in raw_basis])  # (M,)\n",
    "    k = B @ (Ginv @ b_xprime)                   # (N,)\n",
    "    return k\n",
    "\n",
    "\n",
    "# --- Column 1: Polynomials ---\n",
    "P = 10\n",
    "raw_poly = [x**i for i in range(1, P+1)]\n",
    "k_poly = gram_inverse_kernel(raw_poly, x, dx, xprime)\n",
    "\n",
    "# --- Column 2: Gaussians ---\n",
    "sigma = 0.25\n",
    "raw_gauss = [np.exp(-(x-mu)**2/(2*sigma**2)) for mu in centers]\n",
    "k_gauss = gram_inverse_kernel(raw_gauss, x, dx, xprime)\n",
    "\n",
    "# --- Column 3: Sigmoids ---\n",
    "s = 0.15\n",
    "def sigmoid(z): return 1/(1+np.exp(-z))\n",
    "\n",
    "\n",
    "raw_sig = [sigmoid((x-mu)/s) for mu in centers]\n",
    "k_sig = gram_inverse_kernel(raw_sig, x, dx, xprime)\n",
    "\n",
    "# (Optional) match PRML-ish vertical scale for the bottom row\n",
    "# (Bishop’s plot is small-amplitude; scaling doesn’t change the shape)\n",
    "scale = 0.05 / max(np.max(np.abs(k_poly)),\n",
    "                   np.max(np.abs(k_gauss)), np.max(np.abs(k_sig)))\n",
    "k_poly *= scale\n",
    "k_gauss *= scale\n",
    "k_sig *= scale\n",
    "\n",
    "# ---- Plot: 2x3 layout ----\n",
    "fig, axs = plt.subplots(2, 3, figsize=(10, 5))\n",
    "\n",
    "# Top row: basis functions\n",
    "for b in raw_poly:\n",
    "    axs[0, 0].plot(x, b)\n",
    "axs[0, 0].set_title(\"Polynomial\")\n",
    "axs[0, 0].set_xlim(-1, 1)\n",
    "\n",
    "for b in raw_gauss:\n",
    "    axs[0, 1].plot(x, b)\n",
    "axs[0, 1].set_title(\"Gaussian\")\n",
    "axs[0, 1].set_xlim(-1, 1)\n",
    "axs[0, 1].set_ylim(0, 1.05)\n",
    "\n",
    "for b in raw_sig:\n",
    "    axs[0, 2].plot(x, b)\n",
    "axs[0, 2].set_title(\"Sigmoid\")\n",
    "axs[0, 2].set_xlim(-1, 1)\n",
    "axs[0, 2].set_ylim(0, 1.05)\n",
    "\n",
    "# Bottom row: kernel k(x, x')\n",
    "axs[1, 0].plot(x, k_poly, 'b')\n",
    "axs[1, 0].scatter([xprime], [0], c='r', marker='x')\n",
    "axs[1, 1].plot(x, k_gauss, 'b')\n",
    "axs[1, 1].scatter([xprime], [0], c='r', marker='x')\n",
    "axs[1, 2].plot(x, k_sig,  'b')\n",
    "axs[1, 2].scatter([xprime], [0], c='r', marker='x')\n",
    "for j in range(3):\n",
    "    axs[1, j].set_xlim(-1, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e61cc7",
   "metadata": {},
   "source": [
    "# \"Implicit\" Kernels\n",
    "\n",
    "The main advantage of working with kernel-based methods is the possibility to work with high-dimensional feature spaces but computing the kernel function implicitly, meaning by-passing the dot product in feature space. <br>\n",
    "Let's consider a 1D infinite polynomial, \n",
    "$$\n",
    "\\phi(x) = [1,x^1,x^2,\\cdots,x^p]\n",
    "$$\n",
    "where $p$ is a large number. \n",
    "You can try at home to do the fitting of this large polynomial, we have to invert,\n",
    "$$\n",
    "\\left (\\Phi(\\mathbf{X})^\\top \\Phi(\\mathbf{X})+ \\lambda I\\right ).\n",
    "$$\n",
    "$\\Phi(\\mathbf{X})^\\top \\Phi(\\mathbf{X})$ is a square matrix of the size of the feature space, meaning, the larger the feature space representation the higher the computational cost to fit this style of models. <br>\n",
    "\n",
    "If we use the kernel trick, the matrix to inverse is, \n",
    "$$\n",
    "\\left (\\Phi(\\mathbf{X})\\Phi(\\mathbf{X})^\\top + \\lambda I\\right ),\n",
    "$$\n",
    "where $\\Phi(\\mathbf{X})\\Phi(\\mathbf{X})^\\top $ is a square matrix of the size of the data. \n",
    "\n",
    "## Gaussian or Radial basis function kernel\n",
    "\n",
    "To illustrate the advantage of using the kernel function, lets use the Gaussian or RBF kernel. \n",
    "The RBF kernel is defined as, \n",
    "$$\n",
    "k_{\\text{RBF}}(x,x') = e^{\\frac{-(x-x')^2}{2\\ell^2}}.\n",
    "$$\n",
    "This kernel can be rewritten as, \n",
    "$$\n",
    "k_{\\text{RBF}}(x,x') = e^{\\frac{-(x)^2}{2\\ell^2}}e^{\\frac{-(x')^2}{2\\ell^2}}\\left ( \\sum_m^\\infty  \\frac{1}{m!}\\left (\\frac{x^\\top x'}{\\ell^2} \\right )^m \\right ),\n",
    "$$\n",
    "where the terms $e^{\\frac{-(x)^2}{2\\ell^2}}$ and $e^{\\frac{-(x')^2}{2\\ell^2}}$ are simple normalization values. The last term depends on the $(x^\\top x')^m$. we know that as $m\\to \\infty$, \n",
    "$$\n",
    "\\sum_m^\\infty  \\frac{1}{m!}\\left (\\frac{x^\\top x'}{\\ell^2} \\right )^m  \\to e^{\\frac{x\\top x'}{\\ell^2}}\n",
    "$$\n",
    "\n",
    "An RBF kernel is the same as an infinity polynomial regression model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e959fcb",
   "metadata": {},
   "source": [
    "# Common Kernel Functions\n",
    "\n",
    "1. Linear Kernel\n",
    "   $$\n",
    "   k(x, x') = x^T x'\n",
    "   $$\n",
    "2. Polynomial Kernel\n",
    "   $$\n",
    "   k(x, x') = (x^T x' + c)^p\n",
    "   $$\n",
    "3. Radial Basis Function (RBF / Gaussian) Kernel\n",
    "   $$\n",
    "   k(x, x') = \\exp\\left( -\\frac{\\lVert x - x' \\rVert^2}{2\\ell^2} \\right)\n",
    "   $$\n",
    "4. Laplacian Kernel\n",
    "   $$\n",
    "   k(x, x') = \\exp\\left( -\\frac{\\lVert x - x' \\rVert_1}{\\ell} \\right)\n",
    "   $$\n",
    "5. Rational Quadratic Kernel\n",
    "   $$\n",
    "   k(x, x') = \\left( 1 + \\frac{\\lVert x - x' \\rVert^2}{2\\alpha\\ell^2} \\right)^{-\\alpha}\n",
    "   $$\n",
    "6. Cosine Similarity Kernel\n",
    "   $$\n",
    "   k(x, x') = \\frac{x^T x'}{\\lVert x \\rVert \\, \\lVert x' \\rVert}\n",
    "   $$\n",
    "7. Matern Kernel\n",
    "   $$\n",
    "   k(x, x') = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\n",
    "   \\left( \\frac{\\sqrt{2\\nu} \\, \\lVert x - x' \\rVert}{\\sigma} \\right)^\\nu\n",
    "   K_\\nu\\left( \\frac{\\sqrt{2\\nu} \\, \\lVert x - x' \\rVert}{\\sigma} \\right)\n",
    "   $$\n",
    "8. Periodic Kernel\n",
    "   $$\n",
    "   k(x, x') = \\exp\\left( -\\frac{2\\sin^2\\left( \\pi \\lVert x - x' \\rVert / p \\right)}{\\sigma^2} \\right)\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f83870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel, polynomial_kernel, rbf_kernel\n",
    "from sklearn.metrics.pairwise import pairwise_kernels, cosine_similarity\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic, ExpSineSquared, Matern\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a90e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear kernel slice with interactive x' (Jupyter + ipywidgets)\n",
    "# k(x, x') = x^T x'  (in 1D: k(x, x') = x * x')\n",
    "\n",
    "# 1D grid for x\n",
    "x = np.linspace(-3, 3, 800).reshape(-1, 1)\n",
    "\n",
    "# Widget for x'\n",
    "xprime_slider = widgets.FloatSlider(\n",
    "    value=0.0, min=-3.0, max=3.0, step=0.05,\n",
    "    description=\"x'\",\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "def update_plot(change=None):\n",
    "    xprime = xprime_slider.value\n",
    "    x_p = np.array([[xprime]])\n",
    "    k = linear_kernel(x, x_p).ravel()\n",
    "\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        plt.figure(figsize=(7, 3.2))\n",
    "        plt.plot(x.ravel(), k, linewidth=2)\n",
    "        plt.axvline(xprime, linestyle=\"--\", color=\"k\", alpha=0.6)\n",
    "        plt.title(\n",
    "            rf\"Linear kernel with $x'={xprime:.2f}$\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(f\"k(x, {xprime:.2f})\")\n",
    "        plt.ylim(-3, 3)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "xprime_slider.observe(update_plot, names=\"value\")\n",
    "\n",
    "display(xprime_slider, out)\n",
    "update_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7455ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial kernel slice with interactive x' (Jupyter + ipywidgets)\n",
    "# k(x, x') = (x^T x' + c)^p   (in 1D: (x * x' + c)^p)\n",
    "\n",
    "# 1D grid for x\n",
    "x = np.linspace(-3, 3, 800).reshape(-1, 1)\n",
    "\n",
    "# Widgets\n",
    "xprime_slider = widgets.FloatSlider(\n",
    "    value=0.0, min=-3.0, max=3.0, step=0.05,\n",
    "    description=\"x'\",\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "degree_slider = widgets.IntSlider(\n",
    "    value=3, min=1, max=10, step=1,\n",
    "    description=\"degree\"\n",
    ")\n",
    "\n",
    "coef0_slider = widgets.FloatSlider(\n",
    "    value=1.0, min=-2.0, max=2.0, step=0.05,\n",
    "    description=\"c\"\n",
    ")\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "def update_plot(change=None):\n",
    "    xprime = xprime_slider.value\n",
    "    degree = degree_slider.value\n",
    "    coef0 = coef0_slider.value\n",
    "    x_p = np.array([[xprime]])\n",
    "\n",
    "    k = polynomial_kernel(x, x_p, degree=degree, coef0=coef0).ravel()\n",
    "\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        plt.figure(figsize=(7, 3.2))\n",
    "        plt.plot(x.ravel(), k, linewidth=2)\n",
    "        plt.axvline(xprime, linestyle=\"--\", color=\"k\", alpha=0.6)\n",
    "        plt.title(\n",
    "            rf\"Polynomial kernel: $k(x,x')=(x x' + {coef0:.2f})^{degree}$, \"\n",
    "            rf\"$x'={xprime:.2f}$\"\n",
    "        )\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(f\"k(x, {xprime:.2f})\")\n",
    "        plt.ylim(-10, 10)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Attach callbacks\n",
    "xprime_slider.observe(update_plot, names=\"value\")\n",
    "degree_slider.observe(update_plot, names=\"value\")\n",
    "coef0_slider.observe(update_plot, names=\"value\")\n",
    "\n",
    "display(widgets.VBox([xprime_slider, degree_slider, coef0_slider]), out)\n",
    "update_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80d9a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF (Gaussian) kernel slice with interactive x' (Jupyter + ipywidgets)\n",
    "# k(x, x') = exp( -gamma * ||x - x'||^2 )   (in 1D: exp(-gamma (x - x')^2))\n",
    "\n",
    "\n",
    "# 1D grid for x\n",
    "x = np.linspace(-3, 3, 800).reshape(-1, 1)\n",
    "\n",
    "# Widgets\n",
    "xprime_slider = widgets.FloatSlider(\n",
    "    value=0.0, min=-3.0, max=3.0, step=0.05,\n",
    "    description=\"x'\",\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "gamma_slider = widgets.FloatLogSlider(\n",
    "    value=1.0, base=10, min=-2, max=2, step=0.05,\n",
    "    description=\"gamma\"\n",
    ")\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "def update_plot(change=None):\n",
    "    xprime = xprime_slider.value\n",
    "    gamma = gamma_slider.value\n",
    "    x_p = np.array([[xprime]])\n",
    "\n",
    "    k = rbf_kernel(x, x_p, gamma=gamma).ravel()\n",
    "\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        plt.figure(figsize=(7, 3.2))\n",
    "        plt.plot(x.ravel(), k, linewidth=2)\n",
    "        plt.axvline(xprime, linestyle=\"--\", color=\"k\", alpha=0.6)\n",
    "        plt.title(\n",
    "            rf\"RBF kernel: $k(x,x')=\\exp(-\\gamma (x-x')^2)$, \"\n",
    "            rf\"$x'={xprime:.2f}$, $\\gamma={gamma:.3g}$\"\n",
    "        )\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(f\"k(x, {xprime:.2f})\")\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Attach callbacks\n",
    "xprime_slider.observe(update_plot, names=\"value\")\n",
    "gamma_slider.observe(update_plot, names=\"value\")\n",
    "\n",
    "display(widgets.VBox([xprime_slider, gamma_slider]), out)\n",
    "update_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b20b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laplacian kernel slice with interactive x' (Jupyter + ipywidgets)\n",
    "# k(x, x') = exp( -gamma * ||x - x'||_1 )   (in 1D: exp(-gamma * |x - x'|))\n",
    "\n",
    "# 1D grid for x\n",
    "x = np.linspace(-3, 3, 800).reshape(-1, 1)\n",
    "\n",
    "# Widgets\n",
    "xprime_slider = widgets.FloatSlider(\n",
    "    value=0.0, min=-3.0, max=3.0, step=0.05,\n",
    "    description=\"x'\",\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "gamma_slider = widgets.FloatLogSlider(\n",
    "    value=1.0, base=10, min=-2, max=2, step=0.05,\n",
    "    description=\"gamma\"\n",
    ")\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "def update_plot(change=None):\n",
    "    xprime = xprime_slider.value\n",
    "    gamma = gamma_slider.value\n",
    "    x_p = np.array([[xprime]])\n",
    "\n",
    "    # Laplacian kernel via sklearn's pairwise_kernels\n",
    "    k = pairwise_kernels(x, x_p, metric=\"laplacian\", gamma=gamma).ravel()\n",
    "\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        plt.figure(figsize=(7, 3.2))\n",
    "        plt.plot(x.ravel(), k, linewidth=2)\n",
    "        plt.axvline(xprime, linestyle=\"--\", color=\"k\", alpha=0.6)\n",
    "        plt.title(\n",
    "            rf\"Laplacian kernel: $k(x,x')=\\exp(-\\gamma |x-x'|)$, \"\n",
    "            rf\"$x'={xprime:.2f}$, $\\gamma={gamma:.3g}$\"\n",
    "        )\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(f\"k(x, {xprime:.2f})\")\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Attach callbacks\n",
    "xprime_slider.observe(update_plot, names=\"value\")\n",
    "gamma_slider.observe(update_plot, names=\"value\")\n",
    "\n",
    "display(widgets.VBox([xprime_slider, gamma_slider]), out)\n",
    "update_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e49cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity kernel slice with interactive x' (Jupyter + ipywidgets)\n",
    "# k(x, x') = (x^T x') / (||x|| ||x'||)\n",
    "\n",
    "\n",
    "# 1D grid for x (avoid exact zero to keep things well-defined)\n",
    "x = np.linspace(-3, 3, 800).reshape(-1, 1)\n",
    "eps = 1e-8\n",
    "x[np.abs(x) < eps] = eps\n",
    "\n",
    "# Widgets\n",
    "xprime_slider = widgets.FloatSlider(\n",
    "    value=1.0, min=-3.0, max=3.0, step=0.05,\n",
    "    description=\"x'\",\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "def update_plot(change=None):\n",
    "    xprime = xprime_slider.value\n",
    "    x_p = np.array([[xprime if abs(xprime) > eps else eps]])\n",
    "\n",
    "    k = cosine_similarity(x, x_p).ravel()\n",
    "\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        plt.figure(figsize=(7, 3.2))\n",
    "        plt.plot(x.ravel(), k, linewidth=2)\n",
    "        plt.axvline(xprime, linestyle=\"--\", color=\"k\", alpha=0.6)\n",
    "        plt.title(\n",
    "            # $k(x,x')=\\frac{x-x'}{|x||x'|}$, \"\n",
    "            rf\"Cosine similarity:  $k(x,x')=x-x'/|x||x'|)$, \"\n",
    "            rf\"$x'={xprime:.2f}$\"\n",
    "        )\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(f\"k(x, {xprime:.2f})\")\n",
    "        plt.ylim(-1.1, 1.1)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "# Attach callback\n",
    "xprime_slider.observe(update_plot, names=\"value\")\n",
    "\n",
    "display(xprime_slider, out)\n",
    "update_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b468479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rational Quadratic kernel slice with interactive x' (Jupyter + ipywidgets)\n",
    "# k(x, x') = ( 1 + ||x - x'||^2 / (2 * alpha * l^2) )^(-alpha)\n",
    "\n",
    "# 1D grid for x\n",
    "x = np.linspace(-3, 3, 800).reshape(-1, 1)\n",
    "\n",
    "# Widgets\n",
    "xprime_slider = widgets.FloatSlider(\n",
    "    value=0.0, min=-3.0, max=3.0, step=0.05,\n",
    "    description=\"x'\",\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "length_scale_slider = widgets.FloatLogSlider(\n",
    "    value=1.0, base=10, min=-2, max=2, step=0.05,\n",
    "    description=\"ℓ\"\n",
    ")\n",
    "\n",
    "alpha_slider = widgets.FloatLogSlider(\n",
    "    value=1.0, base=10, min=-2, max=2, step=0.05,\n",
    "    description=\"alpha\"\n",
    ")\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "def update_plot(change=None):\n",
    "    xprime = xprime_slider.value\n",
    "    l = length_scale_slider.value\n",
    "    alpha = alpha_slider.value\n",
    "\n",
    "    x_p = np.array([[xprime]])\n",
    "\n",
    "    kfun = RationalQuadratic(length_scale=l, alpha=alpha)\n",
    "    k = kfun(x, x_p).ravel()\n",
    "\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        plt.figure(figsize=(7, 3.2))\n",
    "        plt.plot(x.ravel(), k, linewidth=2)\n",
    "        plt.axvline(xprime, linestyle=\"--\", color=\"k\", alpha=0.6)\n",
    "        plt.title(\n",
    "            rf\"RationalQuadratic: $k(x,x')=\\left(1+\\frac{{(x-x')^2}}{{2\\alpha\\ell^2}}\\right)^{{-\\alpha}}$, \"\n",
    "            rf\"$x'={xprime:.2f}$, $\\ell={l:.3g}$, $\\alpha={alpha:.3g}$\"\n",
    "        )\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(f\"k(x, {xprime:.2f})\")\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Attach callbacks\n",
    "xprime_slider.observe(update_plot, names=\"value\")\n",
    "length_scale_slider.observe(update_plot, names=\"value\")\n",
    "alpha_slider.observe(update_plot, names=\"value\")\n",
    "\n",
    "display(widgets.VBox([xprime_slider, length_scale_slider, alpha_slider]), out)\n",
    "update_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d59bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExpSineSquared (Periodic) kernel slice with interactive x' (Jupyter + ipywidgets)\n",
    "# k(x, x') = exp( - 2 * sin^2(pi * |x - x'| / p) / l^2 )\n",
    "\n",
    "# 1D grid for x\n",
    "x = np.linspace(-3, 3, 1200).reshape(-1, 1)\n",
    "\n",
    "# Widgets\n",
    "xprime_slider = widgets.FloatSlider(\n",
    "    value=0.0, min=-3.0, max=3.0, step=0.05,\n",
    "    description=\"x'\",\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "length_scale_slider = widgets.FloatLogSlider(\n",
    "    value=0.6, base=10, min=-2, max=1, step=0.05,\n",
    "    description=\"ℓ\"\n",
    ")\n",
    "\n",
    "periodicity_slider = widgets.FloatSlider(\n",
    "    value=1.0, min=0.2, max=4.0, step=0.05,\n",
    "    description=\"period p\",\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "def update_plot(change=None):\n",
    "    xprime = xprime_slider.value\n",
    "    l = length_scale_slider.value\n",
    "    p = periodicity_slider.value\n",
    "\n",
    "    x_p = np.array([[xprime]])\n",
    "\n",
    "    kfun = ExpSineSquared(length_scale=l, periodicity=p)\n",
    "    k = kfun(x, x_p).ravel()\n",
    "\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        plt.figure(figsize=(7, 3.2))\n",
    "        plt.plot(x.ravel(), k, linewidth=2)\n",
    "        plt.axvline(xprime, linestyle=\"--\", color=\"k\", alpha=0.6)\n",
    "        plt.title(\n",
    "            rf\"ExpSineSquared (Periodic): $k=\\exp\\!\\left(-\\frac{{2\\sin^2(\\pi|x-x'|/p)}}{{\\ell^2}}\\right)$, \"\n",
    "            rf\"$x'={xprime:.2f}$, $\\ell={l:.3g}$, $p={p:.2f}$\"\n",
    "        )\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(f\"k(x, {xprime:.2f})\")\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Attach callbacks\n",
    "xprime_slider.observe(update_plot, names=\"value\")\n",
    "length_scale_slider.observe(update_plot, names=\"value\")\n",
    "periodicity_slider.observe(update_plot, names=\"value\")\n",
    "\n",
    "display(widgets.VBox(\n",
    "    [xprime_slider, length_scale_slider, periodicity_slider]), out)\n",
    "update_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matérn kernel slices (nu=0.5, 1.5, 2.5) together, interactive x' and gamma\n",
    "\n",
    "# 1D grid for x\n",
    "x = np.linspace(-3, 3, 1000).reshape(-1, 1)\n",
    "\n",
    "# Widget: x'\n",
    "xprime_slider = widgets.FloatSlider(\n",
    "    value=0.0, min=-3.0, max=3.0, step=0.05,\n",
    "    description=\"x'\",\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "# Widget: gamma (log scale)\n",
    "gamma_slider = widgets.FloatLogSlider(\n",
    "    value=1.0, base=10, min=-2, max=2, step=0.05,\n",
    "    description=\"gamma\",\n",
    "    continuous_update=True\n",
    ")\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "def gamma_to_length_scale(gamma):\n",
    "    # Avoid division by zero\n",
    "    gamma = max(float(gamma), 1e-12)\n",
    "    return 1.0 / np.sqrt(2.0 * gamma)\n",
    "\n",
    "\n",
    "def update_plot(change=None):\n",
    "    xprime = xprime_slider.value\n",
    "    gamma = gamma_slider.value\n",
    "    length_scale = gamma_to_length_scale(gamma)\n",
    "\n",
    "    x_p = np.array([[xprime]])\n",
    "\n",
    "    k05 = Matern(length_scale=length_scale, nu=0.5)(x, x_p).ravel()\n",
    "    k15 = Matern(length_scale=length_scale, nu=1.5)(x, x_p).ravel()\n",
    "    k25 = Matern(length_scale=length_scale, nu=2.5)(x, x_p).ravel()\n",
    "\n",
    "    with out:\n",
    "        out.clear_output(wait=True)\n",
    "        plt.figure(figsize=(7, 3.2))\n",
    "        plt.plot(x.ravel(), k05, linewidth=2, label=r\"Matérn $\\nu=0.5$\")\n",
    "        plt.plot(x.ravel(), k15, linewidth=2, label=r\"Matérn $\\nu=1.5$\")\n",
    "        plt.plot(x.ravel(), k25, linewidth=2, label=r\"Matérn $\\nu=2.5$\")\n",
    "        plt.axvline(xprime, linestyle=\"--\", color=\"k\", alpha=0.6)\n",
    "        plt.title(\n",
    "            rf\"Matérn slices at $x'={xprime:.2f}$, \"\n",
    "            rf\"$\\gamma={gamma:.3g}$ (mapped to $\\ell={length_scale:.3g}$)\"\n",
    "        )\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"k(x, x')\")\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "xprime_slider.observe(update_plot, names=\"value\")\n",
    "gamma_slider.observe(update_plot, names=\"value\")\n",
    "\n",
    "display(widgets.VBox([xprime_slider, gamma_slider]), out)\n",
    "update_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237936c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4chem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
